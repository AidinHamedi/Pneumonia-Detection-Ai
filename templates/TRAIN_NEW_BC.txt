import gc
# Garbage Collection (memory)
gc.collect()
tf.keras.backend.clear_session()
# CONF
max_epoch = 256 # 128 for small models 256 for full Fine tuning and big models
subset_epoch = 8 # change it if you are using a combined model | DEF=6 / COMM=8 | Too little can result the model not Learn the patterns and too much makes the model overfit on that subset and perform badly on the next subset
subset_epoch_FT = 5
PL_epoch = 18 # 16 for small models and >=24 for big models
subset_size = 1024 
Conf_batch_size_REV2 = 8 
OneCycleLr_MAXLR = 0.01
OneCycleLr_DEC_A = 0.0005
OneCycleLr_MINLR = 0.0055
Use_ES_ONSUBT = False
EarlyStopping_P = 5
BEST_RSN = 'PAI_model_T'
#VAR
OneCycleLr_CUNLR = OneCycleLr_MAXLR
all_histories = []
best_acc = 0
#Funcs
def add_image_grain_TRLRev2(image, intensity = 0.01):
    # Generate random noise array
    noise = np.random.randint(0, 255, size=image.shape, dtype=np.uint8)

    # Scale the noise array
    scaled_noise = (noise * intensity).astype(np.float32)
    # Add the noise to the image
    noisy_image = cv2.add(image, scaled_noise)

    return noisy_image
def noise_func_TRLRev2(image):
    noise_type = np.random.choice(['L1', 'L2', 'L3', 'none'])
    new_image = np.copy(image)
    
    if noise_type == 'L3':
        intensityL2 = random.uniform(0.001, 0.016)
        intensityL1 = random.uniform(0.005, 0.020)
    else:
        intensityL2 = random.uniform(0.001, 0.027)
        intensityL1 = random.uniform(0.001, 0.028)
    
    block_size_L1 = random.randint(16, 32)
    block_size_L2 = random.randint(32, 64)
    
    if noise_type == 'L2' or noise_type == 'L3':
        for i in range(0, image.shape[0], block_size_L2):
            for j in range(0, image.shape[1], block_size_L2):
                block = image[i:i+block_size_L2, j:j+block_size_L2]
                block = (np.random.rand() * intensityL2 + 1) * block
                new_image[i:i+block_size_L2, j:j+block_size_L2] = block
        image = new_image      
        
    if noise_type == 'L1' or noise_type == 'L3': 
        for i in range(0, image.shape[0], block_size_L1):
            for j in range(0, image.shape[1], block_size_L1):
                block = image[i:i+block_size_L1, j:j+block_size_L1]
                block = (np.random.rand() * intensityL1 + 1) * block
                new_image[i:i+block_size_L1, j:j+block_size_L1] = block
    
    if add_img_grain:
        intensity = random.uniform(0, 0.022)  # Random intensity 
        new_image = add_image_grain_TRLRev2(new_image, intensity=intensity)
    return new_image
#CONST
train_SUB_datagen = ImageDataGenerator(
        horizontal_flip=True,
        vertical_flip=True,
        rotation_range=179,
        zoom_range=0.24, 
        shear_range=0.22,
        width_shift_range=0.21,
        brightness_range=(0.88, 1.12),
        height_shift_range=0.21,
        channel_shift_range=100,
        featurewise_center=False,
        featurewise_std_normalization=False,
        interpolation_order=2,
        fill_mode='nearest',
        preprocessing_function=noise_func_TRLRev2
    )
steps_per_epoch_train_SUB = subset_size // Conf_batch_size_REV2
early_stopping = EarlyStopping(monitor='val_accuracy', patience=EarlyStopping_P, verbose=1, restore_best_weights=True, mode='max')
#MAIN
print('Training the model...')
for epoch in range(1, max_epoch):
    # Start Epoch
    STG = 'Learning the patterns' if epoch < PL_epoch else 'Fine tuning'
    C_subset_epoch = subset_epoch if epoch < PL_epoch else subset_epoch_FT
    start_FULL_time = time.time()
    print_Color(f'\n~*Epoch: ~*{epoch}~*/~*{max_epoch}~* | ~*[{STG}]', ['normal', 'cyan', 'normal', 'green', 'magenta', 'green'], advanced_mode=True)
    print_Color(f'~*Setting model subset epoch.c to ~*[{C_subset_epoch}]~*...', ['yellow', 'green', 'yellow'], advanced_mode=True)
    # DP
    print_Color('Shuffling data...', ['yellow'])
    x_train, y_train = shuffle_data(x_train, y_train)
    print_Color(f'~*Taking a subset of ~*[{subset_size}]~*...', ['yellow', 'green', 'yellow'], advanced_mode=True)
    subset_indices = np.random.choice(x_train.shape[0], subset_size, replace=False)
    x_SUB_train = x_train[subset_indices]
    y_SUB_train = y_train[subset_indices]
    print_Color('Augmenting data...', ['yellow']) 
    train_SUB_augmented_images = train_SUB_datagen.flow(x_SUB_train * 255, y_SUB_train, shuffle=False, batch_size=len(x_SUB_train)).next()
    x_SUB_train = np.clip(train_SUB_augmented_images[0], 0, 255) / 255
    y_SUB_train = train_SUB_augmented_images[1]
    # learning_rate_schedule_SUB
    if epoch > PL_epoch and OneCycleLr_CUNLR > OneCycleLr_MINLR:
        OneCycleLr_CUNLR -= OneCycleLr_DEC_A
        
    learning_rate_schedule_SUB = OneCycleLr(max_lr=OneCycleLr_CUNLR, steps_per_epoch=steps_per_epoch_train_SUB, epochs=C_subset_epoch)
    print_Color(f'~*Setting model OneCycleLr::maxlr to ~*[{OneCycleLr_CUNLR:.6f}]~*...', ['yellow', 'green', 'yellow'], advanced_mode=True)
    # Train
    print_Color('Training on subset...', ['green'])
    start_SUBO_time = time.time()
    SUB_history = model.fit(x_SUB_train,
                        y_SUB_train,
                        epochs=C_subset_epoch,
                        batch_size=Conf_batch_size_REV2,
                        validation_data=(x_test, y_test),
                        verbose='auto',
                        callbacks=[learning_rate_schedule_SUB, early_stopping] if Use_ES_ONSUBT else [learning_rate_schedule_SUB]
    )
    end_SUBO_time = time.time()
    print_Color('Subset training done.', ['green'])
    all_histories.append(SUB_history.history)
    # Evaluate the model on the test data
    evaluation = model.evaluate(x_test, y_test, verbose=0)
    
    # Extract the loss and accuracy from the evaluation results
    loss = evaluation[0]
    acc = evaluation[1]

    # If the accuracy is higher than the best_acc
    if acc > best_acc:
        print("Improved model accuracy from {} to {}. Saving model.".format(best_acc, acc))
        
        # Update the best_acc
        best_acc = acc
        
        # Save the model
        if SAVE_TYPE == 'TF':
            print('Saving full model tf format...')
            model.save(BEST_RSN, save_format='tf')
        else:
            model.save(f'{BEST_RSN}.h5')
    else:
        print("Model accuracy did not improve from {}. Not saving model.".format(best_acc)) 
    # Garbage Collection (memory)
    gc.collect()
    tf.keras.backend.clear_session()   
    # Epoch end
    end_time = time.time()
    epoch_time = end_time - start_FULL_time
    print(f"Time taken for epoch(FULL) {epoch}: {epoch_time:.2f} sec")
    epoch_SUB_time = end_SUBO_time - start_SUBO_time
    print(f"Time taken for epoch(SUBo) {epoch}: {epoch_SUB_time:.2f} sec")
    print_Color(f'<---------------------------------------|Epoch [{epoch}] END|--------------------------------------->', ['cyan'])
# End
history = {}
for key in all_histories[0].keys():
    # For each metric, concatenate the values from all histories
    history[key] = np.concatenate([h[key] for h in all_histories])
print('Training done.\n')