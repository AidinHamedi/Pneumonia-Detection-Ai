import gc
# Garbage Collection (memory)
gc.collect()
tf.keras.backend.clear_session()
#CONF
WTD_augmentation = True
Conf_batch_size = 4 
Learning_rate_conf = 3 # 1 and 2 for custom learning_rate_fn and 3 for OneCycleLr (Better for full training)
#TensorBoard conf
TensorBoard_UF = 1 # 1 for Slow 2 for fast (very slow tarining)
# Learning rate configuration
Learning_rate_conf_SET2C = 3 # 1 for SGD and 2 for Adam and... for lower lr 3 for very high lr
OneCycleLr_MAXLR = 0.0174
# First time
if Learning_rate_conf == 1:
    learning_rate_start = 8e-04
    learning_rate_max = 5e-03
    learning_rate_min = 5e-05
    learning_rate_rampup_epochs = 5
    learning_rate_sustain_epochs = 1
    learning_rate_exp_decay = .3
    #TEMP
    # learning_rate_start = 8e-04
    # learning_rate_max = 1e-02
    # learning_rate_min = 8e-04
    # learning_rate_rampup_epochs = 5
    # learning_rate_sustain_epochs = 3
    # learning_rate_exp_decay = .45
# 2th time
if Learning_rate_conf == 2:
    if Learning_rate_conf_SET2C == 1:
        learning_rate_start = 4.10e-06
        learning_rate_max = 4.10e-06
        learning_rate_min = 4.10e-06
        learning_rate_rampup_epochs = 0
        learning_rate_sustain_epochs = 0
        learning_rate_exp_decay = .1
        
    elif Learning_rate_conf_SET2C == 2:
        learning_rate_start = 4e-07
        learning_rate_max = 4e-07
        learning_rate_min = 4e-07
        learning_rate_rampup_epochs = 0
        learning_rate_sustain_epochs = 0
        learning_rate_exp_decay = .1
    
    elif Learning_rate_conf_SET2C == 3:
        learning_rate_start = 5e-04
        learning_rate_max = 5e-04
        learning_rate_min = 5e-04
        learning_rate_rampup_epochs = 0
        learning_rate_sustain_epochs = 0
        learning_rate_exp_decay = .1
# Function to build learning rate schedule
if Learning_rate_conf in [1,2]:
    def build_learning_rate_fn(lr_start=learning_rate_start,
                            lr_max=learning_rate_max,
                            lr_min=learning_rate_min,
                            lr_rampup_epochs=learning_rate_rampup_epochs,
                            lr_sustain_epochs=learning_rate_sustain_epochs,
                            lr_exp_decay=learning_rate_exp_decay):    
        lr_max = lr_max * tf.distribute.get_strategy().num_replicas_in_sync
        def learning_rate_fn(epoch):
            if epoch < lr_rampup_epochs:
                lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start
            elif epoch < lr_rampup_epochs + lr_sustain_epochs:
                lr = lr_max
            else:
                lr = (lr_max - lr_min) *\
                    lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min
            return lr
        return learning_rate_fn
#WTD_augmentation
if WTD_augmentation:
    print_Color('Using WTD_augmentation...', ['yellow'])
    def TF_add_image_grain(image, intensity = 0.01):
        # Generate random noise array in the range [0, 1]
        noise = tf.random.uniform(shape=tf.shape(image), minval=0, maxval=1, dtype=tf.float32)

        # Scale the noise array
        scaled_noise = noise * intensity
        
        # Add the noise to the image
        noisy_image = tf.math.add(image, scaled_noise)

        # Clip
        if RANGE_NOM:
            noisy_image = tf.clip_by_value(noisy_image, -1.0, 1.0)
        else:
            noisy_image = tf.clip_by_value(noisy_image, 0.0, 255.0)

        return noisy_image
    # Function to augment images
    def augment_images(image, label):
        image = tf.image.random_flip_left_right(image)
        image = tf.image.random_flip_up_down(image)
        image = tf.image.random_contrast(image, 0.2, 1.8)
        image = tf.image.random_brightness(image, max_delta=0.3)
        # Random intensity between 0 and 0.04
        intensity = random.uniform(0, 0.04)
        image = TF_add_image_grain(image, intensity=intensity)
        # Add random rotation
        # image = tf.image.rot90(image, k=random.randint(0, 3)) 
        return image, label

    # Create TensorFlow dataset
    AUTO = tf.data.experimental.AUTOTUNE
    train_dataset = (
        tf.data.Dataset.from_tensor_slices((x_train, y_train))
        .map(augment_images, num_parallel_calls=AUTO)
        .repeat()
        .shuffle(2048)
        .batch(Conf_batch_size)
        .prefetch(AUTO)
    )

# Calculate steps per epoch
steps_per_epoch_train = len(x_train) // Conf_batch_size

# Set up callbacks
class EpochEndMON(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        optimizer = self.model.optimizer
        if hasattr(optimizer, 'lr'):
            lr = tf.keras.backend.get_value(optimizer.lr)
            print(f'\nLearning rate for epoch {epoch+1} is {lr}')
        if hasattr(optimizer, 'momentum'):
            momentum = tf.keras.backend.get_value(optimizer.momentum)
            print(f'Momentum for epoch {epoch+1} is {momentum}')
        if logs:
            val_loss = logs.get('val_loss')
            val_acc = logs.get('val_accuracy')
            print(f'Validation loss for epoch {epoch+1} is {val_loss}')
            print(f'Validation accuracy for epoch {epoch+1} is {val_acc}')

        print_Color_V2(f'`red`<!--------------------------------------|Epoch`yellow` [{epoch+1}]`red` End|--------------------------------------!> `green`PBEâ†“', start_char='`', end_char='`')

# Instantiate the callback
EpochEndMON_callback = EpochEndMON()
if Learning_rate_conf in [1,2]:
    learning_rate_fn = build_learning_rate_fn()
    learning_rate_schedule = LearningRateScheduler(learning_rate_fn, verbose=1)
else:
    learning_rate_schedule = OneCycleLr(max_lr=OneCycleLr_MAXLR, steps_per_epoch=steps_per_epoch_train, epochs=20)
if SAVE_TYPE == 'TF':
    checkpoint_BVAC = ModelCheckpoint('models\\Temp\\bestVAC_model', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)
    checkpoint_BVL = ModelCheckpoint('models\\Temp\\bestVL_model', monitor='val_loss', mode='min', save_best_only=True, verbose=1)
else:
    checkpoint_BVAC = ModelCheckpoint('models\\Temp\\bestVAC_model.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)
    checkpoint_BVL = ModelCheckpoint('models\\Temp\\bestVL_model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)
early_stopping = EarlyStopping(monitor='val_accuracy', patience=8, verbose=1, restore_best_weights=True)
log_dir = 'logs/fit/' + datetime.datetime.now().strftime('y%Y_m%m_d%d-h%H_m%M_s%S')
TensorBoard_update_freq = 'batch' if TensorBoard_UF == 2 else 'epoch'
tensorboard_callback = TensorBoard(log_dir=log_dir, write_images=True, histogram_freq=1, update_freq=TensorBoard_update_freq)

# Train the model
print('Log dir:', log_dir)
#MInfo
print("Input Shape:", model.input_shape)
print("Output Shape:", model.output_shape)
print("Loss Function:", model.loss)
print('Training the model...\n')
if WTD_augmentation:
    history = model.fit(train_dataset,
                        epochs=256,
                        steps_per_epoch=steps_per_epoch_train,
                        batch_size=Conf_batch_size,
                        validation_data=(x_test, y_test),
                        verbose='auto',
                        callbacks=[early_stopping,
                                tensorboard_callback,
                                learning_rate_schedule,
                                checkpoint_BVAC,
                                checkpoint_BVL,
                                EpochEndMON_callback])
else:
    history = model.fit(x_train,
                        y_train,
                        epochs=256,
                        batch_size=Conf_batch_size,
                        validation_data=(x_test, y_test),
                        verbose='auto',
                        callbacks=[early_stopping,
                                tensorboard_callback,
                                learning_rate_schedule,
                                checkpoint_BVAC,
                                checkpoint_BVL,
                                EpochEndMON_callback])
print('Training done.\n')