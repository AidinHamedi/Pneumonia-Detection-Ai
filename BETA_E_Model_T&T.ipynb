{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras/TF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    }
   },
   "outputs": [],
   "source": [
    "CPU_only = False # True to Force TF to use the cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pylibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "if CPU_only:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import cv2\n",
    "import glob \n",
    "import keras\n",
    "import pprint\n",
    "import random\n",
    "import shutil\n",
    "import gzip\n",
    "import glob\n",
    "import pickle\n",
    "import datetime\n",
    "import subprocess\n",
    "import gpu_control\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from hyperas import optim\n",
    "# import tensorflow_addons as tfa\n",
    "from keras_adabound import AdaBound\n",
    "from importlib import reload\n",
    "from keras.losses import categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "from model_profiler import model_profiler\n",
    "from keras_gradient_noise import add_gradient_noise\n",
    "from keras.optimizers import SGD, Adam, Adagrad, Adadelta, Nadam, RMSprop, Adamax\n",
    "# from tensorflow_addons.optimizers import Yogi\n",
    "from adabelief_tf import AdaBeliefOptimizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.regularizers import l2\n",
    "from keras.models import load_model\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from keras import Sequential\n",
    "from random import randint, choice, shuffle\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, Callback, LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D,\\\n",
    "    MaxPooling2D,\\\n",
    "        Flatten,\\\n",
    "            Dense,\\\n",
    "                Dropout,\\\n",
    "                    BatchNormalization,\\\n",
    "                        SeparableConv2D,\\\n",
    "                            Input, Concatenate,\\\n",
    "                                GlobalAveragePooling2D,\\\n",
    "                                    CuDNNLSTM, concatenate,\\\n",
    "                                        Reshape, Multiply\n",
    "# Utils\n",
    "from Utils.one_cycle import OneCycleLr\n",
    "from Utils.lr_find import LrFinder\n",
    "from Utils.print_color_V2_NEW import print_Color_V2\n",
    "from Utils.print_color_V1_OLD import print_Color\n",
    "from Utils.Other import *\n",
    "# Other\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for gpu_instance in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu_instance, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "# Directory paths# Directory paths for training, test and validation image data\n",
    "train_dir = 'Database\\\\Train\\\\Data\\\\train'\n",
    "test_dir = 'Database\\\\Train\\\\Data\\\\test'\n",
    "validation_dir = 'Database\\\\Train\\\\Data\\\\val'\n",
    "img_res = [224, 224, 3]\n",
    "# img_res = [324, 324, 3]\n",
    "# img_res = [224, 224, 3]\n",
    "# img_res = [384, 384, 3] # Very slow needs >=24Gb Vram for batch size of 1 (NR!)\n",
    "interpolation_order_IFG = 2\n",
    "categorical_IMP = True\n",
    "Make_EV_DATA = False\n",
    "R_fill_mode = True\n",
    "add_img_grain = True\n",
    "Save_TS = True\n",
    "Use_SMOTE = False # (⚠️Beta⚠️)\n",
    "ADBD = 1\n",
    "OP_HDC = False\n",
    "SL_EX = '_V1' # _NONOM_V1 | _V1 | _SDNP_V1\n",
    "LNTS = 0\n",
    "adjust_brightness_Mode = True\n",
    "RANGE_NOM = True # False for 0 to 255 True for 0 to 1 >> use False for models like ConvNeXtXLarge (⚠️deprecated⚠️)\n",
    "scale_data_NP_M = False # (⚠️deprecated⚠️)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Policy \"float32\">\n"
     ]
    }
   ],
   "source": [
    "SAVE_TYPE = 'H5'\n",
    "Use_mixed_float16 = False\n",
    "#Other\n",
    "if Use_mixed_float16:\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "else:\n",
    "    tf.keras.mixed_precision.set_global_policy('float32')\n",
    "    \n",
    "print(tf.keras.mixed_precision.global_policy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data processing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;33mUsing Def IDG...\u001b[0m\n",
      "Found 8818 images belonging to 2 classes.\n",
      "\u001b[0;33mLoading all images and labels into memory...\u001b[0m\n",
      "\u001b[0;33mMaking categorical data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mGenerating augmented data \u001b[0m\u001b[0;36m[\u001b[0m\u001b[0;32mADBD: \u001b[0m\u001b[0;31m1\u001b[0m\u001b[0;36m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      ">   Generating ADB[1/1]...\n",
      ">   ├───Applying adaptive histogram equalization...\n",
      ">   ├───Adaptive histogram equalization clip limit = 1.6\n",
      ">   └───Adding the Generated ADB...\n",
      "\u001b[0;33mNormalizing image data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0mData type: \u001b[0m\u001b[0;32mfloat32\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0mRGB Range: \u001b[0m\u001b[0;34mMin = 0.0\u001b[0m\u001b[0m | \u001b[0m\u001b[0;31mMax = 1.0\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0mLabel ratio: \u001b[0m\u001b[0;31m64.36% PNEUMONIA \u001b[0m\u001b[0;35m| \u001b[0m\u001b[0;32m35.64% NORMAL\u001b[0m\n",
      "\u001b[0;33mSetting LNTS...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0mOriginal num_samples: \u001b[0m\u001b[0;32m17636\u001b[0m\n",
      "\u001b[0;33mshuffling data...\u001b[0m\n",
      "\u001b[0;33mSaving TS...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0mSample dir: \u001b[0m\u001b[0;32mSamples/TSR400_y2023_m12_d21-h22_m11_s40\u001b[0m\n",
      "\u001b[0;32mDone.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#normalize_TO_RANGE\n",
    "def normalize_TO_RANGE(arr, min_val, max_val):\n",
    "  arr = arr.astype('float32')\n",
    "  arr = (arr - arr.min()) / (arr.max() - arr.min())\n",
    "  arr = arr * (max_val - min_val) + min_val\n",
    "  return arr\n",
    "#scale_data\n",
    "def scale_data_NP(data):\n",
    "    if scale_data_NP_M:\n",
    "        data = data.astype('float32')\n",
    "        data = (data - 127.5) / 127.5\n",
    "        return data\n",
    "    else:\n",
    "        return data / 255\n",
    "#add_image_grain\n",
    "def add_image_grain(image, intensity = 0.01):\n",
    "    # Generate random noise array\n",
    "    noise = np.random.randint(0, 255, size=image.shape, dtype=np.uint8)\n",
    "\n",
    "    # Scale the noise array\n",
    "    scaled_noise = (noise * intensity).astype(np.float32)\n",
    "    # Add the noise to the image\n",
    "    noisy_image = cv2.add(image, scaled_noise)\n",
    "\n",
    "    return noisy_image\n",
    "#apply_clahe_rgb_array\n",
    "def apply_clahe_rgb_array(images, clip_limit=1.8, tile_grid_size=(8, 8)):\n",
    "    # Create a CLAHE object\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "    \n",
    "    # Iterate over each image in the array\n",
    "    for i in range(len(images)):\n",
    "        # Split the image into color channels\n",
    "        b, g, r = cv2.split(images[i])\n",
    "        \n",
    "        # Convert the channels to the appropriate format\n",
    "        b = cv2.convertScaleAbs(b)\n",
    "        g = cv2.convertScaleAbs(g)\n",
    "        r = cv2.convertScaleAbs(r)\n",
    "        \n",
    "        # Apply adaptive histogram equalization to each channel\n",
    "        equalized_b = clahe.apply(b)\n",
    "        equalized_g = clahe.apply(g)\n",
    "        equalized_r = clahe.apply(r)\n",
    "\n",
    "        # Merge the equalized channels back into an image\n",
    "        equalized_image = cv2.merge((equalized_b, equalized_g, equalized_r))\n",
    "\n",
    "        # Replace the original image with the equalized image in the array\n",
    "        images[i] = equalized_image\n",
    "\n",
    "    return images\n",
    "#noise_func\n",
    "def noise_func(image):\n",
    "    noise_type = np.random.choice(['L1', 'L2', 'L3', 'none'])\n",
    "    new_image = np.copy(image)\n",
    "    \n",
    "    if noise_type == 'L3':\n",
    "        intensityL2 = random.uniform(-0.05, 0.05)\n",
    "        intensityL1 = random.uniform(-0.04, 0.04)\n",
    "    else:\n",
    "        intensityL2 = random.uniform(-0.06, 0.06)\n",
    "        intensityL1 = random.uniform(-0.04, 0.04)\n",
    "    \n",
    "    block_size_L1 = random.randint(16, 32)\n",
    "    block_size_L2 = random.randint(32, 64)\n",
    "    \n",
    "    if noise_type == 'L2' or noise_type == 'L3':\n",
    "        for i in range(0, image.shape[0], block_size_L2):\n",
    "            for j in range(0, image.shape[1], block_size_L2):\n",
    "                block = image[i:i+block_size_L2, j:j+block_size_L2]\n",
    "                block = (np.random.rand() * intensityL2 + 1) * block\n",
    "                new_image[i:i+block_size_L2, j:j+block_size_L2] = block\n",
    "        image = new_image      \n",
    "        \n",
    "    if noise_type == 'L1' or noise_type == 'L3': \n",
    "        for i in range(0, image.shape[0], block_size_L1):\n",
    "            for j in range(0, image.shape[1], block_size_L1):\n",
    "                block = image[i:i+block_size_L1, j:j+block_size_L1]\n",
    "                block = (np.random.rand() * intensityL1 + 1) * block\n",
    "                new_image[i:i+block_size_L1, j:j+block_size_L1] = block\n",
    "    \n",
    "    if add_img_grain:\n",
    "        intensity = random.uniform(0, 0.045)  # Random intensity between 0 and 0.026\n",
    "        new_image = add_image_grain(new_image, intensity=intensity)\n",
    "    return new_image\n",
    "#shuffle_data\n",
    "def shuffle_data(x, y):\n",
    "    indices = np.arange(x.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "    return x, y\n",
    "#save_images_to_dir\n",
    "def save_images_to_dir(images, labels, dir_path):\n",
    "    # create the directory if it doesn't exist\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    # iterate over the images and labels\n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "        # get the class label\n",
    "        class_label = np.argmax(label)\n",
    "        # create the file path\n",
    "        file_path = os.path.join(dir_path, f'image_{i}_class_{class_label}.png')\n",
    "        # save the image to the file path\n",
    "        plt.imsave(file_path, image.squeeze())\n",
    "    # compress the directory\n",
    "    shutil.make_archive(dir_path, 'gztar', dir_path)\n",
    "    # remove the original directory\n",
    "    shutil.rmtree(dir_path)\n",
    "# Create an ImageDataGenerator for the training set\n",
    "if OP_HDC:\n",
    "    print_Color('Using OP_HDC IDG...', ['yellow'])\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        rotation_range=179,\n",
    "        zoom_range=0.24, \n",
    "        shear_range=0.22,\n",
    "        width_shift_range=0.21,\n",
    "        brightness_range=(0.86, 1.13),\n",
    "        height_shift_range=0.21,\n",
    "        channel_shift_range=100,\n",
    "        featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        interpolation_order=interpolation_order_IFG,\n",
    "        fill_mode='nearest', # constant\n",
    "        preprocessing_function=noise_func\n",
    "    )\n",
    "else:\n",
    "    print_Color('Using Def IDG...', ['yellow'])\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        rotation_range=179,\n",
    "        zoom_range=0.26, \n",
    "        shear_range=0.25,\n",
    "        width_shift_range=0.25,\n",
    "        brightness_range=(0.8, 1.15),\n",
    "        height_shift_range=0.25,\n",
    "        channel_shift_range=100,\n",
    "        featurewise_center=False,\n",
    "        interpolation_order=interpolation_order_IFG,\n",
    "        featurewise_std_normalization=False,\n",
    "        fill_mode='nearest', # constant\n",
    "        preprocessing_function=noise_func\n",
    "    )\n",
    "train_datagen_SM = ImageDataGenerator(\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.07, \n",
    "    shear_range=0.07,\n",
    "    width_shift_range=0.07,\n",
    "    brightness_range=(0.99, 1.01),\n",
    "    height_shift_range=0.07,\n",
    "    channel_shift_range=0,\n",
    "    featurewise_center=False,\n",
    "    interpolation_order=interpolation_order_IFG,\n",
    "    featurewise_std_normalization=False\n",
    ")\n",
    "# Create an iterator for the training set\n",
    "train_generator_SM = train_datagen_SM.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_res[0], img_res[1]),\n",
    "    batch_size=sum([len(files) for r, d, files in os.walk(train_dir)]),\n",
    "    class_mode='binary')\n",
    "# Create an ImageDataGenerator for the validation set (OP)\n",
    "if Make_EV_DATA:\n",
    "    val_datagen = ImageDataGenerator(\n",
    "        horizontal_flip=False,\n",
    "        zoom_range = 0.01, \n",
    "        width_shift_range=0.01, \n",
    "        interpolation_order=interpolation_order_IFG,\n",
    "        height_shift_range=0.01)\n",
    "\n",
    "    # Create an iterator for the validation set\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(img_res[0], img_res[1]),\n",
    "        batch_size=sum([len(files) for r, d, files in os.walk(validation_dir)]),\n",
    "        class_mode='binary',\n",
    "        color_mode='rgb')\n",
    "\n",
    "    # Create an ImageDataGenerator for the test set\n",
    "    test_datagen = ImageDataGenerator(\n",
    "        horizontal_flip=False,\n",
    "        zoom_range = 0.01, \n",
    "        width_shift_range=0.01, \n",
    "        interpolation_order=interpolation_order_IFG,\n",
    "        height_shift_range=0.01)\n",
    "\n",
    "    # Create an iterator for the test set\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(img_res[0], img_res[1]),\n",
    "        batch_size=sum([len(files) for r, d, files in os.walk(test_dir)]),\n",
    "        class_mode='binary',\n",
    "        color_mode='rgb')\n",
    "# Load all images and labels into memory\n",
    "print_Color('Loading all images and labels into memory...', ['yellow'])\n",
    "x_train, y_train = next(iter(train_generator_SM))\n",
    "if Make_EV_DATA:\n",
    "    x_val, y_val = next(iter(val_generator))\n",
    "    x_test, y_test = next(iter(test_generator))\n",
    "# fit parameters from data\n",
    "# train_datagen.fit(x_train)\n",
    "#to_categorical (TEMP)\n",
    "if categorical_IMP:\n",
    "    print_Color('Making categorical data...', ['yellow'])\n",
    "    y_train = to_categorical(y_train, num_classes=2)\n",
    "    if Make_EV_DATA:\n",
    "        y_val = to_categorical(y_val, num_classes=2)\n",
    "        y_test = to_categorical(y_test, num_classes=2)\n",
    "# Use_SMOTE\n",
    "if Use_SMOTE:\n",
    "    print_Color('SMOTE...', ['yellow'])\n",
    "    # Convert y_train from one-hot encoding to label encoding\n",
    "    y_train_label_encoded = np.argmax(y_train, axis=1)\n",
    "\n",
    "    # Print the original label distribution\n",
    "    unique, counts = np.unique(y_train_label_encoded, return_counts=True)\n",
    "    print_Color(f'~*- Original label distribution: ~*{dict(zip(unique, counts))}', ['normal', 'blue'], advanced_mode=True)\n",
    "\n",
    "    # Use SMOTE to oversample the minority class\n",
    "    smote = SMOTE(random_state=42)\n",
    "    x_train_res, y_train_res_label_encoded = smote.fit_resample(x_train.reshape(x_train.shape[0], -1), y_train_label_encoded)\n",
    "\n",
    "    # Print the resampled label distribution\n",
    "    unique_res, counts_res = np.unique(y_train_res_label_encoded, return_counts=True)\n",
    "    print_Color(f'~*- Resampled label distribution: ~*{dict(zip(unique_res, counts_res))}', ['normal', 'blue'], advanced_mode=True)\n",
    "\n",
    "    # Reshape x_train_res back to the original x_train shape\n",
    "    x_train_res = x_train_res.reshape(-1, x_train.shape[1], x_train.shape[2], x_train.shape[3])\n",
    "\n",
    "    # Convert y_train_res from label encoding back to one-hot encoding\n",
    "    y_train_res = to_categorical(y_train_res_label_encoded)\n",
    "\n",
    "    # Calculate the ratio of two labels after resampling\n",
    "    pneumonia_count = np.sum(y_train_res[:, 1])\n",
    "    total_count = y_train_res.shape[0]\n",
    "    label_ratio_res = pneumonia_count / total_count\n",
    "    label_ratio_percentage_res = label_ratio_res * 100\n",
    "\n",
    "    # Replace the original data with the resampled data\n",
    "    x_train = x_train_res\n",
    "    y_train = y_train_res\n",
    "\n",
    "    # Delete the resampled data to free up memory\n",
    "    del x_train_res, y_train_res_label_encoded, y_train_res\n",
    "# Generating augmented data\n",
    "print_Color(f'~*Generating augmented data ~*[~*ADBD: ~*{str(ADBD)}~*]~*...',\n",
    "            ['yellow', 'cyan', 'green', 'red', 'cyan', 'yellow'],\n",
    "            advanced_mode=True)\n",
    "if ADBD > 0:\n",
    "    for i in range(ADBD):\n",
    "        # ADB_clip_limit Scheduler>>>\n",
    "        if i == 0:\n",
    "            ADB_clip_limit = 1.6\n",
    "        else:\n",
    "            #V1>>>\n",
    "            CL_SLM = 2.4\n",
    "            ADB_clip_limit = max(2 / (i + 1)**CL_SLM, 0.05)\n",
    "            # Try it in win graphing calculator copy and paste:\n",
    "            #  ┌-------------┬--┬---------------┐\n",
    "            #  │ 𝑦=2/(𝑥+1)^𝑧 ├OR┤ 𝑦=2/(𝑥+1)^2.4 │\n",
    "            #  └-------------┴--┴---------------┘\n",
    "            #V2>>>\n",
    "            # CL_SLM_2 = 1.4\n",
    "            # CL_SLM_Start_2 = 2\n",
    "            # ADB_clip_limit = CL_SLM_Start_2/(i+1)**(i+CL_SLM_2) \n",
    "            # Try it in win graphing calculator copy and paste:\n",
    "            #  ┌-----------------┬--┬-------------------┐\n",
    "            #  │ 𝑦=2/(𝑥+1)^(𝑥+𝑉) ├OR┤ 𝑦=2/(𝑥+1)^(𝑥+1.4) │\n",
    "            #  └-----------------┴--┴-------------------┘\n",
    "        print(f'>   Generating ADB[{i+1}/{ADBD}]...')\n",
    "        # prepare an iterators to scale images\n",
    "        train_iterator = train_datagen.flow(x_train, y_train, batch_size=len(x_train))\n",
    "\n",
    "        # get augmented data\n",
    "        x_train_augmented, y_train_augmented = train_iterator.next()\n",
    "        print(f'>   ├───Applying adaptive histogram equalization...')\n",
    "        print(f'>   ├───Adaptive histogram equalization clip limit = {round(ADB_clip_limit, 2)}')\n",
    "        x_train_augmented = np.clip(x_train_augmented, 0, 255) \n",
    "        #print_Color(f'~*>   |---Grayscale range: ~*Min = {np.min(x_train_augmented)}~* | ~*Max = {np.max(x_train_augmented)}', ['normal', 'blue', 'normal', 'red'], advanced_mode=True)\n",
    "        x_train_augmented = apply_clahe_rgb_array(x_train_augmented, clip_limit=ADB_clip_limit) # compensating the image info loss\n",
    "        print(f'>   └───Adding the Generated ADB...')\n",
    "        # append augmented data to original data\n",
    "        x_train = np.concatenate([x_train, x_train_augmented])\n",
    "        y_train = np.concatenate([y_train, y_train_augmented])\n",
    "        #free up memory\n",
    "        del y_train_augmented\n",
    "        del x_train_augmented\n",
    "# normalizing \n",
    "print_Color('Normalizing image data...', ['yellow'])\n",
    "x_train = np.clip(x_train, 0, 255)  \n",
    "if RANGE_NOM:\n",
    "    x_train = scale_data_NP(x_train)\n",
    "y_train = np.array(y_train) \n",
    "if Make_EV_DATA:\n",
    "    x_test = np.clip(x_test, 0, 255)  \n",
    "    x_val = np.clip(x_val, 0, 255)  \n",
    "    if RANGE_NOM:\n",
    "        x_val = scale_data_NP(x_val)\n",
    "    y_val = np.array(y_val)  \n",
    "    if RANGE_NOM:\n",
    "        x_test = scale_data_NP(x_test)\n",
    "    y_test = np.array(y_test) \n",
    "# Check the data type of image data\n",
    "print_Color(f'~*Data type: ~*{x_train.dtype}', ['normal', 'green'], advanced_mode=True)\n",
    "# Check the range of image data\n",
    "print_Color(f'~*RGB Range: ~*Min = {np.min(x_train)}~* | ~*Max = {np.max(x_train)}', ['normal', 'blue', 'normal', 'red'], advanced_mode=True)\n",
    "# Calculate the ratio of two labels\n",
    "if categorical_IMP:\n",
    "    label_sums = np.sum(y_train, axis=0)\n",
    "    label_ratio = label_sums / (np.sum(y_train) + 1e-10)\n",
    "    label_ratio_percentage = label_ratio * 100\n",
    "    print_Color(f'~*Label ratio: ~*{100 - label_ratio_percentage[0]:.2f}% PNEUMONIA ~*| ~*{label_ratio_percentage[0]:.2f}% NORMAL',\n",
    "                ['normal', 'red', 'magenta', 'green'], advanced_mode=True)    \n",
    "print_Color('Setting LNTS...', ['yellow'])\n",
    "# Get the total number of samples in the arrays\n",
    "num_samples = x_train.shape[0]\n",
    "print_Color(f'~*Original num_samples: ~*{num_samples}', ['normal', 'green'], advanced_mode=True)\n",
    "if LNTS != 0:\n",
    "    print_Color(f'~*Applying LNTS of: ~*{LNTS}', ['normal', 'green'], advanced_mode=True)\n",
    "    print_Color(f'~*SNC: ~*{num_samples - LNTS}', ['normal', 'green'], advanced_mode=True)\n",
    "    # Generate random indices to select LNTS samples\n",
    "    indices = np.random.choice(num_samples, size=LNTS, replace=False)\n",
    "    # Select the samples using the generated indices\n",
    "    x_selected = x_train[indices]\n",
    "    y_selected = y_train[indices]\n",
    "    x_train = x_selected\n",
    "    y_train = y_selected\n",
    "    #free up memory\n",
    "    del x_selected\n",
    "    del y_selected\n",
    "    del indices\n",
    "    #Debug\n",
    "    num_samples = x_train.shape[0]\n",
    "    print_Color(f'~*New num_samples: ~*{num_samples}', ['normal', 'green'], advanced_mode=True)\n",
    "# Shuffle the training data\n",
    "print_Color('shuffling data...', ['yellow'])\n",
    "x_train, y_train = shuffle_data(x_train, y_train)\n",
    "#save_images_to_dir    \n",
    "if Save_TS:\n",
    "    print_Color('Saving TS...', ['yellow'])\n",
    "    SITD = np.random.choice(num_samples, size=400, replace=False)\n",
    "    S_dir = 'Samples/TSR400_' + datetime.datetime.now().strftime('y%Y_m%m_d%d-h%H_m%M_s%S')\n",
    "    print_Color(f'~*Sample dir: ~*{S_dir}', ['normal', 'green'], advanced_mode=True)\n",
    "    if RANGE_NOM:\n",
    "        if scale_data_NP_M:\n",
    "            save_images_to_dir((x_train[SITD] + 1) / 2.0, y_train[SITD], S_dir)\n",
    "        else:\n",
    "            save_images_to_dir(x_train[SITD], y_train[SITD], S_dir)\n",
    "    else:\n",
    "        save_images_to_dir(x_train[SITD] / 255, y_train[SITD], S_dir)\n",
    "print_Color('Done.', ['green'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save EV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'Database\\\\Test\\\\Data\\\\x_val{SL_EX}.npy', x_val)\n",
    "np.save(f'Database\\\\Test\\\\Data\\\\y_val{SL_EX}.npy', y_val)\n",
    "np.save(f'Database\\\\Test\\\\Data\\\\x_test{SL_EX}.npy', x_test)\n",
    "np.save(f'Database\\\\Test\\\\Data\\\\y_test{SL_EX}.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load EV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "x_val = np.load(f'Database\\\\Test\\\\Data\\\\x_val{SL_EX}.npy')\n",
    "y_val = np.load(f'Database\\\\Test\\\\Data\\\\y_val{SL_EX}.npy')\n",
    "x_test = np.load(f'Database\\\\Test\\\\Data\\\\x_test{SL_EX}.npy')\n",
    "y_test = np.load(f'Database\\\\Test\\\\Data\\\\y_test{SL_EX}.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analyzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Select a subset of your data\n",
    "subset_size_pixels = 10  # Change this to the size of the subset you want for individual pixels\n",
    "subset_size_mean = 200  # Change this to the size of the subset you want for mean RGB values\n",
    "indices_pixels = np.random.choice(x_train.shape[0], subset_size_pixels, replace=False)\n",
    "indices_mean = np.random.choice(x_train.shape[0], subset_size_mean, replace=False)\n",
    "subset_pixels = x_train[indices_pixels]\n",
    "subset_mean = x_train[indices_mean]\n",
    "\n",
    "# Reshape the data for calculating Z-scores\n",
    "reshaped_data_pixels = subset_pixels.reshape(-1, subset_pixels.shape[-1])\n",
    "reshaped_data_mean = subset_mean.reshape(-1, subset_mean.shape[-1])\n",
    "\n",
    "# Calculate the mean intensity\n",
    "mean_intensity_pixels = reshaped_data_pixels.mean(axis=-1)\n",
    "mean_intensity_mean = reshaped_data_mean.mean(axis=-1)\n",
    "\n",
    "# Stack the mean intensity with the reshaped data\n",
    "data_with_mean_pixels = np.hstack([reshaped_data_pixels, mean_intensity_pixels.reshape(-1, 1)])\n",
    "data_with_mean_mean = np.hstack([reshaped_data_mean, mean_intensity_mean.reshape(-1, 1)])\n",
    "\n",
    "# Calculate Z-scores\n",
    "z_scores_pixels = np.abs(zscore(data_with_mean_pixels, axis=0))\n",
    "z_scores_mean = np.abs(zscore(data_with_mean_mean, axis=0))\n",
    "\n",
    "# Identify outliers\n",
    "outliers_pixels = np.where(z_scores_pixels > 3)\n",
    "outliers_mean = np.where(z_scores_mean > 3)\n",
    "\n",
    "# Create a 3D scatter plot for RGB channels\n",
    "fig = plt.figure(figsize=(10, 20))\n",
    "\n",
    "# Plot for individual pixels\n",
    "ax = fig.add_subplot(211, projection='3d')\n",
    "ax.scatter(z_scores_pixels[:, 0], z_scores_pixels[:, 1], z_scores_pixels[:, 2], alpha=0.1)\n",
    "ax.scatter(z_scores_pixels[outliers_pixels[0], 0], z_scores_pixels[outliers_pixels[0], 1], z_scores_pixels[outliers_pixels[0], 2], color='red')\n",
    "ax.set_title('Z-Score Scatter Plot for Individual Pixels')\n",
    "ax.set_xlabel('Red')\n",
    "ax.set_ylabel('Green')\n",
    "ax.set_zlabel('Blue')\n",
    "\n",
    "# Plot for mean RGB values\n",
    "ax = fig.add_subplot(212, projection='3d')\n",
    "ax.scatter(z_scores_mean[:, 0], z_scores_mean[:, 1], z_scores_mean[:, 2], alpha=0.1)\n",
    "ax.scatter(z_scores_mean[outliers_mean[0], 0], z_scores_mean[outliers_mean[0], 1], z_scores_mean[outliers_mean[0], 2], color='red')\n",
    "ax.set_title('Z-Score Scatter Plot for Mean RGB Values')\n",
    "ax.set_xlabel('Red')\n",
    "ax.set_ylabel('Green')\n",
    "ax.set_zlabel('Blue')\n",
    "\n",
    "# Density plot of the mean intensity\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.kdeplot(data=z_scores_pixels[:, -1], fill=True)\n",
    "plt.title('Density Plot of Z-Scores for Mean Intensity for Individual Pixels')\n",
    "plt.xlabel('Z-Score')\n",
    "\n",
    "sns.kdeplot(data=z_scores_mean[:, -1], fill=True)\n",
    "plt.title('Density Plot of Z-Scores for Mean Intensity for Mean RGB Values')\n",
    "plt.xlabel('Z-Score')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rev1\n",
    "```\n",
    "recommended: ⚠️\n",
    "statuses: Ready\n",
    "Working: ✅\n",
    "Max fine tuned acc: ≅95.1\n",
    "Max fine tuned acc TLRev2: N/A\n",
    "type: transfer learning>>>(EfficientNetB7)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import EfficientNetB7\n",
    "\n",
    "EfficientNet_M = EfficientNetB7(include_top=True, input_shape=(img_res[0], img_res[1], img_res[2]), weights=None, classes=2, classifier_activation='softmax')\n",
    "# define new model\n",
    "model = Model(inputs=EfficientNet_M.inputs, outputs=EfficientNet_M.outputs)\n",
    "\n",
    "# compile model\n",
    "opt = SGD(momentum=0.9)\n",
    "# opt = SGD(learning_rate=0.008, momentum=0.85, decay=0.001)\n",
    "# opt = Adam()\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rev1.1\n",
    "```\n",
    "recommended: ❌\n",
    "statuses: S.Ready (can improve)\n",
    "Working: ❌\n",
    "Max fine tuned acc: ≅93.2\n",
    "Max fine tuned acc TLRev2: N/A\n",
    "type: transfer learning>>>(ConvNeXtLarge)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ConvNeXtLarge\n",
    "\n",
    "ConvNeXtLarge_M = ConvNeXtLarge(include_top=False, input_shape=(img_res[0], img_res[1], img_res[2]), weights='imagenet', classes=2, classifier_activation='softmax', include_preprocessing=False)\n",
    "# define new model\n",
    "model = Model(inputs=ConvNeXtLarge_M.inputs, outputs=ConvNeXtLarge_M.outputs)\n",
    "\n",
    "# compile model\n",
    "opt = SGD(momentum=0.9)\n",
    "# opt = SGD(learning_rate=0.008, momentum=0.85, decay=0.001)\n",
    "# opt = Adam()\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "### Rev1.2\n",
    "```\n",
    "recommended: ✅\n",
    "statuses: Ready\n",
    "Working: ✅\n",
    "Max fine tuned acc: 95.3\n",
    "Max fine tuned acc TLRev2: 96.96\n",
    "type: transfer learning>>>(EfficientNetB7::CCL)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the model...\n",
      "Total layers in the base model:  806\n",
      "Freezing 0 layers in the base model...\n",
      "Percentage of the base model that is frozen: 0.00%\n",
      "Total model layers:  814\n",
      "Model: \"model\"\n",
      "_____________________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     Trainable  \n",
      "=============================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               Y          \n",
      "                                )]                                                                           \n",
      "                                                                                                             \n",
      " stem_conv (Conv2D)             (None, 112, 112, 64  1728        ['input_1[0][0]']                Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " stem_bn (BatchNormalization)   (None, 112, 112, 64  256         ['stem_conv[0][0]']              Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " stem_activation (Activation)   (None, 112, 112, 64  0           ['stem_bn[0][0]']                Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block1a_dwconv (DepthwiseConv2  (None, 112, 112, 64  576        ['stem_activation[0][0]']        Y          \n",
      " D)                             )                                                                            \n",
      "                                                                                                             \n",
      " block1a_bn (BatchNormalization  (None, 112, 112, 64  256        ['block1a_dwconv[0][0]']         Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block1a_activation (Activation  (None, 112, 112, 64  0          ['block1a_bn[0][0]']             Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block1a_se_squeeze (GlobalAver  (None, 64)          0           ['block1a_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block1a_se_reshape (Reshape)   (None, 1, 1, 64)     0           ['block1a_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block1a_se_reduce (Conv2D)     (None, 1, 1, 16)     1040        ['block1a_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block1a_se_expand (Conv2D)     (None, 1, 1, 64)     1088        ['block1a_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block1a_se_excite (Multiply)   (None, 112, 112, 64  0           ['block1a_activation[0][0]',     Y          \n",
      "                                )                                 'block1a_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block1a_project_conv (Conv2D)  (None, 112, 112, 32  2048        ['block1a_se_excite[0][0]']      Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block1a_project_bn (BatchNorma  (None, 112, 112, 32  128        ['block1a_project_conv[0][0]']   Y          \n",
      " lization)                      )                                                                            \n",
      "                                                                                                             \n",
      " block1b_dwconv (DepthwiseConv2  (None, 112, 112, 32  288        ['block1a_project_bn[0][0]']     Y          \n",
      " D)                             )                                                                            \n",
      "                                                                                                             \n",
      " block1b_bn (BatchNormalization  (None, 112, 112, 32  128        ['block1b_dwconv[0][0]']         Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block1b_activation (Activation  (None, 112, 112, 32  0          ['block1b_bn[0][0]']             Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block1b_se_squeeze (GlobalAver  (None, 32)          0           ['block1b_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block1b_se_reshape (Reshape)   (None, 1, 1, 32)     0           ['block1b_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block1b_se_reduce (Conv2D)     (None, 1, 1, 8)      264         ['block1b_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block1b_se_expand (Conv2D)     (None, 1, 1, 32)     288         ['block1b_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block1b_se_excite (Multiply)   (None, 112, 112, 32  0           ['block1b_activation[0][0]',     Y          \n",
      "                                )                                 'block1b_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block1b_project_conv (Conv2D)  (None, 112, 112, 32  1024        ['block1b_se_excite[0][0]']      Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block1b_project_bn (BatchNorma  (None, 112, 112, 32  128        ['block1b_project_conv[0][0]']   Y          \n",
      " lization)                      )                                                                            \n",
      "                                                                                                             \n",
      " block1b_drop (FixedDropout)    (None, 112, 112, 32  0           ['block1b_project_bn[0][0]']     Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block1b_add (Add)              (None, 112, 112, 32  0           ['block1b_drop[0][0]',           Y          \n",
      "                                )                                 'block1a_project_bn[0][0]']                \n",
      "                                                                                                             \n",
      " block1c_dwconv (DepthwiseConv2  (None, 112, 112, 32  288        ['block1b_add[0][0]']            Y          \n",
      " D)                             )                                                                            \n",
      "                                                                                                             \n",
      " block1c_bn (BatchNormalization  (None, 112, 112, 32  128        ['block1c_dwconv[0][0]']         Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block1c_activation (Activation  (None, 112, 112, 32  0          ['block1c_bn[0][0]']             Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block1c_se_squeeze (GlobalAver  (None, 32)          0           ['block1c_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block1c_se_reshape (Reshape)   (None, 1, 1, 32)     0           ['block1c_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block1c_se_reduce (Conv2D)     (None, 1, 1, 8)      264         ['block1c_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block1c_se_expand (Conv2D)     (None, 1, 1, 32)     288         ['block1c_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block1c_se_excite (Multiply)   (None, 112, 112, 32  0           ['block1c_activation[0][0]',     Y          \n",
      "                                )                                 'block1c_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block1c_project_conv (Conv2D)  (None, 112, 112, 32  1024        ['block1c_se_excite[0][0]']      Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block1c_project_bn (BatchNorma  (None, 112, 112, 32  128        ['block1c_project_conv[0][0]']   Y          \n",
      " lization)                      )                                                                            \n",
      "                                                                                                             \n",
      " block1c_drop (FixedDropout)    (None, 112, 112, 32  0           ['block1c_project_bn[0][0]']     Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block1c_add (Add)              (None, 112, 112, 32  0           ['block1c_drop[0][0]',           Y          \n",
      "                                )                                 'block1b_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block1d_dwconv (DepthwiseConv2  (None, 112, 112, 32  288        ['block1c_add[0][0]']            Y          \n",
      " D)                             )                                                                            \n",
      "                                                                                                             \n",
      " block1d_bn (BatchNormalization  (None, 112, 112, 32  128        ['block1d_dwconv[0][0]']         Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block1d_activation (Activation  (None, 112, 112, 32  0          ['block1d_bn[0][0]']             Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block1d_se_squeeze (GlobalAver  (None, 32)          0           ['block1d_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block1d_se_reshape (Reshape)   (None, 1, 1, 32)     0           ['block1d_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block1d_se_reduce (Conv2D)     (None, 1, 1, 8)      264         ['block1d_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block1d_se_expand (Conv2D)     (None, 1, 1, 32)     288         ['block1d_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block1d_se_excite (Multiply)   (None, 112, 112, 32  0           ['block1d_activation[0][0]',     Y          \n",
      "                                )                                 'block1d_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block1d_project_conv (Conv2D)  (None, 112, 112, 32  1024        ['block1d_se_excite[0][0]']      Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block1d_project_bn (BatchNorma  (None, 112, 112, 32  128        ['block1d_project_conv[0][0]']   Y          \n",
      " lization)                      )                                                                            \n",
      "                                                                                                             \n",
      " block1d_drop (FixedDropout)    (None, 112, 112, 32  0           ['block1d_project_bn[0][0]']     Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block1d_add (Add)              (None, 112, 112, 32  0           ['block1d_drop[0][0]',           Y          \n",
      "                                )                                 'block1c_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block2a_expand_conv (Conv2D)   (None, 112, 112, 19  6144        ['block1d_add[0][0]']            Y          \n",
      "                                2)                                                                           \n",
      "                                                                                                             \n",
      " block2a_expand_bn (BatchNormal  (None, 112, 112, 19  768        ['block2a_expand_conv[0][0]']    Y          \n",
      " ization)                       2)                                                                           \n",
      "                                                                                                             \n",
      " block2a_expand_activation (Act  (None, 112, 112, 19  0          ['block2a_expand_bn[0][0]']      Y          \n",
      " ivation)                       2)                                                                           \n",
      "                                                                                                             \n",
      " block2a_dwconv (DepthwiseConv2  (None, 56, 56, 192)  1728       ['block2a_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block2a_bn (BatchNormalization  (None, 56, 56, 192)  768        ['block2a_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block2a_activation (Activation  (None, 56, 56, 192)  0          ['block2a_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block2a_se_squeeze (GlobalAver  (None, 192)         0           ['block2a_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block2a_se_reshape (Reshape)   (None, 1, 1, 192)    0           ['block2a_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2a_se_reduce (Conv2D)     (None, 1, 1, 8)      1544        ['block2a_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2a_se_expand (Conv2D)     (None, 1, 1, 192)    1728        ['block2a_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block2a_se_excite (Multiply)   (None, 56, 56, 192)  0           ['block2a_activation[0][0]',     Y          \n",
      "                                                                  'block2a_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block2a_project_conv (Conv2D)  (None, 56, 56, 48)   9216        ['block2a_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block2a_project_bn (BatchNorma  (None, 56, 56, 48)  192         ['block2a_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block2b_expand_conv (Conv2D)   (None, 56, 56, 288)  13824       ['block2a_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2b_expand_bn (BatchNormal  (None, 56, 56, 288)  1152       ['block2b_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block2b_expand_activation (Act  (None, 56, 56, 288)  0          ['block2b_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block2b_dwconv (DepthwiseConv2  (None, 56, 56, 288)  2592       ['block2b_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block2b_bn (BatchNormalization  (None, 56, 56, 288)  1152       ['block2b_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block2b_activation (Activation  (None, 56, 56, 288)  0          ['block2b_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block2b_se_squeeze (GlobalAver  (None, 288)         0           ['block2b_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block2b_se_reshape (Reshape)   (None, 1, 1, 288)    0           ['block2b_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2b_se_reduce (Conv2D)     (None, 1, 1, 12)     3468        ['block2b_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2b_se_expand (Conv2D)     (None, 1, 1, 288)    3744        ['block2b_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block2b_se_excite (Multiply)   (None, 56, 56, 288)  0           ['block2b_activation[0][0]',     Y          \n",
      "                                                                  'block2b_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block2b_project_conv (Conv2D)  (None, 56, 56, 48)   13824       ['block2b_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block2b_project_bn (BatchNorma  (None, 56, 56, 48)  192         ['block2b_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block2b_drop (FixedDropout)    (None, 56, 56, 48)   0           ['block2b_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2b_add (Add)              (None, 56, 56, 48)   0           ['block2b_drop[0][0]',           Y          \n",
      "                                                                  'block2a_project_bn[0][0]']                \n",
      "                                                                                                             \n",
      " block2c_expand_conv (Conv2D)   (None, 56, 56, 288)  13824       ['block2b_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block2c_expand_bn (BatchNormal  (None, 56, 56, 288)  1152       ['block2c_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block2c_expand_activation (Act  (None, 56, 56, 288)  0          ['block2c_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block2c_dwconv (DepthwiseConv2  (None, 56, 56, 288)  2592       ['block2c_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block2c_bn (BatchNormalization  (None, 56, 56, 288)  1152       ['block2c_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block2c_activation (Activation  (None, 56, 56, 288)  0          ['block2c_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block2c_se_squeeze (GlobalAver  (None, 288)         0           ['block2c_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block2c_se_reshape (Reshape)   (None, 1, 1, 288)    0           ['block2c_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2c_se_reduce (Conv2D)     (None, 1, 1, 12)     3468        ['block2c_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2c_se_expand (Conv2D)     (None, 1, 1, 288)    3744        ['block2c_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block2c_se_excite (Multiply)   (None, 56, 56, 288)  0           ['block2c_activation[0][0]',     Y          \n",
      "                                                                  'block2c_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block2c_project_conv (Conv2D)  (None, 56, 56, 48)   13824       ['block2c_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block2c_project_bn (BatchNorma  (None, 56, 56, 48)  192         ['block2c_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block2c_drop (FixedDropout)    (None, 56, 56, 48)   0           ['block2c_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2c_add (Add)              (None, 56, 56, 48)   0           ['block2c_drop[0][0]',           Y          \n",
      "                                                                  'block2b_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block2d_expand_conv (Conv2D)   (None, 56, 56, 288)  13824       ['block2c_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block2d_expand_bn (BatchNormal  (None, 56, 56, 288)  1152       ['block2d_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block2d_expand_activation (Act  (None, 56, 56, 288)  0          ['block2d_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block2d_dwconv (DepthwiseConv2  (None, 56, 56, 288)  2592       ['block2d_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block2d_bn (BatchNormalization  (None, 56, 56, 288)  1152       ['block2d_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block2d_activation (Activation  (None, 56, 56, 288)  0          ['block2d_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block2d_se_squeeze (GlobalAver  (None, 288)         0           ['block2d_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block2d_se_reshape (Reshape)   (None, 1, 1, 288)    0           ['block2d_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2d_se_reduce (Conv2D)     (None, 1, 1, 12)     3468        ['block2d_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2d_se_expand (Conv2D)     (None, 1, 1, 288)    3744        ['block2d_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block2d_se_excite (Multiply)   (None, 56, 56, 288)  0           ['block2d_activation[0][0]',     Y          \n",
      "                                                                  'block2d_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block2d_project_conv (Conv2D)  (None, 56, 56, 48)   13824       ['block2d_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block2d_project_bn (BatchNorma  (None, 56, 56, 48)  192         ['block2d_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block2d_drop (FixedDropout)    (None, 56, 56, 48)   0           ['block2d_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2d_add (Add)              (None, 56, 56, 48)   0           ['block2d_drop[0][0]',           Y          \n",
      "                                                                  'block2c_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block2e_expand_conv (Conv2D)   (None, 56, 56, 288)  13824       ['block2d_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block2e_expand_bn (BatchNormal  (None, 56, 56, 288)  1152       ['block2e_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block2e_expand_activation (Act  (None, 56, 56, 288)  0          ['block2e_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block2e_dwconv (DepthwiseConv2  (None, 56, 56, 288)  2592       ['block2e_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block2e_bn (BatchNormalization  (None, 56, 56, 288)  1152       ['block2e_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block2e_activation (Activation  (None, 56, 56, 288)  0          ['block2e_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block2e_se_squeeze (GlobalAver  (None, 288)         0           ['block2e_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block2e_se_reshape (Reshape)   (None, 1, 1, 288)    0           ['block2e_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2e_se_reduce (Conv2D)     (None, 1, 1, 12)     3468        ['block2e_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2e_se_expand (Conv2D)     (None, 1, 1, 288)    3744        ['block2e_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block2e_se_excite (Multiply)   (None, 56, 56, 288)  0           ['block2e_activation[0][0]',     Y          \n",
      "                                                                  'block2e_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block2e_project_conv (Conv2D)  (None, 56, 56, 48)   13824       ['block2e_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block2e_project_bn (BatchNorma  (None, 56, 56, 48)  192         ['block2e_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block2e_drop (FixedDropout)    (None, 56, 56, 48)   0           ['block2e_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2e_add (Add)              (None, 56, 56, 48)   0           ['block2e_drop[0][0]',           Y          \n",
      "                                                                  'block2d_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block2f_expand_conv (Conv2D)   (None, 56, 56, 288)  13824       ['block2e_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block2f_expand_bn (BatchNormal  (None, 56, 56, 288)  1152       ['block2f_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block2f_expand_activation (Act  (None, 56, 56, 288)  0          ['block2f_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block2f_dwconv (DepthwiseConv2  (None, 56, 56, 288)  2592       ['block2f_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block2f_bn (BatchNormalization  (None, 56, 56, 288)  1152       ['block2f_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block2f_activation (Activation  (None, 56, 56, 288)  0          ['block2f_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block2f_se_squeeze (GlobalAver  (None, 288)         0           ['block2f_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block2f_se_reshape (Reshape)   (None, 1, 1, 288)    0           ['block2f_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2f_se_reduce (Conv2D)     (None, 1, 1, 12)     3468        ['block2f_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2f_se_expand (Conv2D)     (None, 1, 1, 288)    3744        ['block2f_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block2f_se_excite (Multiply)   (None, 56, 56, 288)  0           ['block2f_activation[0][0]',     Y          \n",
      "                                                                  'block2f_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block2f_project_conv (Conv2D)  (None, 56, 56, 48)   13824       ['block2f_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block2f_project_bn (BatchNorma  (None, 56, 56, 48)  192         ['block2f_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block2f_drop (FixedDropout)    (None, 56, 56, 48)   0           ['block2f_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2f_add (Add)              (None, 56, 56, 48)   0           ['block2f_drop[0][0]',           Y          \n",
      "                                                                  'block2e_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block2g_expand_conv (Conv2D)   (None, 56, 56, 288)  13824       ['block2f_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block2g_expand_bn (BatchNormal  (None, 56, 56, 288)  1152       ['block2g_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block2g_expand_activation (Act  (None, 56, 56, 288)  0          ['block2g_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block2g_dwconv (DepthwiseConv2  (None, 56, 56, 288)  2592       ['block2g_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block2g_bn (BatchNormalization  (None, 56, 56, 288)  1152       ['block2g_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block2g_activation (Activation  (None, 56, 56, 288)  0          ['block2g_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block2g_se_squeeze (GlobalAver  (None, 288)         0           ['block2g_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block2g_se_reshape (Reshape)   (None, 1, 1, 288)    0           ['block2g_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2g_se_reduce (Conv2D)     (None, 1, 1, 12)     3468        ['block2g_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2g_se_expand (Conv2D)     (None, 1, 1, 288)    3744        ['block2g_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block2g_se_excite (Multiply)   (None, 56, 56, 288)  0           ['block2g_activation[0][0]',     Y          \n",
      "                                                                  'block2g_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block2g_project_conv (Conv2D)  (None, 56, 56, 48)   13824       ['block2g_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block2g_project_bn (BatchNorma  (None, 56, 56, 48)  192         ['block2g_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block2g_drop (FixedDropout)    (None, 56, 56, 48)   0           ['block2g_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block2g_add (Add)              (None, 56, 56, 48)   0           ['block2g_drop[0][0]',           Y          \n",
      "                                                                  'block2f_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block3a_expand_conv (Conv2D)   (None, 56, 56, 288)  13824       ['block2g_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block3a_expand_bn (BatchNormal  (None, 56, 56, 288)  1152       ['block3a_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block3a_expand_activation (Act  (None, 56, 56, 288)  0          ['block3a_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block3a_dwconv (DepthwiseConv2  (None, 28, 28, 288)  7200       ['block3a_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block3a_bn (BatchNormalization  (None, 28, 28, 288)  1152       ['block3a_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block3a_activation (Activation  (None, 28, 28, 288)  0          ['block3a_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block3a_se_squeeze (GlobalAver  (None, 288)         0           ['block3a_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block3a_se_reshape (Reshape)   (None, 1, 1, 288)    0           ['block3a_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3a_se_reduce (Conv2D)     (None, 1, 1, 12)     3468        ['block3a_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3a_se_expand (Conv2D)     (None, 1, 1, 288)    3744        ['block3a_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block3a_se_excite (Multiply)   (None, 28, 28, 288)  0           ['block3a_activation[0][0]',     Y          \n",
      "                                                                  'block3a_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block3a_project_conv (Conv2D)  (None, 28, 28, 80)   23040       ['block3a_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block3a_project_bn (BatchNorma  (None, 28, 28, 80)  320         ['block3a_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block3b_expand_conv (Conv2D)   (None, 28, 28, 480)  38400       ['block3a_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3b_expand_bn (BatchNormal  (None, 28, 28, 480)  1920       ['block3b_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block3b_expand_activation (Act  (None, 28, 28, 480)  0          ['block3b_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block3b_dwconv (DepthwiseConv2  (None, 28, 28, 480)  12000      ['block3b_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block3b_bn (BatchNormalization  (None, 28, 28, 480)  1920       ['block3b_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block3b_activation (Activation  (None, 28, 28, 480)  0          ['block3b_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block3b_se_squeeze (GlobalAver  (None, 480)         0           ['block3b_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block3b_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block3b_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3b_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block3b_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3b_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block3b_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block3b_se_excite (Multiply)   (None, 28, 28, 480)  0           ['block3b_activation[0][0]',     Y          \n",
      "                                                                  'block3b_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block3b_project_conv (Conv2D)  (None, 28, 28, 80)   38400       ['block3b_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block3b_project_bn (BatchNorma  (None, 28, 28, 80)  320         ['block3b_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block3b_drop (FixedDropout)    (None, 28, 28, 80)   0           ['block3b_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3b_add (Add)              (None, 28, 28, 80)   0           ['block3b_drop[0][0]',           Y          \n",
      "                                                                  'block3a_project_bn[0][0]']                \n",
      "                                                                                                             \n",
      " block3c_expand_conv (Conv2D)   (None, 28, 28, 480)  38400       ['block3b_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block3c_expand_bn (BatchNormal  (None, 28, 28, 480)  1920       ['block3c_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block3c_expand_activation (Act  (None, 28, 28, 480)  0          ['block3c_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block3c_dwconv (DepthwiseConv2  (None, 28, 28, 480)  12000      ['block3c_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block3c_bn (BatchNormalization  (None, 28, 28, 480)  1920       ['block3c_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block3c_activation (Activation  (None, 28, 28, 480)  0          ['block3c_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block3c_se_squeeze (GlobalAver  (None, 480)         0           ['block3c_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block3c_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block3c_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3c_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block3c_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3c_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block3c_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block3c_se_excite (Multiply)   (None, 28, 28, 480)  0           ['block3c_activation[0][0]',     Y          \n",
      "                                                                  'block3c_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block3c_project_conv (Conv2D)  (None, 28, 28, 80)   38400       ['block3c_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block3c_project_bn (BatchNorma  (None, 28, 28, 80)  320         ['block3c_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block3c_drop (FixedDropout)    (None, 28, 28, 80)   0           ['block3c_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3c_add (Add)              (None, 28, 28, 80)   0           ['block3c_drop[0][0]',           Y          \n",
      "                                                                  'block3b_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block3d_expand_conv (Conv2D)   (None, 28, 28, 480)  38400       ['block3c_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block3d_expand_bn (BatchNormal  (None, 28, 28, 480)  1920       ['block3d_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block3d_expand_activation (Act  (None, 28, 28, 480)  0          ['block3d_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block3d_dwconv (DepthwiseConv2  (None, 28, 28, 480)  12000      ['block3d_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block3d_bn (BatchNormalization  (None, 28, 28, 480)  1920       ['block3d_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block3d_activation (Activation  (None, 28, 28, 480)  0          ['block3d_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block3d_se_squeeze (GlobalAver  (None, 480)         0           ['block3d_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block3d_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block3d_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3d_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block3d_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3d_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block3d_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block3d_se_excite (Multiply)   (None, 28, 28, 480)  0           ['block3d_activation[0][0]',     Y          \n",
      "                                                                  'block3d_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block3d_project_conv (Conv2D)  (None, 28, 28, 80)   38400       ['block3d_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block3d_project_bn (BatchNorma  (None, 28, 28, 80)  320         ['block3d_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block3d_drop (FixedDropout)    (None, 28, 28, 80)   0           ['block3d_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3d_add (Add)              (None, 28, 28, 80)   0           ['block3d_drop[0][0]',           Y          \n",
      "                                                                  'block3c_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block3e_expand_conv (Conv2D)   (None, 28, 28, 480)  38400       ['block3d_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block3e_expand_bn (BatchNormal  (None, 28, 28, 480)  1920       ['block3e_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block3e_expand_activation (Act  (None, 28, 28, 480)  0          ['block3e_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block3e_dwconv (DepthwiseConv2  (None, 28, 28, 480)  12000      ['block3e_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block3e_bn (BatchNormalization  (None, 28, 28, 480)  1920       ['block3e_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block3e_activation (Activation  (None, 28, 28, 480)  0          ['block3e_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block3e_se_squeeze (GlobalAver  (None, 480)         0           ['block3e_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block3e_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block3e_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3e_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block3e_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3e_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block3e_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block3e_se_excite (Multiply)   (None, 28, 28, 480)  0           ['block3e_activation[0][0]',     Y          \n",
      "                                                                  'block3e_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block3e_project_conv (Conv2D)  (None, 28, 28, 80)   38400       ['block3e_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block3e_project_bn (BatchNorma  (None, 28, 28, 80)  320         ['block3e_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block3e_drop (FixedDropout)    (None, 28, 28, 80)   0           ['block3e_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3e_add (Add)              (None, 28, 28, 80)   0           ['block3e_drop[0][0]',           Y          \n",
      "                                                                  'block3d_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block3f_expand_conv (Conv2D)   (None, 28, 28, 480)  38400       ['block3e_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block3f_expand_bn (BatchNormal  (None, 28, 28, 480)  1920       ['block3f_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block3f_expand_activation (Act  (None, 28, 28, 480)  0          ['block3f_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block3f_dwconv (DepthwiseConv2  (None, 28, 28, 480)  12000      ['block3f_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block3f_bn (BatchNormalization  (None, 28, 28, 480)  1920       ['block3f_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block3f_activation (Activation  (None, 28, 28, 480)  0          ['block3f_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block3f_se_squeeze (GlobalAver  (None, 480)         0           ['block3f_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block3f_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block3f_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3f_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block3f_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3f_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block3f_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block3f_se_excite (Multiply)   (None, 28, 28, 480)  0           ['block3f_activation[0][0]',     Y          \n",
      "                                                                  'block3f_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block3f_project_conv (Conv2D)  (None, 28, 28, 80)   38400       ['block3f_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block3f_project_bn (BatchNorma  (None, 28, 28, 80)  320         ['block3f_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block3f_drop (FixedDropout)    (None, 28, 28, 80)   0           ['block3f_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3f_add (Add)              (None, 28, 28, 80)   0           ['block3f_drop[0][0]',           Y          \n",
      "                                                                  'block3e_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block3g_expand_conv (Conv2D)   (None, 28, 28, 480)  38400       ['block3f_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block3g_expand_bn (BatchNormal  (None, 28, 28, 480)  1920       ['block3g_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block3g_expand_activation (Act  (None, 28, 28, 480)  0          ['block3g_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block3g_dwconv (DepthwiseConv2  (None, 28, 28, 480)  12000      ['block3g_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block3g_bn (BatchNormalization  (None, 28, 28, 480)  1920       ['block3g_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block3g_activation (Activation  (None, 28, 28, 480)  0          ['block3g_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block3g_se_squeeze (GlobalAver  (None, 480)         0           ['block3g_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block3g_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block3g_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3g_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block3g_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3g_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block3g_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block3g_se_excite (Multiply)   (None, 28, 28, 480)  0           ['block3g_activation[0][0]',     Y          \n",
      "                                                                  'block3g_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block3g_project_conv (Conv2D)  (None, 28, 28, 80)   38400       ['block3g_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block3g_project_bn (BatchNorma  (None, 28, 28, 80)  320         ['block3g_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block3g_drop (FixedDropout)    (None, 28, 28, 80)   0           ['block3g_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block3g_add (Add)              (None, 28, 28, 80)   0           ['block3g_drop[0][0]',           Y          \n",
      "                                                                  'block3f_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block4a_expand_conv (Conv2D)   (None, 28, 28, 480)  38400       ['block3g_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block4a_expand_bn (BatchNormal  (None, 28, 28, 480)  1920       ['block4a_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block4a_expand_activation (Act  (None, 28, 28, 480)  0          ['block4a_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block4a_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4a_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block4a_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4a_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4a_activation (Activation  (None, 14, 14, 480)  0          ['block4a_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4a_se_squeeze (GlobalAver  (None, 480)         0           ['block4a_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block4a_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4a_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4a_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4a_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4a_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4a_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4a_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4a_activation[0][0]',     Y          \n",
      "                                                                  'block4a_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block4a_project_conv (Conv2D)  (None, 14, 14, 160)  76800       ['block4a_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4a_project_bn (BatchNorma  (None, 14, 14, 160)  640        ['block4a_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block4b_expand_conv (Conv2D)   (None, 14, 14, 960)  153600      ['block4a_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4b_expand_bn (BatchNormal  (None, 14, 14, 960)  3840       ['block4b_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block4b_expand_activation (Act  (None, 14, 14, 960)  0          ['block4b_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block4b_dwconv (DepthwiseConv2  (None, 14, 14, 960)  8640       ['block4b_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block4b_bn (BatchNormalization  (None, 14, 14, 960)  3840       ['block4b_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4b_activation (Activation  (None, 14, 14, 960)  0          ['block4b_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4b_se_squeeze (GlobalAver  (None, 960)         0           ['block4b_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block4b_se_reshape (Reshape)   (None, 1, 1, 960)    0           ['block4b_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4b_se_reduce (Conv2D)     (None, 1, 1, 40)     38440       ['block4b_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4b_se_expand (Conv2D)     (None, 1, 1, 960)    39360       ['block4b_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4b_se_excite (Multiply)   (None, 14, 14, 960)  0           ['block4b_activation[0][0]',     Y          \n",
      "                                                                  'block4b_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block4b_project_conv (Conv2D)  (None, 14, 14, 160)  153600      ['block4b_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4b_project_bn (BatchNorma  (None, 14, 14, 160)  640        ['block4b_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block4b_drop (FixedDropout)    (None, 14, 14, 160)  0           ['block4b_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4b_add (Add)              (None, 14, 14, 160)  0           ['block4b_drop[0][0]',           Y          \n",
      "                                                                  'block4a_project_bn[0][0]']                \n",
      "                                                                                                             \n",
      " block4c_expand_conv (Conv2D)   (None, 14, 14, 960)  153600      ['block4b_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block4c_expand_bn (BatchNormal  (None, 14, 14, 960)  3840       ['block4c_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block4c_expand_activation (Act  (None, 14, 14, 960)  0          ['block4c_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block4c_dwconv (DepthwiseConv2  (None, 14, 14, 960)  8640       ['block4c_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block4c_bn (BatchNormalization  (None, 14, 14, 960)  3840       ['block4c_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4c_activation (Activation  (None, 14, 14, 960)  0          ['block4c_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4c_se_squeeze (GlobalAver  (None, 960)         0           ['block4c_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block4c_se_reshape (Reshape)   (None, 1, 1, 960)    0           ['block4c_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4c_se_reduce (Conv2D)     (None, 1, 1, 40)     38440       ['block4c_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4c_se_expand (Conv2D)     (None, 1, 1, 960)    39360       ['block4c_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4c_se_excite (Multiply)   (None, 14, 14, 960)  0           ['block4c_activation[0][0]',     Y          \n",
      "                                                                  'block4c_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block4c_project_conv (Conv2D)  (None, 14, 14, 160)  153600      ['block4c_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4c_project_bn (BatchNorma  (None, 14, 14, 160)  640        ['block4c_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block4c_drop (FixedDropout)    (None, 14, 14, 160)  0           ['block4c_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4c_add (Add)              (None, 14, 14, 160)  0           ['block4c_drop[0][0]',           Y          \n",
      "                                                                  'block4b_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block4d_expand_conv (Conv2D)   (None, 14, 14, 960)  153600      ['block4c_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block4d_expand_bn (BatchNormal  (None, 14, 14, 960)  3840       ['block4d_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block4d_expand_activation (Act  (None, 14, 14, 960)  0          ['block4d_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block4d_dwconv (DepthwiseConv2  (None, 14, 14, 960)  8640       ['block4d_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block4d_bn (BatchNormalization  (None, 14, 14, 960)  3840       ['block4d_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4d_activation (Activation  (None, 14, 14, 960)  0          ['block4d_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4d_se_squeeze (GlobalAver  (None, 960)         0           ['block4d_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block4d_se_reshape (Reshape)   (None, 1, 1, 960)    0           ['block4d_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4d_se_reduce (Conv2D)     (None, 1, 1, 40)     38440       ['block4d_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4d_se_expand (Conv2D)     (None, 1, 1, 960)    39360       ['block4d_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4d_se_excite (Multiply)   (None, 14, 14, 960)  0           ['block4d_activation[0][0]',     Y          \n",
      "                                                                  'block4d_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block4d_project_conv (Conv2D)  (None, 14, 14, 160)  153600      ['block4d_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4d_project_bn (BatchNorma  (None, 14, 14, 160)  640        ['block4d_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block4d_drop (FixedDropout)    (None, 14, 14, 160)  0           ['block4d_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4d_add (Add)              (None, 14, 14, 160)  0           ['block4d_drop[0][0]',           Y          \n",
      "                                                                  'block4c_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block4e_expand_conv (Conv2D)   (None, 14, 14, 960)  153600      ['block4d_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block4e_expand_bn (BatchNormal  (None, 14, 14, 960)  3840       ['block4e_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block4e_expand_activation (Act  (None, 14, 14, 960)  0          ['block4e_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block4e_dwconv (DepthwiseConv2  (None, 14, 14, 960)  8640       ['block4e_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block4e_bn (BatchNormalization  (None, 14, 14, 960)  3840       ['block4e_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4e_activation (Activation  (None, 14, 14, 960)  0          ['block4e_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4e_se_squeeze (GlobalAver  (None, 960)         0           ['block4e_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block4e_se_reshape (Reshape)   (None, 1, 1, 960)    0           ['block4e_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4e_se_reduce (Conv2D)     (None, 1, 1, 40)     38440       ['block4e_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4e_se_expand (Conv2D)     (None, 1, 1, 960)    39360       ['block4e_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4e_se_excite (Multiply)   (None, 14, 14, 960)  0           ['block4e_activation[0][0]',     Y          \n",
      "                                                                  'block4e_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block4e_project_conv (Conv2D)  (None, 14, 14, 160)  153600      ['block4e_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4e_project_bn (BatchNorma  (None, 14, 14, 160)  640        ['block4e_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block4e_drop (FixedDropout)    (None, 14, 14, 160)  0           ['block4e_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4e_add (Add)              (None, 14, 14, 160)  0           ['block4e_drop[0][0]',           Y          \n",
      "                                                                  'block4d_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block4f_expand_conv (Conv2D)   (None, 14, 14, 960)  153600      ['block4e_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block4f_expand_bn (BatchNormal  (None, 14, 14, 960)  3840       ['block4f_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block4f_expand_activation (Act  (None, 14, 14, 960)  0          ['block4f_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block4f_dwconv (DepthwiseConv2  (None, 14, 14, 960)  8640       ['block4f_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block4f_bn (BatchNormalization  (None, 14, 14, 960)  3840       ['block4f_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4f_activation (Activation  (None, 14, 14, 960)  0          ['block4f_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4f_se_squeeze (GlobalAver  (None, 960)         0           ['block4f_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block4f_se_reshape (Reshape)   (None, 1, 1, 960)    0           ['block4f_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4f_se_reduce (Conv2D)     (None, 1, 1, 40)     38440       ['block4f_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4f_se_expand (Conv2D)     (None, 1, 1, 960)    39360       ['block4f_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4f_se_excite (Multiply)   (None, 14, 14, 960)  0           ['block4f_activation[0][0]',     Y          \n",
      "                                                                  'block4f_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block4f_project_conv (Conv2D)  (None, 14, 14, 160)  153600      ['block4f_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4f_project_bn (BatchNorma  (None, 14, 14, 160)  640        ['block4f_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block4f_drop (FixedDropout)    (None, 14, 14, 160)  0           ['block4f_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4f_add (Add)              (None, 14, 14, 160)  0           ['block4f_drop[0][0]',           Y          \n",
      "                                                                  'block4e_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block4g_expand_conv (Conv2D)   (None, 14, 14, 960)  153600      ['block4f_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block4g_expand_bn (BatchNormal  (None, 14, 14, 960)  3840       ['block4g_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block4g_expand_activation (Act  (None, 14, 14, 960)  0          ['block4g_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block4g_dwconv (DepthwiseConv2  (None, 14, 14, 960)  8640       ['block4g_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block4g_bn (BatchNormalization  (None, 14, 14, 960)  3840       ['block4g_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4g_activation (Activation  (None, 14, 14, 960)  0          ['block4g_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4g_se_squeeze (GlobalAver  (None, 960)         0           ['block4g_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block4g_se_reshape (Reshape)   (None, 1, 1, 960)    0           ['block4g_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4g_se_reduce (Conv2D)     (None, 1, 1, 40)     38440       ['block4g_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4g_se_expand (Conv2D)     (None, 1, 1, 960)    39360       ['block4g_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4g_se_excite (Multiply)   (None, 14, 14, 960)  0           ['block4g_activation[0][0]',     Y          \n",
      "                                                                  'block4g_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block4g_project_conv (Conv2D)  (None, 14, 14, 160)  153600      ['block4g_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4g_project_bn (BatchNorma  (None, 14, 14, 160)  640        ['block4g_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block4g_drop (FixedDropout)    (None, 14, 14, 160)  0           ['block4g_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4g_add (Add)              (None, 14, 14, 160)  0           ['block4g_drop[0][0]',           Y          \n",
      "                                                                  'block4f_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block4h_expand_conv (Conv2D)   (None, 14, 14, 960)  153600      ['block4g_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block4h_expand_bn (BatchNormal  (None, 14, 14, 960)  3840       ['block4h_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block4h_expand_activation (Act  (None, 14, 14, 960)  0          ['block4h_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block4h_dwconv (DepthwiseConv2  (None, 14, 14, 960)  8640       ['block4h_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block4h_bn (BatchNormalization  (None, 14, 14, 960)  3840       ['block4h_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4h_activation (Activation  (None, 14, 14, 960)  0          ['block4h_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4h_se_squeeze (GlobalAver  (None, 960)         0           ['block4h_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block4h_se_reshape (Reshape)   (None, 1, 1, 960)    0           ['block4h_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4h_se_reduce (Conv2D)     (None, 1, 1, 40)     38440       ['block4h_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4h_se_expand (Conv2D)     (None, 1, 1, 960)    39360       ['block4h_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4h_se_excite (Multiply)   (None, 14, 14, 960)  0           ['block4h_activation[0][0]',     Y          \n",
      "                                                                  'block4h_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block4h_project_conv (Conv2D)  (None, 14, 14, 160)  153600      ['block4h_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4h_project_bn (BatchNorma  (None, 14, 14, 160)  640        ['block4h_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block4h_drop (FixedDropout)    (None, 14, 14, 160)  0           ['block4h_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4h_add (Add)              (None, 14, 14, 160)  0           ['block4h_drop[0][0]',           Y          \n",
      "                                                                  'block4g_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block4i_expand_conv (Conv2D)   (None, 14, 14, 960)  153600      ['block4h_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block4i_expand_bn (BatchNormal  (None, 14, 14, 960)  3840       ['block4i_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block4i_expand_activation (Act  (None, 14, 14, 960)  0          ['block4i_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block4i_dwconv (DepthwiseConv2  (None, 14, 14, 960)  8640       ['block4i_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block4i_bn (BatchNormalization  (None, 14, 14, 960)  3840       ['block4i_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4i_activation (Activation  (None, 14, 14, 960)  0          ['block4i_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4i_se_squeeze (GlobalAver  (None, 960)         0           ['block4i_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block4i_se_reshape (Reshape)   (None, 1, 1, 960)    0           ['block4i_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4i_se_reduce (Conv2D)     (None, 1, 1, 40)     38440       ['block4i_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4i_se_expand (Conv2D)     (None, 1, 1, 960)    39360       ['block4i_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4i_se_excite (Multiply)   (None, 14, 14, 960)  0           ['block4i_activation[0][0]',     Y          \n",
      "                                                                  'block4i_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block4i_project_conv (Conv2D)  (None, 14, 14, 160)  153600      ['block4i_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4i_project_bn (BatchNorma  (None, 14, 14, 160)  640        ['block4i_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block4i_drop (FixedDropout)    (None, 14, 14, 160)  0           ['block4i_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4i_add (Add)              (None, 14, 14, 160)  0           ['block4i_drop[0][0]',           Y          \n",
      "                                                                  'block4h_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block4j_expand_conv (Conv2D)   (None, 14, 14, 960)  153600      ['block4i_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block4j_expand_bn (BatchNormal  (None, 14, 14, 960)  3840       ['block4j_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block4j_expand_activation (Act  (None, 14, 14, 960)  0          ['block4j_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block4j_dwconv (DepthwiseConv2  (None, 14, 14, 960)  8640       ['block4j_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block4j_bn (BatchNormalization  (None, 14, 14, 960)  3840       ['block4j_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4j_activation (Activation  (None, 14, 14, 960)  0          ['block4j_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block4j_se_squeeze (GlobalAver  (None, 960)         0           ['block4j_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block4j_se_reshape (Reshape)   (None, 1, 1, 960)    0           ['block4j_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4j_se_reduce (Conv2D)     (None, 1, 1, 40)     38440       ['block4j_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4j_se_expand (Conv2D)     (None, 1, 1, 960)    39360       ['block4j_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4j_se_excite (Multiply)   (None, 14, 14, 960)  0           ['block4j_activation[0][0]',     Y          \n",
      "                                                                  'block4j_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block4j_project_conv (Conv2D)  (None, 14, 14, 160)  153600      ['block4j_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block4j_project_bn (BatchNorma  (None, 14, 14, 160)  640        ['block4j_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block4j_drop (FixedDropout)    (None, 14, 14, 160)  0           ['block4j_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block4j_add (Add)              (None, 14, 14, 160)  0           ['block4j_drop[0][0]',           Y          \n",
      "                                                                  'block4i_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block5a_expand_conv (Conv2D)   (None, 14, 14, 960)  153600      ['block4j_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block5a_expand_bn (BatchNormal  (None, 14, 14, 960)  3840       ['block5a_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block5a_expand_activation (Act  (None, 14, 14, 960)  0          ['block5a_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block5a_dwconv (DepthwiseConv2  (None, 14, 14, 960)  24000      ['block5a_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block5a_bn (BatchNormalization  (None, 14, 14, 960)  3840       ['block5a_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block5a_activation (Activation  (None, 14, 14, 960)  0          ['block5a_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block5a_se_squeeze (GlobalAver  (None, 960)         0           ['block5a_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block5a_se_reshape (Reshape)   (None, 1, 1, 960)    0           ['block5a_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5a_se_reduce (Conv2D)     (None, 1, 1, 40)     38440       ['block5a_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5a_se_expand (Conv2D)     (None, 1, 1, 960)    39360       ['block5a_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5a_se_excite (Multiply)   (None, 14, 14, 960)  0           ['block5a_activation[0][0]',     Y          \n",
      "                                                                  'block5a_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block5a_project_conv (Conv2D)  (None, 14, 14, 224)  215040      ['block5a_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5a_project_bn (BatchNorma  (None, 14, 14, 224)  896        ['block5a_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block5b_expand_conv (Conv2D)   (None, 14, 14, 1344  301056      ['block5a_project_bn[0][0]']     Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block5b_expand_bn (BatchNormal  (None, 14, 14, 1344  5376       ['block5b_expand_conv[0][0]']    Y          \n",
      " ization)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5b_expand_activation (Act  (None, 14, 14, 1344  0          ['block5b_expand_bn[0][0]']      Y          \n",
      " ivation)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5b_dwconv (DepthwiseConv2  (None, 14, 14, 1344  33600      ['block5b_expand_activation[0][  Y          \n",
      " D)                             )                                0]']                                        \n",
      "                                                                                                             \n",
      " block5b_bn (BatchNormalization  (None, 14, 14, 1344  5376       ['block5b_dwconv[0][0]']         Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5b_activation (Activation  (None, 14, 14, 1344  0          ['block5b_bn[0][0]']             Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5b_se_squeeze (GlobalAver  (None, 1344)        0           ['block5b_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block5b_se_reshape (Reshape)   (None, 1, 1, 1344)   0           ['block5b_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5b_se_reduce (Conv2D)     (None, 1, 1, 56)     75320       ['block5b_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5b_se_expand (Conv2D)     (None, 1, 1, 1344)   76608       ['block5b_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5b_se_excite (Multiply)   (None, 14, 14, 1344  0           ['block5b_activation[0][0]',     Y          \n",
      "                                )                                 'block5b_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block5b_project_conv (Conv2D)  (None, 14, 14, 224)  301056      ['block5b_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5b_project_bn (BatchNorma  (None, 14, 14, 224)  896        ['block5b_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block5b_drop (FixedDropout)    (None, 14, 14, 224)  0           ['block5b_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5b_add (Add)              (None, 14, 14, 224)  0           ['block5b_drop[0][0]',           Y          \n",
      "                                                                  'block5a_project_bn[0][0]']                \n",
      "                                                                                                             \n",
      " block5c_expand_conv (Conv2D)   (None, 14, 14, 1344  301056      ['block5b_add[0][0]']            Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block5c_expand_bn (BatchNormal  (None, 14, 14, 1344  5376       ['block5c_expand_conv[0][0]']    Y          \n",
      " ization)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5c_expand_activation (Act  (None, 14, 14, 1344  0          ['block5c_expand_bn[0][0]']      Y          \n",
      " ivation)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5c_dwconv (DepthwiseConv2  (None, 14, 14, 1344  33600      ['block5c_expand_activation[0][  Y          \n",
      " D)                             )                                0]']                                        \n",
      "                                                                                                             \n",
      " block5c_bn (BatchNormalization  (None, 14, 14, 1344  5376       ['block5c_dwconv[0][0]']         Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5c_activation (Activation  (None, 14, 14, 1344  0          ['block5c_bn[0][0]']             Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5c_se_squeeze (GlobalAver  (None, 1344)        0           ['block5c_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block5c_se_reshape (Reshape)   (None, 1, 1, 1344)   0           ['block5c_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5c_se_reduce (Conv2D)     (None, 1, 1, 56)     75320       ['block5c_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5c_se_expand (Conv2D)     (None, 1, 1, 1344)   76608       ['block5c_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5c_se_excite (Multiply)   (None, 14, 14, 1344  0           ['block5c_activation[0][0]',     Y          \n",
      "                                )                                 'block5c_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block5c_project_conv (Conv2D)  (None, 14, 14, 224)  301056      ['block5c_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5c_project_bn (BatchNorma  (None, 14, 14, 224)  896        ['block5c_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block5c_drop (FixedDropout)    (None, 14, 14, 224)  0           ['block5c_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5c_add (Add)              (None, 14, 14, 224)  0           ['block5c_drop[0][0]',           Y          \n",
      "                                                                  'block5b_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block5d_expand_conv (Conv2D)   (None, 14, 14, 1344  301056      ['block5c_add[0][0]']            Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block5d_expand_bn (BatchNormal  (None, 14, 14, 1344  5376       ['block5d_expand_conv[0][0]']    Y          \n",
      " ization)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5d_expand_activation (Act  (None, 14, 14, 1344  0          ['block5d_expand_bn[0][0]']      Y          \n",
      " ivation)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5d_dwconv (DepthwiseConv2  (None, 14, 14, 1344  33600      ['block5d_expand_activation[0][  Y          \n",
      " D)                             )                                0]']                                        \n",
      "                                                                                                             \n",
      " block5d_bn (BatchNormalization  (None, 14, 14, 1344  5376       ['block5d_dwconv[0][0]']         Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5d_activation (Activation  (None, 14, 14, 1344  0          ['block5d_bn[0][0]']             Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5d_se_squeeze (GlobalAver  (None, 1344)        0           ['block5d_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block5d_se_reshape (Reshape)   (None, 1, 1, 1344)   0           ['block5d_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5d_se_reduce (Conv2D)     (None, 1, 1, 56)     75320       ['block5d_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5d_se_expand (Conv2D)     (None, 1, 1, 1344)   76608       ['block5d_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5d_se_excite (Multiply)   (None, 14, 14, 1344  0           ['block5d_activation[0][0]',     Y          \n",
      "                                )                                 'block5d_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block5d_project_conv (Conv2D)  (None, 14, 14, 224)  301056      ['block5d_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5d_project_bn (BatchNorma  (None, 14, 14, 224)  896        ['block5d_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block5d_drop (FixedDropout)    (None, 14, 14, 224)  0           ['block5d_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5d_add (Add)              (None, 14, 14, 224)  0           ['block5d_drop[0][0]',           Y          \n",
      "                                                                  'block5c_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block5e_expand_conv (Conv2D)   (None, 14, 14, 1344  301056      ['block5d_add[0][0]']            Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block5e_expand_bn (BatchNormal  (None, 14, 14, 1344  5376       ['block5e_expand_conv[0][0]']    Y          \n",
      " ization)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5e_expand_activation (Act  (None, 14, 14, 1344  0          ['block5e_expand_bn[0][0]']      Y          \n",
      " ivation)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5e_dwconv (DepthwiseConv2  (None, 14, 14, 1344  33600      ['block5e_expand_activation[0][  Y          \n",
      " D)                             )                                0]']                                        \n",
      "                                                                                                             \n",
      " block5e_bn (BatchNormalization  (None, 14, 14, 1344  5376       ['block5e_dwconv[0][0]']         Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5e_activation (Activation  (None, 14, 14, 1344  0          ['block5e_bn[0][0]']             Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5e_se_squeeze (GlobalAver  (None, 1344)        0           ['block5e_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block5e_se_reshape (Reshape)   (None, 1, 1, 1344)   0           ['block5e_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5e_se_reduce (Conv2D)     (None, 1, 1, 56)     75320       ['block5e_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5e_se_expand (Conv2D)     (None, 1, 1, 1344)   76608       ['block5e_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5e_se_excite (Multiply)   (None, 14, 14, 1344  0           ['block5e_activation[0][0]',     Y          \n",
      "                                )                                 'block5e_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block5e_project_conv (Conv2D)  (None, 14, 14, 224)  301056      ['block5e_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5e_project_bn (BatchNorma  (None, 14, 14, 224)  896        ['block5e_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block5e_drop (FixedDropout)    (None, 14, 14, 224)  0           ['block5e_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5e_add (Add)              (None, 14, 14, 224)  0           ['block5e_drop[0][0]',           Y          \n",
      "                                                                  'block5d_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block5f_expand_conv (Conv2D)   (None, 14, 14, 1344  301056      ['block5e_add[0][0]']            Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block5f_expand_bn (BatchNormal  (None, 14, 14, 1344  5376       ['block5f_expand_conv[0][0]']    Y          \n",
      " ization)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5f_expand_activation (Act  (None, 14, 14, 1344  0          ['block5f_expand_bn[0][0]']      Y          \n",
      " ivation)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5f_dwconv (DepthwiseConv2  (None, 14, 14, 1344  33600      ['block5f_expand_activation[0][  Y          \n",
      " D)                             )                                0]']                                        \n",
      "                                                                                                             \n",
      " block5f_bn (BatchNormalization  (None, 14, 14, 1344  5376       ['block5f_dwconv[0][0]']         Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5f_activation (Activation  (None, 14, 14, 1344  0          ['block5f_bn[0][0]']             Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5f_se_squeeze (GlobalAver  (None, 1344)        0           ['block5f_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block5f_se_reshape (Reshape)   (None, 1, 1, 1344)   0           ['block5f_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5f_se_reduce (Conv2D)     (None, 1, 1, 56)     75320       ['block5f_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5f_se_expand (Conv2D)     (None, 1, 1, 1344)   76608       ['block5f_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5f_se_excite (Multiply)   (None, 14, 14, 1344  0           ['block5f_activation[0][0]',     Y          \n",
      "                                )                                 'block5f_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block5f_project_conv (Conv2D)  (None, 14, 14, 224)  301056      ['block5f_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5f_project_bn (BatchNorma  (None, 14, 14, 224)  896        ['block5f_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block5f_drop (FixedDropout)    (None, 14, 14, 224)  0           ['block5f_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5f_add (Add)              (None, 14, 14, 224)  0           ['block5f_drop[0][0]',           Y          \n",
      "                                                                  'block5e_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block5g_expand_conv (Conv2D)   (None, 14, 14, 1344  301056      ['block5f_add[0][0]']            Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block5g_expand_bn (BatchNormal  (None, 14, 14, 1344  5376       ['block5g_expand_conv[0][0]']    Y          \n",
      " ization)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5g_expand_activation (Act  (None, 14, 14, 1344  0          ['block5g_expand_bn[0][0]']      Y          \n",
      " ivation)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5g_dwconv (DepthwiseConv2  (None, 14, 14, 1344  33600      ['block5g_expand_activation[0][  Y          \n",
      " D)                             )                                0]']                                        \n",
      "                                                                                                             \n",
      " block5g_bn (BatchNormalization  (None, 14, 14, 1344  5376       ['block5g_dwconv[0][0]']         Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5g_activation (Activation  (None, 14, 14, 1344  0          ['block5g_bn[0][0]']             Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5g_se_squeeze (GlobalAver  (None, 1344)        0           ['block5g_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block5g_se_reshape (Reshape)   (None, 1, 1, 1344)   0           ['block5g_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5g_se_reduce (Conv2D)     (None, 1, 1, 56)     75320       ['block5g_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5g_se_expand (Conv2D)     (None, 1, 1, 1344)   76608       ['block5g_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5g_se_excite (Multiply)   (None, 14, 14, 1344  0           ['block5g_activation[0][0]',     Y          \n",
      "                                )                                 'block5g_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block5g_project_conv (Conv2D)  (None, 14, 14, 224)  301056      ['block5g_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5g_project_bn (BatchNorma  (None, 14, 14, 224)  896        ['block5g_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block5g_drop (FixedDropout)    (None, 14, 14, 224)  0           ['block5g_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5g_add (Add)              (None, 14, 14, 224)  0           ['block5g_drop[0][0]',           Y          \n",
      "                                                                  'block5f_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block5h_expand_conv (Conv2D)   (None, 14, 14, 1344  301056      ['block5g_add[0][0]']            Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block5h_expand_bn (BatchNormal  (None, 14, 14, 1344  5376       ['block5h_expand_conv[0][0]']    Y          \n",
      " ization)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5h_expand_activation (Act  (None, 14, 14, 1344  0          ['block5h_expand_bn[0][0]']      Y          \n",
      " ivation)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5h_dwconv (DepthwiseConv2  (None, 14, 14, 1344  33600      ['block5h_expand_activation[0][  Y          \n",
      " D)                             )                                0]']                                        \n",
      "                                                                                                             \n",
      " block5h_bn (BatchNormalization  (None, 14, 14, 1344  5376       ['block5h_dwconv[0][0]']         Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5h_activation (Activation  (None, 14, 14, 1344  0          ['block5h_bn[0][0]']             Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5h_se_squeeze (GlobalAver  (None, 1344)        0           ['block5h_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block5h_se_reshape (Reshape)   (None, 1, 1, 1344)   0           ['block5h_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5h_se_reduce (Conv2D)     (None, 1, 1, 56)     75320       ['block5h_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5h_se_expand (Conv2D)     (None, 1, 1, 1344)   76608       ['block5h_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5h_se_excite (Multiply)   (None, 14, 14, 1344  0           ['block5h_activation[0][0]',     Y          \n",
      "                                )                                 'block5h_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block5h_project_conv (Conv2D)  (None, 14, 14, 224)  301056      ['block5h_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5h_project_bn (BatchNorma  (None, 14, 14, 224)  896        ['block5h_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block5h_drop (FixedDropout)    (None, 14, 14, 224)  0           ['block5h_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5h_add (Add)              (None, 14, 14, 224)  0           ['block5h_drop[0][0]',           Y          \n",
      "                                                                  'block5g_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block5i_expand_conv (Conv2D)   (None, 14, 14, 1344  301056      ['block5h_add[0][0]']            Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block5i_expand_bn (BatchNormal  (None, 14, 14, 1344  5376       ['block5i_expand_conv[0][0]']    Y          \n",
      " ization)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5i_expand_activation (Act  (None, 14, 14, 1344  0          ['block5i_expand_bn[0][0]']      Y          \n",
      " ivation)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5i_dwconv (DepthwiseConv2  (None, 14, 14, 1344  33600      ['block5i_expand_activation[0][  Y          \n",
      " D)                             )                                0]']                                        \n",
      "                                                                                                             \n",
      " block5i_bn (BatchNormalization  (None, 14, 14, 1344  5376       ['block5i_dwconv[0][0]']         Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5i_activation (Activation  (None, 14, 14, 1344  0          ['block5i_bn[0][0]']             Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5i_se_squeeze (GlobalAver  (None, 1344)        0           ['block5i_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block5i_se_reshape (Reshape)   (None, 1, 1, 1344)   0           ['block5i_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5i_se_reduce (Conv2D)     (None, 1, 1, 56)     75320       ['block5i_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5i_se_expand (Conv2D)     (None, 1, 1, 1344)   76608       ['block5i_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5i_se_excite (Multiply)   (None, 14, 14, 1344  0           ['block5i_activation[0][0]',     Y          \n",
      "                                )                                 'block5i_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block5i_project_conv (Conv2D)  (None, 14, 14, 224)  301056      ['block5i_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5i_project_bn (BatchNorma  (None, 14, 14, 224)  896        ['block5i_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block5i_drop (FixedDropout)    (None, 14, 14, 224)  0           ['block5i_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5i_add (Add)              (None, 14, 14, 224)  0           ['block5i_drop[0][0]',           Y          \n",
      "                                                                  'block5h_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block5j_expand_conv (Conv2D)   (None, 14, 14, 1344  301056      ['block5i_add[0][0]']            Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block5j_expand_bn (BatchNormal  (None, 14, 14, 1344  5376       ['block5j_expand_conv[0][0]']    Y          \n",
      " ization)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5j_expand_activation (Act  (None, 14, 14, 1344  0          ['block5j_expand_bn[0][0]']      Y          \n",
      " ivation)                       )                                                                            \n",
      "                                                                                                             \n",
      " block5j_dwconv (DepthwiseConv2  (None, 14, 14, 1344  33600      ['block5j_expand_activation[0][  Y          \n",
      " D)                             )                                0]']                                        \n",
      "                                                                                                             \n",
      " block5j_bn (BatchNormalization  (None, 14, 14, 1344  5376       ['block5j_dwconv[0][0]']         Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5j_activation (Activation  (None, 14, 14, 1344  0          ['block5j_bn[0][0]']             Y          \n",
      " )                              )                                                                            \n",
      "                                                                                                             \n",
      " block5j_se_squeeze (GlobalAver  (None, 1344)        0           ['block5j_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block5j_se_reshape (Reshape)   (None, 1, 1, 1344)   0           ['block5j_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5j_se_reduce (Conv2D)     (None, 1, 1, 56)     75320       ['block5j_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5j_se_expand (Conv2D)     (None, 1, 1, 1344)   76608       ['block5j_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5j_se_excite (Multiply)   (None, 14, 14, 1344  0           ['block5j_activation[0][0]',     Y          \n",
      "                                )                                 'block5j_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block5j_project_conv (Conv2D)  (None, 14, 14, 224)  301056      ['block5j_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block5j_project_bn (BatchNorma  (None, 14, 14, 224)  896        ['block5j_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block5j_drop (FixedDropout)    (None, 14, 14, 224)  0           ['block5j_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block5j_add (Add)              (None, 14, 14, 224)  0           ['block5j_drop[0][0]',           Y          \n",
      "                                                                  'block5i_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block6a_expand_conv (Conv2D)   (None, 14, 14, 1344  301056      ['block5j_add[0][0]']            Y          \n",
      "                                )                                                                            \n",
      "                                                                                                             \n",
      " block6a_expand_bn (BatchNormal  (None, 14, 14, 1344  5376       ['block6a_expand_conv[0][0]']    Y          \n",
      " ization)                       )                                                                            \n",
      "                                                                                                             \n",
      " block6a_expand_activation (Act  (None, 14, 14, 1344  0          ['block6a_expand_bn[0][0]']      Y          \n",
      " ivation)                       )                                                                            \n",
      "                                                                                                             \n",
      " block6a_dwconv (DepthwiseConv2  (None, 7, 7, 1344)  33600       ['block6a_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block6a_bn (BatchNormalization  (None, 7, 7, 1344)  5376        ['block6a_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6a_activation (Activation  (None, 7, 7, 1344)  0           ['block6a_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6a_se_squeeze (GlobalAver  (None, 1344)        0           ['block6a_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block6a_se_reshape (Reshape)   (None, 1, 1, 1344)   0           ['block6a_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6a_se_reduce (Conv2D)     (None, 1, 1, 56)     75320       ['block6a_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6a_se_expand (Conv2D)     (None, 1, 1, 1344)   76608       ['block6a_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6a_se_excite (Multiply)   (None, 7, 7, 1344)   0           ['block6a_activation[0][0]',     Y          \n",
      "                                                                  'block6a_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block6a_project_conv (Conv2D)  (None, 7, 7, 384)    516096      ['block6a_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6a_project_bn (BatchNorma  (None, 7, 7, 384)   1536        ['block6a_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block6b_expand_conv (Conv2D)   (None, 7, 7, 2304)   884736      ['block6a_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6b_expand_bn (BatchNormal  (None, 7, 7, 2304)  9216        ['block6b_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block6b_expand_activation (Act  (None, 7, 7, 2304)  0           ['block6b_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block6b_dwconv (DepthwiseConv2  (None, 7, 7, 2304)  57600       ['block6b_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block6b_bn (BatchNormalization  (None, 7, 7, 2304)  9216        ['block6b_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6b_activation (Activation  (None, 7, 7, 2304)  0           ['block6b_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6b_se_squeeze (GlobalAver  (None, 2304)        0           ['block6b_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block6b_se_reshape (Reshape)   (None, 1, 1, 2304)   0           ['block6b_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6b_se_reduce (Conv2D)     (None, 1, 1, 96)     221280      ['block6b_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6b_se_expand (Conv2D)     (None, 1, 1, 2304)   223488      ['block6b_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6b_se_excite (Multiply)   (None, 7, 7, 2304)   0           ['block6b_activation[0][0]',     Y          \n",
      "                                                                  'block6b_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block6b_project_conv (Conv2D)  (None, 7, 7, 384)    884736      ['block6b_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6b_project_bn (BatchNorma  (None, 7, 7, 384)   1536        ['block6b_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block6b_drop (FixedDropout)    (None, 7, 7, 384)    0           ['block6b_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6b_add (Add)              (None, 7, 7, 384)    0           ['block6b_drop[0][0]',           Y          \n",
      "                                                                  'block6a_project_bn[0][0]']                \n",
      "                                                                                                             \n",
      " block6c_expand_conv (Conv2D)   (None, 7, 7, 2304)   884736      ['block6b_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block6c_expand_bn (BatchNormal  (None, 7, 7, 2304)  9216        ['block6c_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block6c_expand_activation (Act  (None, 7, 7, 2304)  0           ['block6c_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block6c_dwconv (DepthwiseConv2  (None, 7, 7, 2304)  57600       ['block6c_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block6c_bn (BatchNormalization  (None, 7, 7, 2304)  9216        ['block6c_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6c_activation (Activation  (None, 7, 7, 2304)  0           ['block6c_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6c_se_squeeze (GlobalAver  (None, 2304)        0           ['block6c_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block6c_se_reshape (Reshape)   (None, 1, 1, 2304)   0           ['block6c_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6c_se_reduce (Conv2D)     (None, 1, 1, 96)     221280      ['block6c_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6c_se_expand (Conv2D)     (None, 1, 1, 2304)   223488      ['block6c_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6c_se_excite (Multiply)   (None, 7, 7, 2304)   0           ['block6c_activation[0][0]',     Y          \n",
      "                                                                  'block6c_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block6c_project_conv (Conv2D)  (None, 7, 7, 384)    884736      ['block6c_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6c_project_bn (BatchNorma  (None, 7, 7, 384)   1536        ['block6c_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block6c_drop (FixedDropout)    (None, 7, 7, 384)    0           ['block6c_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6c_add (Add)              (None, 7, 7, 384)    0           ['block6c_drop[0][0]',           Y          \n",
      "                                                                  'block6b_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block6d_expand_conv (Conv2D)   (None, 7, 7, 2304)   884736      ['block6c_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block6d_expand_bn (BatchNormal  (None, 7, 7, 2304)  9216        ['block6d_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block6d_expand_activation (Act  (None, 7, 7, 2304)  0           ['block6d_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block6d_dwconv (DepthwiseConv2  (None, 7, 7, 2304)  57600       ['block6d_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block6d_bn (BatchNormalization  (None, 7, 7, 2304)  9216        ['block6d_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6d_activation (Activation  (None, 7, 7, 2304)  0           ['block6d_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6d_se_squeeze (GlobalAver  (None, 2304)        0           ['block6d_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block6d_se_reshape (Reshape)   (None, 1, 1, 2304)   0           ['block6d_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6d_se_reduce (Conv2D)     (None, 1, 1, 96)     221280      ['block6d_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6d_se_expand (Conv2D)     (None, 1, 1, 2304)   223488      ['block6d_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6d_se_excite (Multiply)   (None, 7, 7, 2304)   0           ['block6d_activation[0][0]',     Y          \n",
      "                                                                  'block6d_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block6d_project_conv (Conv2D)  (None, 7, 7, 384)    884736      ['block6d_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6d_project_bn (BatchNorma  (None, 7, 7, 384)   1536        ['block6d_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block6d_drop (FixedDropout)    (None, 7, 7, 384)    0           ['block6d_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6d_add (Add)              (None, 7, 7, 384)    0           ['block6d_drop[0][0]',           Y          \n",
      "                                                                  'block6c_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block6e_expand_conv (Conv2D)   (None, 7, 7, 2304)   884736      ['block6d_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block6e_expand_bn (BatchNormal  (None, 7, 7, 2304)  9216        ['block6e_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block6e_expand_activation (Act  (None, 7, 7, 2304)  0           ['block6e_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block6e_dwconv (DepthwiseConv2  (None, 7, 7, 2304)  57600       ['block6e_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block6e_bn (BatchNormalization  (None, 7, 7, 2304)  9216        ['block6e_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6e_activation (Activation  (None, 7, 7, 2304)  0           ['block6e_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6e_se_squeeze (GlobalAver  (None, 2304)        0           ['block6e_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block6e_se_reshape (Reshape)   (None, 1, 1, 2304)   0           ['block6e_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6e_se_reduce (Conv2D)     (None, 1, 1, 96)     221280      ['block6e_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6e_se_expand (Conv2D)     (None, 1, 1, 2304)   223488      ['block6e_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6e_se_excite (Multiply)   (None, 7, 7, 2304)   0           ['block6e_activation[0][0]',     Y          \n",
      "                                                                  'block6e_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block6e_project_conv (Conv2D)  (None, 7, 7, 384)    884736      ['block6e_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6e_project_bn (BatchNorma  (None, 7, 7, 384)   1536        ['block6e_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block6e_drop (FixedDropout)    (None, 7, 7, 384)    0           ['block6e_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6e_add (Add)              (None, 7, 7, 384)    0           ['block6e_drop[0][0]',           Y          \n",
      "                                                                  'block6d_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block6f_expand_conv (Conv2D)   (None, 7, 7, 2304)   884736      ['block6e_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block6f_expand_bn (BatchNormal  (None, 7, 7, 2304)  9216        ['block6f_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block6f_expand_activation (Act  (None, 7, 7, 2304)  0           ['block6f_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block6f_dwconv (DepthwiseConv2  (None, 7, 7, 2304)  57600       ['block6f_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block6f_bn (BatchNormalization  (None, 7, 7, 2304)  9216        ['block6f_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6f_activation (Activation  (None, 7, 7, 2304)  0           ['block6f_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6f_se_squeeze (GlobalAver  (None, 2304)        0           ['block6f_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block6f_se_reshape (Reshape)   (None, 1, 1, 2304)   0           ['block6f_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6f_se_reduce (Conv2D)     (None, 1, 1, 96)     221280      ['block6f_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6f_se_expand (Conv2D)     (None, 1, 1, 2304)   223488      ['block6f_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6f_se_excite (Multiply)   (None, 7, 7, 2304)   0           ['block6f_activation[0][0]',     Y          \n",
      "                                                                  'block6f_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block6f_project_conv (Conv2D)  (None, 7, 7, 384)    884736      ['block6f_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6f_project_bn (BatchNorma  (None, 7, 7, 384)   1536        ['block6f_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block6f_drop (FixedDropout)    (None, 7, 7, 384)    0           ['block6f_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6f_add (Add)              (None, 7, 7, 384)    0           ['block6f_drop[0][0]',           Y          \n",
      "                                                                  'block6e_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block6g_expand_conv (Conv2D)   (None, 7, 7, 2304)   884736      ['block6f_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block6g_expand_bn (BatchNormal  (None, 7, 7, 2304)  9216        ['block6g_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block6g_expand_activation (Act  (None, 7, 7, 2304)  0           ['block6g_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block6g_dwconv (DepthwiseConv2  (None, 7, 7, 2304)  57600       ['block6g_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block6g_bn (BatchNormalization  (None, 7, 7, 2304)  9216        ['block6g_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6g_activation (Activation  (None, 7, 7, 2304)  0           ['block6g_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6g_se_squeeze (GlobalAver  (None, 2304)        0           ['block6g_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block6g_se_reshape (Reshape)   (None, 1, 1, 2304)   0           ['block6g_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6g_se_reduce (Conv2D)     (None, 1, 1, 96)     221280      ['block6g_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6g_se_expand (Conv2D)     (None, 1, 1, 2304)   223488      ['block6g_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6g_se_excite (Multiply)   (None, 7, 7, 2304)   0           ['block6g_activation[0][0]',     Y          \n",
      "                                                                  'block6g_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block6g_project_conv (Conv2D)  (None, 7, 7, 384)    884736      ['block6g_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6g_project_bn (BatchNorma  (None, 7, 7, 384)   1536        ['block6g_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block6g_drop (FixedDropout)    (None, 7, 7, 384)    0           ['block6g_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6g_add (Add)              (None, 7, 7, 384)    0           ['block6g_drop[0][0]',           Y          \n",
      "                                                                  'block6f_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block6h_expand_conv (Conv2D)   (None, 7, 7, 2304)   884736      ['block6g_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block6h_expand_bn (BatchNormal  (None, 7, 7, 2304)  9216        ['block6h_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block6h_expand_activation (Act  (None, 7, 7, 2304)  0           ['block6h_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block6h_dwconv (DepthwiseConv2  (None, 7, 7, 2304)  57600       ['block6h_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block6h_bn (BatchNormalization  (None, 7, 7, 2304)  9216        ['block6h_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6h_activation (Activation  (None, 7, 7, 2304)  0           ['block6h_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6h_se_squeeze (GlobalAver  (None, 2304)        0           ['block6h_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block6h_se_reshape (Reshape)   (None, 1, 1, 2304)   0           ['block6h_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6h_se_reduce (Conv2D)     (None, 1, 1, 96)     221280      ['block6h_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6h_se_expand (Conv2D)     (None, 1, 1, 2304)   223488      ['block6h_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6h_se_excite (Multiply)   (None, 7, 7, 2304)   0           ['block6h_activation[0][0]',     Y          \n",
      "                                                                  'block6h_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block6h_project_conv (Conv2D)  (None, 7, 7, 384)    884736      ['block6h_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6h_project_bn (BatchNorma  (None, 7, 7, 384)   1536        ['block6h_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block6h_drop (FixedDropout)    (None, 7, 7, 384)    0           ['block6h_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6h_add (Add)              (None, 7, 7, 384)    0           ['block6h_drop[0][0]',           Y          \n",
      "                                                                  'block6g_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block6i_expand_conv (Conv2D)   (None, 7, 7, 2304)   884736      ['block6h_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block6i_expand_bn (BatchNormal  (None, 7, 7, 2304)  9216        ['block6i_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block6i_expand_activation (Act  (None, 7, 7, 2304)  0           ['block6i_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block6i_dwconv (DepthwiseConv2  (None, 7, 7, 2304)  57600       ['block6i_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block6i_bn (BatchNormalization  (None, 7, 7, 2304)  9216        ['block6i_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6i_activation (Activation  (None, 7, 7, 2304)  0           ['block6i_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6i_se_squeeze (GlobalAver  (None, 2304)        0           ['block6i_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block6i_se_reshape (Reshape)   (None, 1, 1, 2304)   0           ['block6i_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6i_se_reduce (Conv2D)     (None, 1, 1, 96)     221280      ['block6i_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6i_se_expand (Conv2D)     (None, 1, 1, 2304)   223488      ['block6i_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6i_se_excite (Multiply)   (None, 7, 7, 2304)   0           ['block6i_activation[0][0]',     Y          \n",
      "                                                                  'block6i_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block6i_project_conv (Conv2D)  (None, 7, 7, 384)    884736      ['block6i_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6i_project_bn (BatchNorma  (None, 7, 7, 384)   1536        ['block6i_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block6i_drop (FixedDropout)    (None, 7, 7, 384)    0           ['block6i_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6i_add (Add)              (None, 7, 7, 384)    0           ['block6i_drop[0][0]',           Y          \n",
      "                                                                  'block6h_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block6j_expand_conv (Conv2D)   (None, 7, 7, 2304)   884736      ['block6i_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block6j_expand_bn (BatchNormal  (None, 7, 7, 2304)  9216        ['block6j_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block6j_expand_activation (Act  (None, 7, 7, 2304)  0           ['block6j_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block6j_dwconv (DepthwiseConv2  (None, 7, 7, 2304)  57600       ['block6j_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block6j_bn (BatchNormalization  (None, 7, 7, 2304)  9216        ['block6j_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6j_activation (Activation  (None, 7, 7, 2304)  0           ['block6j_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6j_se_squeeze (GlobalAver  (None, 2304)        0           ['block6j_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block6j_se_reshape (Reshape)   (None, 1, 1, 2304)   0           ['block6j_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6j_se_reduce (Conv2D)     (None, 1, 1, 96)     221280      ['block6j_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6j_se_expand (Conv2D)     (None, 1, 1, 2304)   223488      ['block6j_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6j_se_excite (Multiply)   (None, 7, 7, 2304)   0           ['block6j_activation[0][0]',     Y          \n",
      "                                                                  'block6j_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block6j_project_conv (Conv2D)  (None, 7, 7, 384)    884736      ['block6j_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6j_project_bn (BatchNorma  (None, 7, 7, 384)   1536        ['block6j_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block6j_drop (FixedDropout)    (None, 7, 7, 384)    0           ['block6j_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6j_add (Add)              (None, 7, 7, 384)    0           ['block6j_drop[0][0]',           Y          \n",
      "                                                                  'block6i_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block6k_expand_conv (Conv2D)   (None, 7, 7, 2304)   884736      ['block6j_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block6k_expand_bn (BatchNormal  (None, 7, 7, 2304)  9216        ['block6k_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block6k_expand_activation (Act  (None, 7, 7, 2304)  0           ['block6k_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block6k_dwconv (DepthwiseConv2  (None, 7, 7, 2304)  57600       ['block6k_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block6k_bn (BatchNormalization  (None, 7, 7, 2304)  9216        ['block6k_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6k_activation (Activation  (None, 7, 7, 2304)  0           ['block6k_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6k_se_squeeze (GlobalAver  (None, 2304)        0           ['block6k_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block6k_se_reshape (Reshape)   (None, 1, 1, 2304)   0           ['block6k_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6k_se_reduce (Conv2D)     (None, 1, 1, 96)     221280      ['block6k_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6k_se_expand (Conv2D)     (None, 1, 1, 2304)   223488      ['block6k_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6k_se_excite (Multiply)   (None, 7, 7, 2304)   0           ['block6k_activation[0][0]',     Y          \n",
      "                                                                  'block6k_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block6k_project_conv (Conv2D)  (None, 7, 7, 384)    884736      ['block6k_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6k_project_bn (BatchNorma  (None, 7, 7, 384)   1536        ['block6k_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block6k_drop (FixedDropout)    (None, 7, 7, 384)    0           ['block6k_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6k_add (Add)              (None, 7, 7, 384)    0           ['block6k_drop[0][0]',           Y          \n",
      "                                                                  'block6j_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block6l_expand_conv (Conv2D)   (None, 7, 7, 2304)   884736      ['block6k_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block6l_expand_bn (BatchNormal  (None, 7, 7, 2304)  9216        ['block6l_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block6l_expand_activation (Act  (None, 7, 7, 2304)  0           ['block6l_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block6l_dwconv (DepthwiseConv2  (None, 7, 7, 2304)  57600       ['block6l_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block6l_bn (BatchNormalization  (None, 7, 7, 2304)  9216        ['block6l_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6l_activation (Activation  (None, 7, 7, 2304)  0           ['block6l_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6l_se_squeeze (GlobalAver  (None, 2304)        0           ['block6l_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block6l_se_reshape (Reshape)   (None, 1, 1, 2304)   0           ['block6l_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6l_se_reduce (Conv2D)     (None, 1, 1, 96)     221280      ['block6l_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6l_se_expand (Conv2D)     (None, 1, 1, 2304)   223488      ['block6l_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6l_se_excite (Multiply)   (None, 7, 7, 2304)   0           ['block6l_activation[0][0]',     Y          \n",
      "                                                                  'block6l_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block6l_project_conv (Conv2D)  (None, 7, 7, 384)    884736      ['block6l_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6l_project_bn (BatchNorma  (None, 7, 7, 384)   1536        ['block6l_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block6l_drop (FixedDropout)    (None, 7, 7, 384)    0           ['block6l_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6l_add (Add)              (None, 7, 7, 384)    0           ['block6l_drop[0][0]',           Y          \n",
      "                                                                  'block6k_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block6m_expand_conv (Conv2D)   (None, 7, 7, 2304)   884736      ['block6l_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block6m_expand_bn (BatchNormal  (None, 7, 7, 2304)  9216        ['block6m_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block6m_expand_activation (Act  (None, 7, 7, 2304)  0           ['block6m_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block6m_dwconv (DepthwiseConv2  (None, 7, 7, 2304)  57600       ['block6m_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block6m_bn (BatchNormalization  (None, 7, 7, 2304)  9216        ['block6m_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6m_activation (Activation  (None, 7, 7, 2304)  0           ['block6m_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block6m_se_squeeze (GlobalAver  (None, 2304)        0           ['block6m_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block6m_se_reshape (Reshape)   (None, 1, 1, 2304)   0           ['block6m_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6m_se_reduce (Conv2D)     (None, 1, 1, 96)     221280      ['block6m_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6m_se_expand (Conv2D)     (None, 1, 1, 2304)   223488      ['block6m_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6m_se_excite (Multiply)   (None, 7, 7, 2304)   0           ['block6m_activation[0][0]',     Y          \n",
      "                                                                  'block6m_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block6m_project_conv (Conv2D)  (None, 7, 7, 384)    884736      ['block6m_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block6m_project_bn (BatchNorma  (None, 7, 7, 384)   1536        ['block6m_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block6m_drop (FixedDropout)    (None, 7, 7, 384)    0           ['block6m_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block6m_add (Add)              (None, 7, 7, 384)    0           ['block6m_drop[0][0]',           Y          \n",
      "                                                                  'block6l_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block7a_expand_conv (Conv2D)   (None, 7, 7, 2304)   884736      ['block6m_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block7a_expand_bn (BatchNormal  (None, 7, 7, 2304)  9216        ['block7a_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block7a_expand_activation (Act  (None, 7, 7, 2304)  0           ['block7a_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block7a_dwconv (DepthwiseConv2  (None, 7, 7, 2304)  20736       ['block7a_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block7a_bn (BatchNormalization  (None, 7, 7, 2304)  9216        ['block7a_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block7a_activation (Activation  (None, 7, 7, 2304)  0           ['block7a_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block7a_se_squeeze (GlobalAver  (None, 2304)        0           ['block7a_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block7a_se_reshape (Reshape)   (None, 1, 1, 2304)   0           ['block7a_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block7a_se_reduce (Conv2D)     (None, 1, 1, 96)     221280      ['block7a_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block7a_se_expand (Conv2D)     (None, 1, 1, 2304)   223488      ['block7a_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block7a_se_excite (Multiply)   (None, 7, 7, 2304)   0           ['block7a_activation[0][0]',     Y          \n",
      "                                                                  'block7a_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block7a_project_conv (Conv2D)  (None, 7, 7, 640)    1474560     ['block7a_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block7a_project_bn (BatchNorma  (None, 7, 7, 640)   2560        ['block7a_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block7b_expand_conv (Conv2D)   (None, 7, 7, 3840)   2457600     ['block7a_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block7b_expand_bn (BatchNormal  (None, 7, 7, 3840)  15360       ['block7b_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block7b_expand_activation (Act  (None, 7, 7, 3840)  0           ['block7b_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block7b_dwconv (DepthwiseConv2  (None, 7, 7, 3840)  34560       ['block7b_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block7b_bn (BatchNormalization  (None, 7, 7, 3840)  15360       ['block7b_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block7b_activation (Activation  (None, 7, 7, 3840)  0           ['block7b_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block7b_se_squeeze (GlobalAver  (None, 3840)        0           ['block7b_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block7b_se_reshape (Reshape)   (None, 1, 1, 3840)   0           ['block7b_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block7b_se_reduce (Conv2D)     (None, 1, 1, 160)    614560      ['block7b_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block7b_se_expand (Conv2D)     (None, 1, 1, 3840)   618240      ['block7b_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block7b_se_excite (Multiply)   (None, 7, 7, 3840)   0           ['block7b_activation[0][0]',     Y          \n",
      "                                                                  'block7b_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block7b_project_conv (Conv2D)  (None, 7, 7, 640)    2457600     ['block7b_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block7b_project_bn (BatchNorma  (None, 7, 7, 640)   2560        ['block7b_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block7b_drop (FixedDropout)    (None, 7, 7, 640)    0           ['block7b_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block7b_add (Add)              (None, 7, 7, 640)    0           ['block7b_drop[0][0]',           Y          \n",
      "                                                                  'block7a_project_bn[0][0]']                \n",
      "                                                                                                             \n",
      " block7c_expand_conv (Conv2D)   (None, 7, 7, 3840)   2457600     ['block7b_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block7c_expand_bn (BatchNormal  (None, 7, 7, 3840)  15360       ['block7c_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block7c_expand_activation (Act  (None, 7, 7, 3840)  0           ['block7c_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block7c_dwconv (DepthwiseConv2  (None, 7, 7, 3840)  34560       ['block7c_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block7c_bn (BatchNormalization  (None, 7, 7, 3840)  15360       ['block7c_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block7c_activation (Activation  (None, 7, 7, 3840)  0           ['block7c_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block7c_se_squeeze (GlobalAver  (None, 3840)        0           ['block7c_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block7c_se_reshape (Reshape)   (None, 1, 1, 3840)   0           ['block7c_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block7c_se_reduce (Conv2D)     (None, 1, 1, 160)    614560      ['block7c_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block7c_se_expand (Conv2D)     (None, 1, 1, 3840)   618240      ['block7c_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block7c_se_excite (Multiply)   (None, 7, 7, 3840)   0           ['block7c_activation[0][0]',     Y          \n",
      "                                                                  'block7c_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block7c_project_conv (Conv2D)  (None, 7, 7, 640)    2457600     ['block7c_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block7c_project_bn (BatchNorma  (None, 7, 7, 640)   2560        ['block7c_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block7c_drop (FixedDropout)    (None, 7, 7, 640)    0           ['block7c_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block7c_add (Add)              (None, 7, 7, 640)    0           ['block7c_drop[0][0]',           Y          \n",
      "                                                                  'block7b_add[0][0]']                       \n",
      "                                                                                                             \n",
      " block7d_expand_conv (Conv2D)   (None, 7, 7, 3840)   2457600     ['block7c_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " block7d_expand_bn (BatchNormal  (None, 7, 7, 3840)  15360       ['block7d_expand_conv[0][0]']    Y          \n",
      " ization)                                                                                                    \n",
      "                                                                                                             \n",
      " block7d_expand_activation (Act  (None, 7, 7, 3840)  0           ['block7d_expand_bn[0][0]']      Y          \n",
      " ivation)                                                                                                    \n",
      "                                                                                                             \n",
      " block7d_dwconv (DepthwiseConv2  (None, 7, 7, 3840)  34560       ['block7d_expand_activation[0][  Y          \n",
      " D)                                                              0]']                                        \n",
      "                                                                                                             \n",
      " block7d_bn (BatchNormalization  (None, 7, 7, 3840)  15360       ['block7d_dwconv[0][0]']         Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block7d_activation (Activation  (None, 7, 7, 3840)  0           ['block7d_bn[0][0]']             Y          \n",
      " )                                                                                                           \n",
      "                                                                                                             \n",
      " block7d_se_squeeze (GlobalAver  (None, 3840)        0           ['block7d_activation[0][0]']     Y          \n",
      " agePooling2D)                                                                                               \n",
      "                                                                                                             \n",
      " block7d_se_reshape (Reshape)   (None, 1, 1, 3840)   0           ['block7d_se_squeeze[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block7d_se_reduce (Conv2D)     (None, 1, 1, 160)    614560      ['block7d_se_reshape[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block7d_se_expand (Conv2D)     (None, 1, 1, 3840)   618240      ['block7d_se_reduce[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block7d_se_excite (Multiply)   (None, 7, 7, 3840)   0           ['block7d_activation[0][0]',     Y          \n",
      "                                                                  'block7d_se_expand[0][0]']                 \n",
      "                                                                                                             \n",
      " block7d_project_conv (Conv2D)  (None, 7, 7, 640)    2457600     ['block7d_se_excite[0][0]']      Y          \n",
      "                                                                                                             \n",
      " block7d_project_bn (BatchNorma  (None, 7, 7, 640)   2560        ['block7d_project_conv[0][0]']   Y          \n",
      " lization)                                                                                                   \n",
      "                                                                                                             \n",
      " block7d_drop (FixedDropout)    (None, 7, 7, 640)    0           ['block7d_project_bn[0][0]']     Y          \n",
      "                                                                                                             \n",
      " block7d_add (Add)              (None, 7, 7, 640)    0           ['block7d_drop[0][0]',           Y          \n",
      "                                                                  'block7c_add[0][0]']                       \n",
      "                                                                                                             \n",
      " top_conv (Conv2D)              (None, 7, 7, 2560)   1638400     ['block7d_add[0][0]']            Y          \n",
      "                                                                                                             \n",
      " top_bn (BatchNormalization)    (None, 7, 7, 2560)   10240       ['top_conv[0][0]']               Y          \n",
      "                                                                                                             \n",
      " top_activation (Activation)    (None, 7, 7, 2560)   0           ['top_bn[0][0]']                 Y          \n",
      "                                                                                                             \n",
      " global_average_pooling2d (Glob  (None, 2560)        0           ['top_activation[0][0]']         Y          \n",
      " alAveragePooling2D)                                                                                         \n",
      "                                                                                                             \n",
      " dense (Dense)                  (None, 512)          1311232     ['global_average_pooling2d[0][0  Y          \n",
      "                                                                 ]']                                         \n",
      "                                                                                                             \n",
      " dropout (Dropout)              (None, 512)          0           ['dense[0][0]']                  Y          \n",
      "                                                                                                             \n",
      " batch_normalization (BatchNorm  (None, 512)         2048        ['dropout[0][0]']                Y          \n",
      " alization)                                                                                                  \n",
      "                                                                                                             \n",
      " dense_1 (Dense)                (None, 512)          262656      ['batch_normalization[0][0]']    Y          \n",
      "                                                                                                             \n",
      " batch_normalization_1 (BatchNo  (None, 512)         2048        ['dense_1[0][0]']                Y          \n",
      " rmalization)                                                                                                \n",
      "                                                                                                             \n",
      " dense_2 (Dense)                (None, 128)          65664       ['batch_normalization_1[0][0]']  Y          \n",
      "                                                                                                             \n",
      " dense_3 (Dense)                (None, 2)            258         ['dense_2[0][0]']                Y          \n",
      "                                                                                                             \n",
      "=============================================================================================================\n",
      "Total params: 65,741,586\n",
      "Trainable params: 65,428,818\n",
      "Non-trainable params: 312,768\n",
      "_____________________________________________________________________________________________________________\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "from efficientnet.keras import EfficientNetB7 as KENB7\n",
    "# FUNC\n",
    "def Eff_B7_NS(freeze_layers):\n",
    "    base_model = KENB7(input_shape=(\n",
    "        img_res[0], img_res[1], img_res[2]), weights='noisy-student', include_top=False)\n",
    "    print('Total layers in the base model: ', len(base_model.layers))\n",
    "    print(f'Freezing {freeze_layers} layers in the base model...')\n",
    "    # Freeze the specified number of layers\n",
    "    for layer in base_model.layers[:freeze_layers]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Unfreeze the rest\n",
    "    for layer in base_model.layers[freeze_layers:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Calculate the percentage of the model that is frozen\n",
    "    frozen_percentage = ((freeze_layers + 1e-10) /\n",
    "                         len(base_model.layers)) * 100\n",
    "    print(\n",
    "        f'Percentage of the base model that is frozen: {frozen_percentage:.2f}%')\n",
    "    # adding CDL\n",
    "    base_model_FT = GlobalAveragePooling2D()(base_model.output)\n",
    "    Dense_L1 = Dense(512, activation='relu',\n",
    "                     kernel_regularizer=l2(0.02))(base_model_FT)\n",
    "    Dropout_L1 = Dropout(0.1)(Dense_L1)\n",
    "    BatchNorm_L2 = BatchNormalization()(Dropout_L1)\n",
    "    Dense_L2 = Dense(512, activation='relu',\n",
    "                     kernel_regularizer=l2(0.01))(BatchNorm_L2)\n",
    "    BatchNorm_L3 = BatchNormalization()(Dense_L2)\n",
    "    Dense_L3 = Dense(128, activation='relu')(BatchNorm_L3)\n",
    "    # predictions = Dense(2, activation='softmax')(Dense_L3) / predictions = Dense(1, activation='sigmoid')(Dense_L3)\n",
    "    predictions = Dense(2, activation='softmax')(Dense_L3)\n",
    "\n",
    "    model_EfficientNetB7_NS = Model(\n",
    "        inputs=base_model.input, outputs=predictions)\n",
    "    print('Total model layers: ', len(model_EfficientNetB7_NS.layers))\n",
    "    # OPT/compile\n",
    "    opt = SGD(momentum=0.9, nesterov=False)\n",
    "    # opt = Nadam()\n",
    "    # opt = Adamax()\n",
    "    # opt = RMSprop(momentum=0.9)\n",
    "    # opt = Adagrad()\n",
    "    # opt = AdaBeliefOptimizer(epsilon=1e-7, rectify=False, weight_decay=5e-4, print_change_log=False, total_steps=0, amsgrad=False)\n",
    "    # opt = Yogi()\n",
    "    model_EfficientNetB7_NS.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])  # categorical_crossentropy / binary_crossentropy\n",
    "\n",
    "    return model_EfficientNetB7_NS\n",
    "\n",
    "print('Creating the model...')\n",
    "# Main\n",
    "freeze_layers = 0\n",
    "model = Eff_B7_NS(freeze_layers)\n",
    "model.summary(show_trainable=True, expand_nested=True)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rev1.3\n",
    "```\n",
    "recommended: ❌\n",
    "statuses: Test\n",
    "Working: ✅\n",
    "Max fine tuned acc: ⚠️\n",
    "Max fine tuned acc TLRev2: ⚠️\n",
    "type: transfer learning>>>(EfficientNetB7|Xception::CCL)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet.keras import EfficientNetB7 as KENB7\n",
    "from keras.applications.xception import Xception\n",
    "\n",
    "#FUNC\n",
    "def Combo_Model(freeze_layers1, freeze_layers2):\n",
    "    # Define a common input\n",
    "    common_input = Input(shape=(img_res[0], img_res[1], img_res[2]))\n",
    "\n",
    "    # Base model 1\n",
    "    base_model1 = KENB7(input_shape=(img_res[0], img_res[1], img_res[2]), weights='noisy-student', include_top=False)\n",
    "    # base_model1.load_weights('models\\Ready\\Other\\EfficientNetB7_PRET.h5', by_name=True, skip_mismatch=True)\n",
    "    base_model1_out = base_model1(common_input)\n",
    "    \n",
    "    # Base model 2\n",
    "    base_model2 = Xception(input_shape=(img_res[0], img_res[1], img_res[2]), weights='imagenet', include_top=False)\n",
    "    # base_model1.load_weights('models\\Ready\\Other\\Xception_PRET.h5', by_name=True, skip_mismatch=True)\n",
    "    base_model2_out = base_model2(common_input)\n",
    "\n",
    "    print('Total base_model1 layers: ', len(base_model1.layers))\n",
    "    print('Total base_model2 layers: ', len(base_model2.layers))\n",
    "    \n",
    "    # Freeze the specified number of layers in both models\n",
    "    for layer in base_model1.layers[:freeze_layers1]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model2.layers[:freeze_layers2]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Unfreeze the rest in both models\n",
    "    for layer in base_model1.layers[freeze_layers1:]:\n",
    "        layer.trainable = True\n",
    "    for layer in base_model2.layers[freeze_layers2:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Combine the output of the two base models\n",
    "    combined = concatenate([GlobalAveragePooling2D()(base_model1_out), GlobalAveragePooling2D()(base_model2_out)])\n",
    "\n",
    "    # adding CDL\n",
    "    Dense_L1 = Dense(1024, activation='relu', kernel_regularizer=l2(0.03))(combined)\n",
    "    Dropout_L1 = Dropout(0.4)(Dense_L1) \n",
    "    BatchNorm_L2 = BatchNormalization()(Dropout_L1)\n",
    "    Dense_L2 = Dense(512, activation='relu', kernel_regularizer=l2(0.02))(BatchNorm_L2)\n",
    "    BatchNorm_L3 = BatchNormalization()(Dense_L2)\n",
    "    Dense_L3 = Dense(128, activation='relu')(BatchNorm_L3)\n",
    "    predictions = Dense(2, activation='softmax')(Dense_L3)\n",
    "\n",
    "    combo_model = Model(inputs=common_input, outputs=predictions)   \n",
    "    print('Total model layers: ', len(combo_model.layers))\n",
    "    \n",
    "    #OPT/compile\n",
    "    opt = SGD(momentum=0.9)\n",
    "    combo_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return combo_model\n",
    "\n",
    "print('Creating the model...')\n",
    "# Main\n",
    "freeze_layers_1 = 0\n",
    "freeze_layers_2 = 0\n",
    "model = Combo_Model(freeze_layers_1, freeze_layers_2)\n",
    "model.summary(show_trainable=True, expand_nested=True)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rev1.4\n",
    "```\n",
    "recommended: ⚠️\n",
    "statuses: Test\n",
    "Working: ✅\n",
    "Max fine tuned acc: ⚠️\n",
    "Max fine tuned acc TLRev2: ≅95.64\n",
    "type: transfer learning>>>(EfficientNetV2XL)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_efficientnet_v2 import EfficientNetV2XL\n",
    "\n",
    "EfficientNet_M = EfficientNetV2XL(input_shape=(img_res[0], img_res[1], img_res[2]), pretrained='imagenet21k-ft1k', num_classes=2, dropout=0.4)\n",
    "# define new model\n",
    "model = Model(inputs=EfficientNet_M.inputs, outputs=EfficientNet_M.outputs)\n",
    "\n",
    "# compile model\n",
    "# opt = SGD(momentum=0.9)\n",
    "opt = AdaBeliefOptimizer(epsilon=1e-7, rectify=False, weight_decay=1e-2, print_change_log=False, total_steps=0, amsgrad=False)\n",
    "# opt = AdaBeliefOptimizer(epsilon=1e-7, rectify=False, weight_decay=1e-3)\n",
    "# opt = Adam()\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "freeze_layers = 0\n",
    "model.summary(show_trainable=True, expand_nested=True)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V(T) Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet.keras import EfficientNetL2 as KENBL2\n",
    "#FUNC\n",
    "def Eff_B7_NS(freeze_layers):\n",
    "    base_model = KENBL2(input_shape=(img_res[0], img_res[1], img_res[2]),\n",
    "                        weights='./download/Models/EFN_L2/efficientnet-l2_noisy-student_notop.h5',\n",
    "                        include_top=False,\n",
    "                        drop_connect_rate=0)\n",
    "    print('Total layers in the base model: ', len(base_model.layers))\n",
    "    print(f'Freezing {freeze_layers} layers in the base model...')\n",
    "    # Freeze the specified number of layers\n",
    "    for layer in base_model.layers[:freeze_layers]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Unfreeze the rest\n",
    "    for layer in base_model.layers[freeze_layers:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Calculate the percentage of the model that is frozen\n",
    "    frozen_percentage = ((freeze_layers + 1e-10) / len(base_model.layers)) * 100\n",
    "    print(f'Percentage of the base model that is frozen: {frozen_percentage:.2f}%')\n",
    "    # adding CDL\n",
    "    base_model_FT = GlobalAveragePooling2D()(base_model.output)\n",
    "    Dense_L1 = Dense(512, activation='relu', kernel_regularizer=l2(0.02))(base_model_FT)\n",
    "    Dropout_L1 = Dropout(0.1)(Dense_L1) \n",
    "    BatchNorm_L2 = BatchNormalization()(Dropout_L1)\n",
    "    Dense_L2 = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(BatchNorm_L2)\n",
    "    BatchNorm_L3 = BatchNormalization()(Dense_L2)\n",
    "    Dense_L3 = Dense(128, activation='relu')(BatchNorm_L3)\n",
    "    predictions = Dense(2, activation='softmax')(Dense_L3)\n",
    "\n",
    "    model_EfficientNetB7_NS = Model(inputs=base_model.input, outputs=predictions)   \n",
    "    print('Total model layers: ', len(model_EfficientNetB7_NS.layers))\n",
    "    #OPT/compile\n",
    "    opt = SGD(momentum=0.9)\n",
    "    # opt = Yogi()\n",
    "    model_EfficientNetB7_NS.compile(optimizer = opt,  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model_EfficientNetB7_NS\n",
    "print('Creating the model...')\n",
    "# Main\n",
    "freeze_layers = 0\n",
    "model = Eff_B7_NS(freeze_layers)\n",
    "model.summary(show_trainable=True, expand_nested=True)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V(T) Beta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_efficientnet_v2 import EfficientNetV2M\n",
    "\n",
    "EfficientNet_M = EfficientNetV2M(input_shape=(img_res[0], img_res[1], img_res[2]), num_classes=2, dropout=0.5)\n",
    "# define new model\n",
    "model = Model(inputs=EfficientNet_M.inputs, outputs=EfficientNet_M.outputs)\n",
    "\n",
    "# compile model\n",
    "opt = SGD(momentum=0.9)\n",
    "# opt = AdaBeliefOptimizer(epsilon=1e-7, rectify=False, weight_decay=1e-3)\n",
    "# opt = Adam()\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "freeze_layers = 0\n",
    "model.summary(show_trainable=True, expand_nested=True)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V(T) Beta3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ConvNeXtXLarge\n",
    "from keras.layers import Lambda\n",
    "#FUNC\n",
    "def Eff_B7_NS():\n",
    "    # Add a Lambda layer at the beginning to scale the input\n",
    "    input = Input(shape=(img_res[0], img_res[1], img_res[2]))\n",
    "    x = Lambda(lambda image: image * 255)(input)\n",
    "    \n",
    "    base_model = ConvNeXtXLarge(include_top=False, weights='imagenet', classes=2, classifier_activation='softmax', include_preprocessing=True)(x)\n",
    "    # adding CDL\n",
    "    base_model_FT = GlobalAveragePooling2D()(base_model)\n",
    "    Dense_L1 = Dense(512, activation='relu', kernel_regularizer=l2(0.02))(base_model_FT)\n",
    "    Dropout_L1 = Dropout(0.1)(Dense_L1) \n",
    "    BatchNorm_L2 = BatchNormalization()(Dropout_L1)\n",
    "    Dense_L2 = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(BatchNorm_L2)\n",
    "    BatchNorm_L3 = BatchNormalization()(Dense_L2)\n",
    "    Dense_L3 = Dense(128, activation='relu')(BatchNorm_L3)\n",
    "    predictions = Dense(2, activation='softmax')(Dense_L3)\n",
    "\n",
    "    model_EfficientNetB7_NS = Model(inputs=input, outputs=predictions)   \n",
    "    print('Total model layers: ', len(model_EfficientNetB7_NS.layers))\n",
    "    #OPT/compile\n",
    "    opt = SGD(momentum=0.9)\n",
    "    # opt = Yogi()\n",
    "    model_EfficientNetB7_NS.compile(optimizer=opt,  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model_EfficientNetB7_NS\n",
    "\n",
    "print('Creating the model...')\n",
    "# Main\n",
    "model = Eff_B7_NS()\n",
    "model.summary(show_trainable=True, expand_nested=True)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR FINDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "# Garbage Collection (memory)\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "#CONF/Other\n",
    "LRF_OPT = SGD(momentum=0.9)\n",
    "LFR_batch_size = 1  # or any other batch size that fits in your memory\n",
    "LRF_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(LFR_batch_size)\n",
    "# Instantiate LrFinder\n",
    "lr_find = LrFinder(model, LRF_OPT, tf.keras.losses.categorical_crossentropy)\n",
    "\n",
    "# Start range_test\n",
    "lr_find.range_test(LRF_dataset)\n",
    "lr_find.plot_lrs(skip_end=0, suggestion=True, show_grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_img_file = 'model_1.png'\n",
    "keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficientnet.tfkeras\n",
    "# Configuration\n",
    "PRMC = False\n",
    "freeze_from_opposite = False\n",
    "Extra_EXT = '_T'\n",
    "freeze_layers = 0  \n",
    "randomly_frozen_layers = 0 \n",
    "freeze_last_seven = True  \n",
    "# CEC_opt = Adagrad()\n",
    "# CEC_opt = Yogi()\n",
    "# CEC_opt = AdaBeliefOptimizer(epsilon=1e-7, rectify=False, weight_decay=1e-3)\n",
    "CEC_opt = SGD(momentum=0.9, nesterov=False)\n",
    "# CEC_opt = Adam()\n",
    "# Main\n",
    "try:\n",
    "    if SAVE_TYPE == 'TF':\n",
    "        model = load_model(f'PAI_model{Extra_EXT}', compile=PRMC)\n",
    "    else:\n",
    "        model = load_model(f'PAI_model{Extra_EXT}.h5', compile=PRMC)\n",
    "except (ImportError, IOError) as e:\n",
    "    print(f'\\033[91mfailed to load the model ERROR:\\n{e}')\n",
    "else:\n",
    "    print('\\033[92mLoading model done.')\n",
    "    if not PRMC:\n",
    "        print('Compiling the AI model...\\033[0m')\n",
    "        \n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "        \n",
    "        # Select random layers to freeze\n",
    "        frozen_layer_indices = random.sample(range(len(model.layers)), randomly_frozen_layers)\n",
    "        \n",
    "        for i, layer in enumerate(model.layers):\n",
    "            if i in frozen_layer_indices:\n",
    "                layer.trainable = False\n",
    "            else:\n",
    "                if freeze_from_opposite and (i > len(model.layers) - freeze_layers):\n",
    "                    layer.trainable = False\n",
    "                elif (not freeze_from_opposite) and i < freeze_layers:\n",
    "                    layer.trainable = False\n",
    "                else:\n",
    "                    layer.trainable = True\n",
    "        \n",
    "        for layer in model.layers[-7:]:\n",
    "            layer.trainable = not freeze_last_seven\n",
    "            \n",
    "        model.compile(optimizer=CEC_opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.summary(show_trainable=True, expand_nested=True)\n",
    "        print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('PAI_model_weights.h5')\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[-7:]:\n",
    "    if hasattr(layer, 'kernel_initializer') and hasattr(layer, 'bias_initializer'):\n",
    "        weight_initializer = layer.kernel_initializer\n",
    "        bias_initializer = layer.bias_initializer\n",
    "\n",
    "        old_weights, old_biases = layer.get_weights()\n",
    "\n",
    "        layer.set_weights([\n",
    "            weight_initializer(shape=old_weights.shape),\n",
    "            bias_initializer(shape=len(old_biases))\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rev2 (THE BEST)\n",
    "```\n",
    "Working: ✅\n",
    "Other:\n",
    " + Tensorboard works.\n",
    " + Perverts overfitting.\n",
    " + Lower memory usage.\n",
    " - Slow training.\n",
    " + Achieving higher acc.\n",
    " - Some models dont work.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "\u001b[0;33m\n",
      "Setup Verbose:\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSetting TensorBoard Log dir to \u001b[0m\u001b[0;32m[logs/fit/y2023_m12_d21-h22_m11_s55]\u001b[0m\u001b[0;36m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mUse_extended_tensorboard \u001b[0m\u001b[0;32m[True]\u001b[0m\u001b[0;36m.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mDebug_OUTPUT_DPS \u001b[0m\u001b[0;32m[True]\u001b[0m\u001b[0;36m.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mOneCycleLr_UFTS \u001b[0m\u001b[0;32m[False]\u001b[0m\u001b[0;36m.\u001b[0m\n",
      "\u001b[0;33mSetup Verbose END.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m1\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 0)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Loading fitted ImageDataGenerator...\u001b[0m\n",
      "\u001b[0;33m- ImageDataGenerator fit done.\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;31m- Debug DP Sample dir: \u001b[0m\u001b[0;32mSamples/TSR_SUB_400_y2023_m12_d21-h22_m12_s25\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 1/6\n",
      "143/256 [===============>..............] - ETA: 16s - loss: 21.5970 - accuracy: 0.5953\u001b[0;31m\n",
      "Pausing training due to high GPU temperature! (for [60]sec)\u001b[0m\n",
      "\u001b[0;33mResuming training...\u001b[0m\n",
      "256/256 [==============================] - 125s 425ms/step - loss: 18.7877 - accuracy: 0.6108 - val_loss: 11.4826 - val_accuracy: 0.7083 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 2/6\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 6.3373 - accuracy: 0.6782 - val_loss: 2.9446 - val_accuracy: 0.7756 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 3/6\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 1.9396 - accuracy: 0.7393 - val_loss: 1.3086 - val_accuracy: 0.8750 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 4/6\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.9828 - accuracy: 0.7778 - val_loss: 0.6867 - val_accuracy: 0.9054 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 5/6\n",
      "256/256 [==============================] - 49s 192ms/step - loss: 0.6569 - accuracy: 0.8315 - val_loss: 0.4673 - val_accuracy: 0.9151 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 6/6\n",
      "256/256 [==============================] - 50s 195ms/step - loss: 0.5070 - accuracy: 0.8760 - val_loss: 0.4048 - val_accuracy: 0.9231 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-006-0.9231.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9231\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.4048\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model accuracy from \u001b[0m\u001b[0;32m0 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.9230769276618958\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model loss from \u001b[0m\u001b[0;32minf \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.4047916531562805\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m414.90 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m362.92 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m51.98 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [1] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m2\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 6)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 7/12\n",
      "256/256 [==============================] - 51s 184ms/step - loss: 0.5474 - accuracy: 0.8428 - val_loss: 0.3377 - val_accuracy: 0.9199 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 8/12\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.5264 - accuracy: 0.8350 - val_loss: 1.1843 - val_accuracy: 0.6667 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 9/12\n",
      "256/256 [==============================] - 51s 198ms/step - loss: 0.5311 - accuracy: 0.8145 - val_loss: 0.2925 - val_accuracy: 0.9247 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 10/12\n",
      "256/256 [==============================] - 49s 190ms/step - loss: 0.4503 - accuracy: 0.8555 - val_loss: 0.5356 - val_accuracy: 0.8462 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 11/12\n",
      "256/256 [==============================] - 50s 194ms/step - loss: 0.4070 - accuracy: 0.8491 - val_loss: 0.3295 - val_accuracy: 0.9054 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 12/12\n",
      "207/256 [=======================>......] - ETA: 7s - loss: 0.3178 - accuracy: 0.8979\u001b[0;31m\n",
      "Pausing training due to high GPU temperature! (for [60]sec)\u001b[0m\n",
      "\u001b[0;33mResuming training...\u001b[0m\n",
      "256/256 [==============================] - 109s 428ms/step - loss: 0.3238 - accuracy: 0.8989 - val_loss: 0.2795 - val_accuracy: 0.9103 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-009-0.9247.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9247\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2925\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model accuracy from \u001b[0m\u001b[0;32m0.9230769276618958 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.9246794581413269\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model loss from \u001b[0m\u001b[0;32m0.4047916531562805 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.2925480306148529\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m400.69 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m358.11 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m42.58 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [2] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m3\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 12)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 13/18\n",
      "256/256 [==============================] - 55s 202ms/step - loss: 0.4700 - accuracy: 0.8335 - val_loss: 0.3299 - val_accuracy: 0.9103 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 14/18\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.4631 - accuracy: 0.8262 - val_loss: 0.3475 - val_accuracy: 0.9087 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 15/18\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.4167 - accuracy: 0.8574 - val_loss: 0.3250 - val_accuracy: 0.9151 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 16/18\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.4224 - accuracy: 0.8555 - val_loss: 0.3555 - val_accuracy: 0.8926 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 17/18\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.3426 - accuracy: 0.8823 - val_loss: 0.2659 - val_accuracy: 0.9247 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 18/18\n",
      "256/256 [==============================] - 45s 174ms/step - loss: 0.2764 - accuracy: 0.9033 - val_loss: 0.2557 - val_accuracy: 0.9247 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-017-0.9247.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9247\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2659\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9246794581413269. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model loss from \u001b[0m\u001b[0;32m0.2925480306148529 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.26587748527526855\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m313.02 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m280.98 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m32.03 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [3] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m4\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 18)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 19/24\n",
      "256/256 [==============================] - 49s 179ms/step - loss: 0.3731 - accuracy: 0.8628 - val_loss: 0.2503 - val_accuracy: 0.9263 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 20/24\n",
      "256/256 [==============================] - 44s 172ms/step - loss: 0.4015 - accuracy: 0.8457 - val_loss: 0.2964 - val_accuracy: 0.9215 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 21/24\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.4235 - accuracy: 0.8433 - val_loss: 0.2734 - val_accuracy: 0.9295 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 22/24\n",
      "256/256 [==============================] - 44s 172ms/step - loss: 0.3687 - accuracy: 0.8643 - val_loss: 0.3075 - val_accuracy: 0.9279 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 23/24\n",
      "256/256 [==============================] - 44s 171ms/step - loss: 0.3025 - accuracy: 0.8975 - val_loss: 0.3312 - val_accuracy: 0.8494 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 24/24\n",
      "256/256 [==============================] - 44s 172ms/step - loss: 0.2619 - accuracy: 0.9121 - val_loss: 0.2756 - val_accuracy: 0.8718 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-021-0.9295.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9295\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2733\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model accuracy from \u001b[0m\u001b[0;32m0.9246794581413269 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.9294871687889099\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.26587748527526855. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m304.80 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m271.53 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m33.27 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [4] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m5\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 24)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 25/30\n",
      "256/256 [==============================] - 49s 180ms/step - loss: 0.4056 - accuracy: 0.8521 - val_loss: 0.2638 - val_accuracy: 0.9231 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 26/30\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.4220 - accuracy: 0.8540 - val_loss: 0.4272 - val_accuracy: 0.8510 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 27/30\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.4353 - accuracy: 0.8315 - val_loss: 0.3853 - val_accuracy: 0.9231 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 28/30\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.4237 - accuracy: 0.8442 - val_loss: 0.2622 - val_accuracy: 0.9311 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 29/30\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.3540 - accuracy: 0.8809 - val_loss: 0.2614 - val_accuracy: 0.9359 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 30/30\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2650 - accuracy: 0.9170 - val_loss: 0.2279 - val_accuracy: 0.9295 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-029-0.9359.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9359\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2614\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model accuracy from \u001b[0m\u001b[0;32m0.9294871687889099 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.9358974099159241\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model loss from \u001b[0m\u001b[0;32m0.26587748527526855 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.26140671968460083\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m312.07 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m277.58 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m34.48 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [5] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m6\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 30)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 31/36\n",
      "256/256 [==============================] - 55s 200ms/step - loss: 0.3614 - accuracy: 0.8853 - val_loss: 0.3434 - val_accuracy: 0.9247 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 32/36\n",
      "256/256 [==============================] - 50s 194ms/step - loss: 0.3995 - accuracy: 0.8574 - val_loss: 0.4120 - val_accuracy: 0.8846 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 33/36\n",
      "256/256 [==============================] - 50s 194ms/step - loss: 0.3560 - accuracy: 0.8770 - val_loss: 0.3237 - val_accuracy: 0.8910 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 34/36\n",
      "256/256 [==============================] - 50s 193ms/step - loss: 0.3332 - accuracy: 0.8862 - val_loss: 0.3489 - val_accuracy: 0.8702 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 35/36\n",
      "256/256 [==============================] - 49s 192ms/step - loss: 0.2828 - accuracy: 0.9077 - val_loss: 0.2354 - val_accuracy: 0.9087 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 36/36\n",
      "256/256 [==============================] - 50s 196ms/step - loss: 0.2543 - accuracy: 0.9087 - val_loss: 0.2251 - val_accuracy: 0.9327 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-036-0.9327.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9327\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2251\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9358974099159241. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model loss from \u001b[0m\u001b[0;32m0.26140671968460083 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.22506119310855865\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m338.91 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m304.51 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m34.40 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [6] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m7\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 36)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 37/42\n",
      "256/256 [==============================] - 56s 202ms/step - loss: 0.3474 - accuracy: 0.8711 - val_loss: 0.3355 - val_accuracy: 0.9343 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 38/42\n",
      "256/256 [==============================] - 50s 196ms/step - loss: 0.3520 - accuracy: 0.8970 - val_loss: 0.3168 - val_accuracy: 0.8814 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 39/42\n",
      "256/256 [==============================] - 49s 189ms/step - loss: 0.3267 - accuracy: 0.8945 - val_loss: 0.3317 - val_accuracy: 0.9263 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 40/42\n",
      "256/256 [==============================] - 49s 191ms/step - loss: 0.3790 - accuracy: 0.8872 - val_loss: 0.3114 - val_accuracy: 0.9103 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 41/42\n",
      "256/256 [==============================] - 50s 195ms/step - loss: 0.2749 - accuracy: 0.9233 - val_loss: 0.2513 - val_accuracy: 0.9103 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 42/42\n",
      "256/256 [==============================] - 49s 192ms/step - loss: 0.2435 - accuracy: 0.9312 - val_loss: 0.2553 - val_accuracy: 0.9103 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-037-0.9343.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9343\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3355\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9358974099159241. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.22506119310855865. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m343.71 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m304.46 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m39.26 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [7] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m8\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 42)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 43/48\n",
      "256/256 [==============================] - 57s 204ms/step - loss: 0.3539 - accuracy: 0.8828 - val_loss: 0.2467 - val_accuracy: 0.9375 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 44/48\n",
      "256/256 [==============================] - 50s 195ms/step - loss: 0.4044 - accuracy: 0.8677 - val_loss: 0.3088 - val_accuracy: 0.9199 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 45/48\n",
      "256/256 [==============================] - 49s 192ms/step - loss: 0.3713 - accuracy: 0.8667 - val_loss: 0.2881 - val_accuracy: 0.9375 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 46/48\n",
      "256/256 [==============================] - 49s 191ms/step - loss: 0.3581 - accuracy: 0.8765 - val_loss: 0.3279 - val_accuracy: 0.9135 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 47/48\n",
      "256/256 [==============================] - 49s 192ms/step - loss: 0.2639 - accuracy: 0.9263 - val_loss: 0.2231 - val_accuracy: 0.9295 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 48/48\n",
      "256/256 [==============================] - 49s 193ms/step - loss: 0.2355 - accuracy: 0.9238 - val_loss: 0.2298 - val_accuracy: 0.9327 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-043-0.9375.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9375\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2467\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model accuracy from \u001b[0m\u001b[0;32m0.9358974099159241 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.9375\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.22506119310855865. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m345.83 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m305.03 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m40.80 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [8] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m9\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 48)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 49/54\n",
      "256/256 [==============================] - 55s 199ms/step - loss: 0.3562 - accuracy: 0.8745 - val_loss: 0.2796 - val_accuracy: 0.8974 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 50/54\n",
      "256/256 [==============================] - 51s 200ms/step - loss: 0.3632 - accuracy: 0.8892 - val_loss: 0.2843 - val_accuracy: 0.9327 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 51/54\n",
      "256/256 [==============================] - 50s 194ms/step - loss: 0.3904 - accuracy: 0.8652 - val_loss: 0.3229 - val_accuracy: 0.8894 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 52/54\n",
      "256/256 [==============================] - 50s 195ms/step - loss: 0.3412 - accuracy: 0.8926 - val_loss: 0.2218 - val_accuracy: 0.9487 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 53/54\n",
      "256/256 [==============================] - 50s 193ms/step - loss: 0.2635 - accuracy: 0.9204 - val_loss: 0.2349 - val_accuracy: 0.9327 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 54/54\n",
      "256/256 [==============================] - 49s 190ms/step - loss: 0.2201 - accuracy: 0.9331 - val_loss: 0.2753 - val_accuracy: 0.8910 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-052-0.9487.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9487\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2218\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model accuracy from \u001b[0m\u001b[0;32m0.9375 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.9487179517745972\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model loss from \u001b[0m\u001b[0;32m0.22506119310855865 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.22177116572856903\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m349.93 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m305.99 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m43.93 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [9] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m10\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 54)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 55/60\n",
      "256/256 [==============================] - 57s 205ms/step - loss: 0.3582 - accuracy: 0.8853 - val_loss: 1.1253 - val_accuracy: 0.6955 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 56/60\n",
      "256/256 [==============================] - 51s 196ms/step - loss: 0.3713 - accuracy: 0.8823 - val_loss: 0.4083 - val_accuracy: 0.7885 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 57/60\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.3291 - accuracy: 0.8848 - val_loss: 0.3009 - val_accuracy: 0.8606 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 58/60\n",
      "256/256 [==============================] - 50s 195ms/step - loss: 0.3178 - accuracy: 0.9058 - val_loss: 0.4083 - val_accuracy: 0.8590 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 59/60\n",
      "256/256 [==============================] - 51s 199ms/step - loss: 0.2571 - accuracy: 0.9307 - val_loss: 0.2562 - val_accuracy: 0.9038 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 60/60\n",
      "256/256 [==============================] - 51s 198ms/step - loss: 0.2568 - accuracy: 0.9209 - val_loss: 0.2685 - val_accuracy: 0.9038 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-059-0.9038.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9038\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2562\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.22177116572856903. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m350.67 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m311.58 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m39.08 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [10] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m11\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 60)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 61/66\n",
      "256/256 [==============================] - 57s 205ms/step - loss: 0.3156 - accuracy: 0.8970 - val_loss: 0.2816 - val_accuracy: 0.9119 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 62/66\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.3533 - accuracy: 0.8877 - val_loss: 0.2945 - val_accuracy: 0.9183 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 63/66\n",
      "256/256 [==============================] - 50s 196ms/step - loss: 0.3629 - accuracy: 0.8882 - val_loss: 0.2772 - val_accuracy: 0.9103 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 64/66\n",
      "256/256 [==============================] - 50s 195ms/step - loss: 0.3021 - accuracy: 0.9136 - val_loss: 0.3078 - val_accuracy: 0.9054 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 65/66\n",
      "256/256 [==============================] - 50s 193ms/step - loss: 0.2472 - accuracy: 0.9219 - val_loss: 0.4162 - val_accuracy: 0.8189 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 66/66\n",
      "256/256 [==============================] - 51s 200ms/step - loss: 0.2013 - accuracy: 0.9517 - val_loss: 0.2229 - val_accuracy: 0.9263 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-066-0.9263.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9263\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2229\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.22177116572856903. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m350.09 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m311.16 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m38.92 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [11] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m12\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 66)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 67/72\n",
      "256/256 [==============================] - 55s 196ms/step - loss: 0.2781 - accuracy: 0.9111 - val_loss: 0.5012 - val_accuracy: 0.8237 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 68/72\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.3271 - accuracy: 0.9038 - val_loss: 0.2999 - val_accuracy: 0.9279 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 69/72\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.3290 - accuracy: 0.9087 - val_loss: 0.3948 - val_accuracy: 0.9279 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 70/72\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.3285 - accuracy: 0.8979 - val_loss: 0.3564 - val_accuracy: 0.9263 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 71/72\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2478 - accuracy: 0.9302 - val_loss: 0.2477 - val_accuracy: 0.9279 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 72/72\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1993 - accuracy: 0.9458 - val_loss: 0.2491 - val_accuracy: 0.9327 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-072-0.9327.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9327\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2491\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.22177116572856903. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m323.09 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m283.75 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m39.34 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [12] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m13\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 72)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 73/78\n",
      "256/256 [==============================] - 50s 183ms/step - loss: 0.2939 - accuracy: 0.9072 - val_loss: 0.3677 - val_accuracy: 0.9343 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 74/78\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.3566 - accuracy: 0.8843 - val_loss: 0.3543 - val_accuracy: 0.9247 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 75/78\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.3509 - accuracy: 0.8916 - val_loss: 0.3390 - val_accuracy: 0.9167 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 76/78\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.3289 - accuracy: 0.9097 - val_loss: 0.3772 - val_accuracy: 0.9135 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 77/78\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2213 - accuracy: 0.9404 - val_loss: 0.2276 - val_accuracy: 0.9423 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 78/78\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1726 - accuracy: 0.9600 - val_loss: 0.2253 - val_accuracy: 0.9407 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-077-0.9423.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9423\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2276\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.22177116572856903. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m312.50 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m279.87 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m32.63 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [13] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m14\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 78)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 79/84\n",
      "256/256 [==============================] - 50s 184ms/step - loss: 0.2854 - accuracy: 0.9194 - val_loss: 0.3002 - val_accuracy: 0.9263 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 80/84\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.3165 - accuracy: 0.8979 - val_loss: 0.4973 - val_accuracy: 0.8077 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 81/84\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.3448 - accuracy: 0.8999 - val_loss: 0.4124 - val_accuracy: 0.9231 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 82/84\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.3198 - accuracy: 0.9214 - val_loss: 0.2585 - val_accuracy: 0.9279 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 83/84\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.2571 - accuracy: 0.9268 - val_loss: 0.2274 - val_accuracy: 0.9311 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 84/84\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2081 - accuracy: 0.9497 - val_loss: 0.2196 - val_accuracy: 0.9359 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-084-0.9359.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9359\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2196\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model loss from \u001b[0m\u001b[0;32m0.22177116572856903 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.2195880115032196\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m316.10 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m281.32 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m34.78 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [14] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m15\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 84)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 85/90\n",
      "256/256 [==============================] - 50s 183ms/step - loss: 0.2659 - accuracy: 0.9204 - val_loss: 0.2711 - val_accuracy: 0.9407 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 86/90\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.3555 - accuracy: 0.8950 - val_loss: 0.4089 - val_accuracy: 0.8333 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 87/90\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.3097 - accuracy: 0.9155 - val_loss: 0.3248 - val_accuracy: 0.9151 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 88/90\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.2897 - accuracy: 0.9131 - val_loss: 0.2811 - val_accuracy: 0.9135 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 89/90\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2093 - accuracy: 0.9497 - val_loss: 0.2760 - val_accuracy: 0.9247 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 90/90\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1866 - accuracy: 0.9536 - val_loss: 0.2868 - val_accuracy: 0.9151 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-085-0.9407.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9407\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2711\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.2195880115032196. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m312.70 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m279.38 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m33.32 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [15] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m16\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 90)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 91/96\n",
      "256/256 [==============================] - 50s 183ms/step - loss: 0.3088 - accuracy: 0.9058 - val_loss: 0.2267 - val_accuracy: 0.9359 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 92/96\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.3698 - accuracy: 0.8774 - val_loss: 0.3892 - val_accuracy: 0.9215 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 93/96\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.3414 - accuracy: 0.9053 - val_loss: 0.4820 - val_accuracy: 0.8638 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 94/96\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.3151 - accuracy: 0.9082 - val_loss: 0.2973 - val_accuracy: 0.9439 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 95/96\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2220 - accuracy: 0.9453 - val_loss: 0.2623 - val_accuracy: 0.9215 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 96/96\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2062 - accuracy: 0.9409 - val_loss: 0.2417 - val_accuracy: 0.9199 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-094-0.9439.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9439\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2973\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.2195880115032196. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m313.30 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m280.20 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m33.10 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [16] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m17\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 96)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 97/102\n",
      "256/256 [==============================] - 51s 184ms/step - loss: 0.3256 - accuracy: 0.9087 - val_loss: 0.2532 - val_accuracy: 0.9247 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 98/102\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.3436 - accuracy: 0.8960 - val_loss: 0.2876 - val_accuracy: 0.9199 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 99/102\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.3054 - accuracy: 0.9033 - val_loss: 0.2421 - val_accuracy: 0.9343 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 100/102\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.3065 - accuracy: 0.9214 - val_loss: 0.4420 - val_accuracy: 0.9375 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 101/102\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.2821 - accuracy: 0.9214 - val_loss: 0.2932 - val_accuracy: 0.9375 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 102/102\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1880 - accuracy: 0.9526 - val_loss: 0.2346 - val_accuracy: 0.9423 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-102-0.9423.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9423\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2346\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.2195880115032196. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m314.92 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m281.14 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m33.79 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [17] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m18\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 102)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33m└───Shuffling data...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 103/108\n",
      "256/256 [==============================] - 51s 184ms/step - loss: 0.2700 - accuracy: 0.9189 - val_loss: 0.2869 - val_accuracy: 0.9311 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 104/108\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.3335 - accuracy: 0.9072 - val_loss: 0.4121 - val_accuracy: 0.9103 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 105/108\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.3382 - accuracy: 0.9150 - val_loss: 0.3712 - val_accuracy: 0.9327 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 106/108\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2884 - accuracy: 0.9351 - val_loss: 0.3762 - val_accuracy: 0.8590 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 107/108\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.2295 - accuracy: 0.9419 - val_loss: 0.2174 - val_accuracy: 0.9423 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 108/108\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1938 - accuracy: 0.9492 - val_loss: 0.2403 - val_accuracy: 0.9423 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-107-0.9423.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9423\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2174\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model loss from \u001b[0m\u001b[0;32m0.2195880115032196 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.21743160486221313\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m321.67 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m282.30 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m39.36 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [18] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m19\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 108)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 109/114\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.2896 - accuracy: 0.9160 - val_loss: 0.3929 - val_accuracy: 0.8990 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 110/114\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.3225 - accuracy: 0.9116 - val_loss: 0.2669 - val_accuracy: 0.9263 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 111/114\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2926 - accuracy: 0.9229 - val_loss: 0.4608 - val_accuracy: 0.8878 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 112/114\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2669 - accuracy: 0.9209 - val_loss: 0.2904 - val_accuracy: 0.9247 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 113/114\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2234 - accuracy: 0.9473 - val_loss: 0.4278 - val_accuracy: 0.9247 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 114/114\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1819 - accuracy: 0.9546 - val_loss: 0.2227 - val_accuracy: 0.9375 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-114-0.9375.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9375\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2227\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.21743160486221313. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m316.72 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m282.49 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m34.24 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [19] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m20\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 114)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 115/120\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.2638 - accuracy: 0.9160 - val_loss: 0.2409 - val_accuracy: 0.9391 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 116/120\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.3396 - accuracy: 0.8979 - val_loss: 0.2746 - val_accuracy: 0.9199 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 117/120\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2945 - accuracy: 0.9136 - val_loss: 0.2682 - val_accuracy: 0.9167 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 118/120\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2795 - accuracy: 0.9316 - val_loss: 0.3202 - val_accuracy: 0.9295 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 119/120\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2383 - accuracy: 0.9355 - val_loss: 0.2700 - val_accuracy: 0.9311 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 120/120\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1877 - accuracy: 0.9478 - val_loss: 0.3409 - val_accuracy: 0.9327 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-115-0.9391.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9391\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2409\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.21743160486221313. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m316.52 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m281.61 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m34.91 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [20] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m21\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 120)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 121/126\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.3039 - accuracy: 0.9131 - val_loss: 0.3509 - val_accuracy: 0.9375 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 122/126\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.3234 - accuracy: 0.9092 - val_loss: 0.4477 - val_accuracy: 0.8013 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 123/126\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.3611 - accuracy: 0.8965 - val_loss: 0.3186 - val_accuracy: 0.8910 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 124/126\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2612 - accuracy: 0.9375 - val_loss: 0.3147 - val_accuracy: 0.9199 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 125/126\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2596 - accuracy: 0.9287 - val_loss: 0.2446 - val_accuracy: 0.9247 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 126/126\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1895 - accuracy: 0.9585 - val_loss: 0.2598 - val_accuracy: 0.9279 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-121-0.9375.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9375\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3509\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.21743160486221313. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m317.91 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m282.50 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m35.41 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [21] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m22\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 126)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 127/132\n",
      "256/256 [==============================] - 51s 186ms/step - loss: 0.3124 - accuracy: 0.9150 - val_loss: 0.3509 - val_accuracy: 0.9263 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 128/132\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.3525 - accuracy: 0.8901 - val_loss: 0.2671 - val_accuracy: 0.9119 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 129/132\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.3308 - accuracy: 0.9009 - val_loss: 0.3931 - val_accuracy: 0.8333 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 130/132\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.2898 - accuracy: 0.9199 - val_loss: 0.2497 - val_accuracy: 0.9343 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 131/132\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2178 - accuracy: 0.9434 - val_loss: 0.3649 - val_accuracy: 0.9263 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 132/132\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1691 - accuracy: 0.9561 - val_loss: 0.2934 - val_accuracy: 0.9295 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-130-0.9343.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9343\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2497\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.21743160486221313. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m318.28 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m282.45 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m35.83 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [22] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m23\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 132)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Learning the patterns]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 133/138\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.2837 - accuracy: 0.9141 - val_loss: 0.3335 - val_accuracy: 0.9311 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 134/138\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.2763 - accuracy: 0.9194 - val_loss: 0.2933 - val_accuracy: 0.9359 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 135/138\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2699 - accuracy: 0.9297 - val_loss: 0.2995 - val_accuracy: 0.9071 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 136/138\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2943 - accuracy: 0.9238 - val_loss: 0.2386 - val_accuracy: 0.9215 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 137/138\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2269 - accuracy: 0.9380 - val_loss: 0.2149 - val_accuracy: 0.9375 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 138/138\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.1804 - accuracy: 0.9580 - val_loss: 0.2456 - val_accuracy: 0.9391 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-138-0.9391.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9391\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2455\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.21743160486221313. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m317.84 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m282.05 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m35.79 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [23] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m24\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 138)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0147\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 139/144\n",
      "256/256 [==============================] - 50s 180ms/step - loss: 0.2347 - accuracy: 0.9297 - val_loss: 0.3001 - val_accuracy: 0.9295 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 140/144\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2650 - accuracy: 0.9229 - val_loss: 0.2768 - val_accuracy: 0.9359 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 141/144\n",
      "256/256 [==============================] - 45s 174ms/step - loss: 0.3086 - accuracy: 0.9121 - val_loss: 0.2986 - val_accuracy: 0.9231 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 142/144\n",
      "256/256 [==============================] - 44s 173ms/step - loss: 0.2620 - accuracy: 0.9331 - val_loss: 0.3041 - val_accuracy: 0.9103 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 143/144\n",
      "256/256 [==============================] - 45s 174ms/step - loss: 0.2440 - accuracy: 0.9375 - val_loss: 0.2865 - val_accuracy: 0.9167 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 144/144\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2283 - accuracy: 0.9385 - val_loss: 0.3130 - val_accuracy: 0.9263 - lr: 5.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-140-0.9359.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9359\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2768\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.21743160486221313. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m310.50 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m274.43 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m36.07 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [24] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m25\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 144)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01464\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 145/150\n",
      "256/256 [==============================] - 50s 181ms/step - loss: 0.3185 - accuracy: 0.9077 - val_loss: 0.2748 - val_accuracy: 0.9263 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 146/150\n",
      "256/256 [==============================] - 45s 174ms/step - loss: 0.3295 - accuracy: 0.9131 - val_loss: 0.3341 - val_accuracy: 0.9199 - lr: 0.0146 - momentum: 0.8506\n",
      "Epoch 147/150\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.3034 - accuracy: 0.9287 - val_loss: 0.4131 - val_accuracy: 0.8974 - lr: 0.0119 - momentum: 0.8688\n",
      "Epoch 148/150\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2569 - accuracy: 0.9287 - val_loss: 0.3959 - val_accuracy: 0.8958 - lr: 0.0068 - momentum: 0.9037\n",
      "Epoch 149/150\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.2157 - accuracy: 0.9395 - val_loss: 0.3447 - val_accuracy: 0.9327 - lr: 0.0020 - momentum: 0.9367\n",
      "Epoch 150/150\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1528 - accuracy: 0.9604 - val_loss: 0.3013 - val_accuracy: 0.9359 - lr: 5.8560e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9359\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3013\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.21743160486221313. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m311.94 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m276.41 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m35.53 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [25] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m26\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 150)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01458\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 151/156\n",
      "256/256 [==============================] - 49s 178ms/step - loss: 0.3113 - accuracy: 0.9087 - val_loss: 0.2813 - val_accuracy: 0.9327 - lr: 0.0088 - momentum: 0.8915\n",
      "Epoch 152/156\n",
      "256/256 [==============================] - 45s 174ms/step - loss: 0.2978 - accuracy: 0.9126 - val_loss: 0.3443 - val_accuracy: 0.9006 - lr: 0.0145 - momentum: 0.8506\n",
      "Epoch 153/156\n",
      "256/256 [==============================] - 45s 174ms/step - loss: 0.3167 - accuracy: 0.9155 - val_loss: 0.3006 - val_accuracy: 0.9006 - lr: 0.0118 - momentum: 0.8688\n",
      "Epoch 154/156\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2776 - accuracy: 0.9355 - val_loss: 0.2325 - val_accuracy: 0.9375 - lr: 0.0067 - momentum: 0.9037\n",
      "Epoch 155/156\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2035 - accuracy: 0.9531 - val_loss: 0.2372 - val_accuracy: 0.9311 - lr: 0.0019 - momentum: 0.9367\n",
      "Epoch 156/156\n",
      "256/256 [==============================] - 45s 174ms/step - loss: 0.1659 - accuracy: 0.9580 - val_loss: 0.2305 - val_accuracy: 0.9279 - lr: 5.8320e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9279\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2304\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.21743160486221313. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m309.28 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m273.82 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m35.46 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [26] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m27\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 156)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01452\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 157/162\n",
      "256/256 [==============================] - 49s 179ms/step - loss: 0.2430 - accuracy: 0.9316 - val_loss: 0.2585 - val_accuracy: 0.9215 - lr: 0.0087 - momentum: 0.8915\n",
      "Epoch 158/162\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2832 - accuracy: 0.9243 - val_loss: 0.3427 - val_accuracy: 0.9183 - lr: 0.0144 - momentum: 0.8506\n",
      "Epoch 159/162\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.3188 - accuracy: 0.9102 - val_loss: 0.5467 - val_accuracy: 0.8894 - lr: 0.0118 - momentum: 0.8688\n",
      "Epoch 160/162\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.3255 - accuracy: 0.9131 - val_loss: 0.3769 - val_accuracy: 0.9279 - lr: 0.0067 - momentum: 0.9037\n",
      "Epoch 161/162\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2106 - accuracy: 0.9546 - val_loss: 0.2703 - val_accuracy: 0.9407 - lr: 0.0019 - momentum: 0.9367\n",
      "Epoch 162/162\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.1778 - accuracy: 0.9536 - val_loss: 0.2150 - val_accuracy: 0.9215 - lr: 5.8080e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-161-0.9407.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9407\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2703\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.21743160486221313. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m312.10 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m275.52 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m36.58 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [27] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m28\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 162)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01446\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 163/168\n",
      "256/256 [==============================] - 50s 181ms/step - loss: 0.2903 - accuracy: 0.9170 - val_loss: 0.2352 - val_accuracy: 0.9391 - lr: 0.0087 - momentum: 0.8915\n",
      "Epoch 164/168\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.3076 - accuracy: 0.9126 - val_loss: 0.3734 - val_accuracy: 0.8942 - lr: 0.0144 - momentum: 0.8506\n",
      "Epoch 165/168\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2687 - accuracy: 0.9248 - val_loss: 0.5690 - val_accuracy: 0.8958 - lr: 0.0117 - momentum: 0.8688\n",
      "Epoch 166/168\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2571 - accuracy: 0.9385 - val_loss: 1.7227 - val_accuracy: 0.7147 - lr: 0.0067 - momentum: 0.9037\n",
      "Epoch 167/168\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2202 - accuracy: 0.9409 - val_loss: 0.2906 - val_accuracy: 0.9359 - lr: 0.0019 - momentum: 0.9367\n",
      "Epoch 168/168\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.1599 - accuracy: 0.9565 - val_loss: 0.3119 - val_accuracy: 0.9343 - lr: 5.7840e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9343\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3119\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.21743160486221313. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m311.90 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m275.90 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m36.00 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [28] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m29\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 168)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0144\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 169/174\n",
      "256/256 [==============================] - 49s 179ms/step - loss: 0.2573 - accuracy: 0.9277 - val_loss: 0.2854 - val_accuracy: 0.9295 - lr: 0.0087 - momentum: 0.8915\n",
      "Epoch 170/174\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2967 - accuracy: 0.9146 - val_loss: 0.3175 - val_accuracy: 0.9375 - lr: 0.0143 - momentum: 0.8506\n",
      "Epoch 171/174\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2875 - accuracy: 0.9219 - val_loss: 0.3138 - val_accuracy: 0.9343 - lr: 0.0117 - momentum: 0.8688\n",
      "Epoch 172/174\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2320 - accuracy: 0.9395 - val_loss: 0.4648 - val_accuracy: 0.9343 - lr: 0.0067 - momentum: 0.9037\n",
      "Epoch 173/174\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2052 - accuracy: 0.9531 - val_loss: 0.2382 - val_accuracy: 0.9439 - lr: 0.0019 - momentum: 0.9367\n",
      "Epoch 174/174\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1521 - accuracy: 0.9683 - val_loss: 0.2314 - val_accuracy: 0.9423 - lr: 5.7600e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9423\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2314\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.21743160486221313. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m312.51 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m276.07 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m36.44 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [29] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m30\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 174)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01434\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 175/180\n",
      "256/256 [==============================] - 49s 179ms/step - loss: 0.2609 - accuracy: 0.9287 - val_loss: 0.2654 - val_accuracy: 0.9407 - lr: 0.0086 - momentum: 0.8915\n",
      "Epoch 176/180\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2647 - accuracy: 0.9385 - val_loss: 0.2827 - val_accuracy: 0.9054 - lr: 0.0143 - momentum: 0.8506\n",
      "Epoch 177/180\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2902 - accuracy: 0.9131 - val_loss: 0.3556 - val_accuracy: 0.9263 - lr: 0.0116 - momentum: 0.8688\n",
      "Epoch 178/180\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2357 - accuracy: 0.9448 - val_loss: 0.2267 - val_accuracy: 0.9359 - lr: 0.0066 - momentum: 0.9037\n",
      "Epoch 179/180\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.1682 - accuracy: 0.9644 - val_loss: 0.2848 - val_accuracy: 0.9327 - lr: 0.0019 - momentum: 0.9367\n",
      "Epoch 180/180\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.1863 - accuracy: 0.9585 - val_loss: 0.2182 - val_accuracy: 0.9375 - lr: 5.7360e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9375\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2182\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.21743160486221313. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m311.12 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m274.94 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m36.18 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [30] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m31\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 180)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01428\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 181/186\n",
      "256/256 [==============================] - 49s 178ms/step - loss: 0.2376 - accuracy: 0.9355 - val_loss: 0.4749 - val_accuracy: 0.9183 - lr: 0.0086 - momentum: 0.8915\n",
      "Epoch 182/186\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2735 - accuracy: 0.9219 - val_loss: 0.6848 - val_accuracy: 0.8734 - lr: 0.0142 - momentum: 0.8506\n",
      "Epoch 183/186\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2613 - accuracy: 0.9326 - val_loss: 0.3324 - val_accuracy: 0.9359 - lr: 0.0116 - momentum: 0.8688\n",
      "Epoch 184/186\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2152 - accuracy: 0.9414 - val_loss: 0.2848 - val_accuracy: 0.9423 - lr: 0.0066 - momentum: 0.9037\n",
      "Epoch 185/186\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1656 - accuracy: 0.9634 - val_loss: 0.3078 - val_accuracy: 0.8910 - lr: 0.0019 - momentum: 0.9367\n",
      "Epoch 186/186\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1400 - accuracy: 0.9702 - val_loss: 0.2298 - val_accuracy: 0.9311 - lr: 5.7120e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9311\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2298\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.21743160486221313. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m312.11 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m275.49 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m36.62 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [31] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m32\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 186)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01422\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 187/192\n",
      "256/256 [==============================] - 50s 180ms/step - loss: 0.2385 - accuracy: 0.9321 - val_loss: 0.2560 - val_accuracy: 0.9295 - lr: 0.0086 - momentum: 0.8915\n",
      "Epoch 188/192\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2592 - accuracy: 0.9258 - val_loss: 0.7048 - val_accuracy: 0.6458 - lr: 0.0141 - momentum: 0.8506\n",
      "Epoch 189/192\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2519 - accuracy: 0.9346 - val_loss: 0.2099 - val_accuracy: 0.9471 - lr: 0.0115 - momentum: 0.8688\n",
      "Epoch 190/192\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2154 - accuracy: 0.9497 - val_loss: 0.3195 - val_accuracy: 0.8878 - lr: 0.0066 - momentum: 0.9037\n",
      "Epoch 191/192\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.1856 - accuracy: 0.9546 - val_loss: 0.2592 - val_accuracy: 0.9407 - lr: 0.0019 - momentum: 0.9367\n",
      "Epoch 192/192\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1447 - accuracy: 0.9658 - val_loss: 0.2122 - val_accuracy: 0.9439 - lr: 5.6880e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-189-0.9471.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9471\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2099\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9487179517745972. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model loss from \u001b[0m\u001b[0;32m0.21743160486221313 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.20990584790706635\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m315.82 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m276.17 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m39.65 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [32] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m33\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 192)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01416\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 193/198\n",
      "256/256 [==============================] - 50s 181ms/step - loss: 0.2852 - accuracy: 0.9233 - val_loss: 0.2252 - val_accuracy: 0.9551 - lr: 0.0085 - momentum: 0.8915\n",
      "Epoch 194/198\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2926 - accuracy: 0.9175 - val_loss: 0.3387 - val_accuracy: 0.8702 - lr: 0.0141 - momentum: 0.8506\n",
      "Epoch 195/198\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.3013 - accuracy: 0.9111 - val_loss: 0.2416 - val_accuracy: 0.9535 - lr: 0.0115 - momentum: 0.8688\n",
      "Epoch 196/198\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2782 - accuracy: 0.9297 - val_loss: 0.2526 - val_accuracy: 0.9327 - lr: 0.0066 - momentum: 0.9037\n",
      "Epoch 197/198\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.1860 - accuracy: 0.9556 - val_loss: 0.2367 - val_accuracy: 0.9375 - lr: 0.0019 - momentum: 0.9367\n",
      "Epoch 198/198\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.1499 - accuracy: 0.9644 - val_loss: 0.2806 - val_accuracy: 0.9343 - lr: 5.6640e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-193-0.9551.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9551\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2252\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model accuracy from \u001b[0m\u001b[0;32m0.9487179517745972 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.9551281929016113\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m316.28 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m276.31 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m39.97 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [33] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m34\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 198)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0141\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 199/204\n",
      "256/256 [==============================] - 50s 182ms/step - loss: 0.2587 - accuracy: 0.9248 - val_loss: 0.2475 - val_accuracy: 0.9215 - lr: 0.0085 - momentum: 0.8915\n",
      "Epoch 200/204\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.2758 - accuracy: 0.9204 - val_loss: 0.2672 - val_accuracy: 0.9407 - lr: 0.0140 - momentum: 0.8506\n",
      "Epoch 201/204\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2505 - accuracy: 0.9341 - val_loss: 0.3928 - val_accuracy: 0.8574 - lr: 0.0114 - momentum: 0.8688\n",
      "Epoch 202/204\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2604 - accuracy: 0.9316 - val_loss: 0.2649 - val_accuracy: 0.9295 - lr: 0.0065 - momentum: 0.9037\n",
      "Epoch 203/204\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.1986 - accuracy: 0.9551 - val_loss: 0.2377 - val_accuracy: 0.9391 - lr: 0.0019 - momentum: 0.9367\n",
      "Epoch 204/204\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.1519 - accuracy: 0.9624 - val_loss: 0.2117 - val_accuracy: 0.9407 - lr: 5.6400e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9407\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2117\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9551281929016113. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m314.19 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m276.11 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m38.07 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [34] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m35\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 204)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01404\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 205/210\n",
      "256/256 [==============================] - 50s 179ms/step - loss: 0.2498 - accuracy: 0.9204 - val_loss: 0.3110 - val_accuracy: 0.9183 - lr: 0.0085 - momentum: 0.8915\n",
      "Epoch 206/210\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.3439 - accuracy: 0.8901 - val_loss: 0.3838 - val_accuracy: 0.9167 - lr: 0.0140 - momentum: 0.8506\n",
      "Epoch 207/210\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.3214 - accuracy: 0.9214 - val_loss: 0.3202 - val_accuracy: 0.8622 - lr: 0.0114 - momentum: 0.8688\n",
      "Epoch 208/210\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2891 - accuracy: 0.9170 - val_loss: 0.4629 - val_accuracy: 0.9038 - lr: 0.0065 - momentum: 0.9037\n",
      "Epoch 209/210\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2779 - accuracy: 0.9424 - val_loss: 0.4567 - val_accuracy: 0.9247 - lr: 0.0019 - momentum: 0.9367\n",
      "Epoch 210/210\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2118 - accuracy: 0.9570 - val_loss: 0.4585 - val_accuracy: 0.9183 - lr: 5.6160e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9199\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.4585\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9551281929016113. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m314.31 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m276.11 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m38.20 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [35] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m36\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 210)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33m└───Shuffling data...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01398\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 211/216\n",
      "256/256 [==============================] - 49s 179ms/step - loss: 0.3015 - accuracy: 0.9155 - val_loss: 0.7072 - val_accuracy: 0.8381 - lr: 0.0084 - momentum: 0.8915\n",
      "Epoch 212/216\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.3203 - accuracy: 0.9116 - val_loss: 0.2684 - val_accuracy: 0.9455 - lr: 0.0139 - momentum: 0.8506\n",
      "Epoch 213/216\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2697 - accuracy: 0.9307 - val_loss: 0.3074 - val_accuracy: 0.9407 - lr: 0.0113 - momentum: 0.8688\n",
      "Epoch 214/216\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2326 - accuracy: 0.9404 - val_loss: 0.2609 - val_accuracy: 0.9423 - lr: 0.0065 - momentum: 0.9037\n",
      "Epoch 215/216\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2165 - accuracy: 0.9458 - val_loss: 0.4155 - val_accuracy: 0.9359 - lr: 0.0019 - momentum: 0.9367\n",
      "Epoch 216/216\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.1497 - accuracy: 0.9663 - val_loss: 0.4218 - val_accuracy: 0.9295 - lr: 5.5920e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9295\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.4218\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9551281929016113. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m316.30 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m276.30 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m40.00 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [36] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m37\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 216)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01392\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 217/222\n",
      "256/256 [==============================] - 50s 179ms/step - loss: 0.2571 - accuracy: 0.9263 - val_loss: 0.3335 - val_accuracy: 0.9359 - lr: 0.0084 - momentum: 0.8915\n",
      "Epoch 218/222\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2999 - accuracy: 0.9131 - val_loss: 0.2366 - val_accuracy: 0.9423 - lr: 0.0138 - momentum: 0.8506\n",
      "Epoch 219/222\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2779 - accuracy: 0.9331 - val_loss: 0.2879 - val_accuracy: 0.8974 - lr: 0.0113 - momentum: 0.8688\n",
      "Epoch 220/222\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2349 - accuracy: 0.9399 - val_loss: 0.3135 - val_accuracy: 0.8990 - lr: 0.0064 - momentum: 0.9037\n",
      "Epoch 221/222\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2008 - accuracy: 0.9546 - val_loss: 0.2773 - val_accuracy: 0.9247 - lr: 0.0019 - momentum: 0.9367\n",
      "Epoch 222/222\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1394 - accuracy: 0.9683 - val_loss: 0.3434 - val_accuracy: 0.9279 - lr: 5.5680e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9279\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3434\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9551281929016113. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m314.11 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m276.01 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m38.11 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [37] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m38\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 222)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01386\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 223/228\n",
      "256/256 [==============================] - 50s 180ms/step - loss: 0.2307 - accuracy: 0.9307 - val_loss: 0.2328 - val_accuracy: 0.9375 - lr: 0.0083 - momentum: 0.8915\n",
      "Epoch 224/228\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2818 - accuracy: 0.9204 - val_loss: 0.4113 - val_accuracy: 0.8718 - lr: 0.0138 - momentum: 0.8506\n",
      "Epoch 225/228\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.3218 - accuracy: 0.9136 - val_loss: 0.2276 - val_accuracy: 0.9279 - lr: 0.0113 - momentum: 0.8688\n",
      "Epoch 226/228\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1970 - accuracy: 0.9492 - val_loss: 0.3562 - val_accuracy: 0.8638 - lr: 0.0064 - momentum: 0.9037\n",
      "Epoch 227/228\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1782 - accuracy: 0.9639 - val_loss: 0.2323 - val_accuracy: 0.9295 - lr: 0.0018 - momentum: 0.9367\n",
      "Epoch 228/228\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1440 - accuracy: 0.9648 - val_loss: 0.2243 - val_accuracy: 0.9375 - lr: 5.5440e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9375\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2243\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9551281929016113. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m313.88 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m275.76 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m38.13 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [38] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m39\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 228)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0138\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 229/234\n",
      "256/256 [==============================] - 50s 180ms/step - loss: 0.2119 - accuracy: 0.9375 - val_loss: 0.2668 - val_accuracy: 0.9423 - lr: 0.0083 - momentum: 0.8915\n",
      "Epoch 230/234\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2587 - accuracy: 0.9321 - val_loss: 0.5260 - val_accuracy: 0.8157 - lr: 0.0137 - momentum: 0.8506\n",
      "Epoch 231/234\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2948 - accuracy: 0.9136 - val_loss: 0.2672 - val_accuracy: 0.9343 - lr: 0.0112 - momentum: 0.8688\n",
      "Epoch 232/234\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1842 - accuracy: 0.9541 - val_loss: 0.2506 - val_accuracy: 0.8990 - lr: 0.0064 - momentum: 0.9037\n",
      "Epoch 233/234\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1561 - accuracy: 0.9595 - val_loss: 0.3073 - val_accuracy: 0.9327 - lr: 0.0018 - momentum: 0.9367\n",
      "Epoch 234/234\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1629 - accuracy: 0.9688 - val_loss: 0.2668 - val_accuracy: 0.9279 - lr: 5.5200e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9279\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2668\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9551281929016113. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m314.90 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m276.19 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m38.71 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [39] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m40\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 234)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01374\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 235/240\n",
      "256/256 [==============================] - 50s 179ms/step - loss: 0.2425 - accuracy: 0.9351 - val_loss: 0.2922 - val_accuracy: 0.9359 - lr: 0.0083 - momentum: 0.8915\n",
      "Epoch 236/240\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2727 - accuracy: 0.9082 - val_loss: 0.5939 - val_accuracy: 0.7740 - lr: 0.0137 - momentum: 0.8506\n",
      "Epoch 237/240\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2865 - accuracy: 0.9287 - val_loss: 0.3467 - val_accuracy: 0.9231 - lr: 0.0112 - momentum: 0.8688\n",
      "Epoch 238/240\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2779 - accuracy: 0.9360 - val_loss: 0.3094 - val_accuracy: 0.9119 - lr: 0.0064 - momentum: 0.9037\n",
      "Epoch 239/240\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1906 - accuracy: 0.9619 - val_loss: 0.3536 - val_accuracy: 0.9263 - lr: 0.0018 - momentum: 0.9367\n",
      "Epoch 240/240\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1518 - accuracy: 0.9673 - val_loss: 0.2827 - val_accuracy: 0.9231 - lr: 5.4960e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9231\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2827\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9551281929016113. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m315.31 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m275.95 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m39.36 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [40] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m41\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 240)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01368\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 241/246\n",
      "256/256 [==============================] - 50s 180ms/step - loss: 0.2409 - accuracy: 0.9297 - val_loss: 0.3638 - val_accuracy: 0.9006 - lr: 0.0082 - momentum: 0.8915\n",
      "Epoch 242/246\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2893 - accuracy: 0.9214 - val_loss: 0.4081 - val_accuracy: 0.8462 - lr: 0.0136 - momentum: 0.8506\n",
      "Epoch 243/246\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2682 - accuracy: 0.9346 - val_loss: 0.3090 - val_accuracy: 0.8910 - lr: 0.0111 - momentum: 0.8688\n",
      "Epoch 244/246\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2327 - accuracy: 0.9502 - val_loss: 0.3252 - val_accuracy: 0.9022 - lr: 0.0063 - momentum: 0.9037\n",
      "Epoch 245/246\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2007 - accuracy: 0.9521 - val_loss: 0.2587 - val_accuracy: 0.9006 - lr: 0.0018 - momentum: 0.9367\n",
      "Epoch 246/246\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.1625 - accuracy: 0.9653 - val_loss: 0.2111 - val_accuracy: 0.9231 - lr: 5.4720e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9231\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2111\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9551281929016113. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m315.11 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m275.97 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m39.14 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [41] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m42\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 246)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01362\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 247/252\n",
      "256/256 [==============================] - 50s 180ms/step - loss: 0.2656 - accuracy: 0.9224 - val_loss: 0.2411 - val_accuracy: 0.9343 - lr: 0.0082 - momentum: 0.8915\n",
      "Epoch 248/252\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2665 - accuracy: 0.9355 - val_loss: 0.3326 - val_accuracy: 0.9343 - lr: 0.0135 - momentum: 0.8506\n",
      "Epoch 249/252\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2297 - accuracy: 0.9478 - val_loss: 0.3011 - val_accuracy: 0.9423 - lr: 0.0111 - momentum: 0.8688\n",
      "Epoch 250/252\n",
      "256/256 [==============================] - 45s 175ms/step - loss: 0.2331 - accuracy: 0.9424 - val_loss: 0.2626 - val_accuracy: 0.9407 - lr: 0.0063 - momentum: 0.9037\n",
      "Epoch 251/252\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1532 - accuracy: 0.9619 - val_loss: 0.2534 - val_accuracy: 0.9375 - lr: 0.0018 - momentum: 0.9367\n",
      "Epoch 252/252\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1258 - accuracy: 0.9717 - val_loss: 0.2721 - val_accuracy: 0.9375 - lr: 5.4480e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9375\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2721\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9551281929016113. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m315.89 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m276.09 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m39.81 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [42] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m43\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 252)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01356\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 253/258\n",
      "256/256 [==============================] - 50s 180ms/step - loss: 0.2579 - accuracy: 0.9268 - val_loss: 0.2668 - val_accuracy: 0.9359 - lr: 0.0082 - momentum: 0.8915\n",
      "Epoch 254/258\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2815 - accuracy: 0.9316 - val_loss: 0.3802 - val_accuracy: 0.9375 - lr: 0.0135 - momentum: 0.8506\n",
      "Epoch 255/258\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2712 - accuracy: 0.9287 - val_loss: 0.2258 - val_accuracy: 0.9407 - lr: 0.0110 - momentum: 0.8688\n",
      "Epoch 256/258\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2200 - accuracy: 0.9517 - val_loss: 0.2347 - val_accuracy: 0.9455 - lr: 0.0063 - momentum: 0.9037\n",
      "Epoch 257/258\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1762 - accuracy: 0.9595 - val_loss: 0.2185 - val_accuracy: 0.9487 - lr: 0.0018 - momentum: 0.9367\n",
      "Epoch 258/258\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1379 - accuracy: 0.9673 - val_loss: 0.3341 - val_accuracy: 0.9375 - lr: 5.4240e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9375\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3341\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9551281929016113. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m316.30 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m277.32 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m38.98 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [43] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m44\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 258)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0135\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 259/264\n",
      "256/256 [==============================] - 50s 179ms/step - loss: 0.2152 - accuracy: 0.9424 - val_loss: 0.4052 - val_accuracy: 0.9375 - lr: 0.0081 - momentum: 0.8915\n",
      "Epoch 260/264\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2398 - accuracy: 0.9346 - val_loss: 0.2218 - val_accuracy: 0.9567 - lr: 0.0134 - momentum: 0.8506\n",
      "Epoch 261/264\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2608 - accuracy: 0.9302 - val_loss: 0.2922 - val_accuracy: 0.9327 - lr: 0.0110 - momentum: 0.8688\n",
      "Epoch 262/264\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2365 - accuracy: 0.9497 - val_loss: 0.3023 - val_accuracy: 0.9247 - lr: 0.0062 - momentum: 0.9037\n",
      "Epoch 263/264\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1514 - accuracy: 0.9692 - val_loss: 0.3830 - val_accuracy: 0.9311 - lr: 0.0018 - momentum: 0.9367\n",
      "Epoch 264/264\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1281 - accuracy: 0.9736 - val_loss: 0.4081 - val_accuracy: 0.9263 - lr: 5.4000e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-260-0.9567.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9567\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2218\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model accuracy from \u001b[0m\u001b[0;32m0.9551281929016113 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.9567307829856873\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m319.99 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m277.29 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m42.70 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [44] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m45\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 264)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01344\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 265/270\n",
      "256/256 [==============================] - 50s 182ms/step - loss: 0.2583 - accuracy: 0.9321 - val_loss: 0.2134 - val_accuracy: 0.9551 - lr: 0.0081 - momentum: 0.8915\n",
      "Epoch 266/270\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2626 - accuracy: 0.9414 - val_loss: 0.3394 - val_accuracy: 0.8830 - lr: 0.0134 - momentum: 0.8506\n",
      "Epoch 267/270\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2253 - accuracy: 0.9497 - val_loss: 0.2738 - val_accuracy: 0.9375 - lr: 0.0109 - momentum: 0.8688\n",
      "Epoch 268/270\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1872 - accuracy: 0.9551 - val_loss: 0.2326 - val_accuracy: 0.9439 - lr: 0.0062 - momentum: 0.9037\n",
      "Epoch 269/270\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1712 - accuracy: 0.9619 - val_loss: 0.3063 - val_accuracy: 0.8814 - lr: 0.0018 - momentum: 0.9367\n",
      "Epoch 270/270\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1562 - accuracy: 0.9609 - val_loss: 0.2072 - val_accuracy: 0.9407 - lr: 5.3760e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-265-0.9551.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9551\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2134\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m318.47 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m277.75 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m40.72 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [45] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m46\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 270)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01338\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 271/276\n",
      "256/256 [==============================] - 51s 183ms/step - loss: 0.2065 - accuracy: 0.9492 - val_loss: 0.2984 - val_accuracy: 0.9215 - lr: 0.0081 - momentum: 0.8915\n",
      "Epoch 272/276\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2423 - accuracy: 0.9321 - val_loss: 0.3009 - val_accuracy: 0.9375 - lr: 0.0133 - momentum: 0.8506\n",
      "Epoch 273/276\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2357 - accuracy: 0.9390 - val_loss: 0.2944 - val_accuracy: 0.9359 - lr: 0.0109 - momentum: 0.8688\n",
      "Epoch 274/276\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2622 - accuracy: 0.9468 - val_loss: 0.3181 - val_accuracy: 0.9391 - lr: 0.0062 - momentum: 0.9037\n",
      "Epoch 275/276\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1550 - accuracy: 0.9668 - val_loss: 0.4649 - val_accuracy: 0.9215 - lr: 0.0018 - momentum: 0.9367\n",
      "Epoch 276/276\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1352 - accuracy: 0.9678 - val_loss: 0.3932 - val_accuracy: 0.9295 - lr: 5.3520e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9295\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3932\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m320.07 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m279.36 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m40.70 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [46] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m47\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 276)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01332\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 277/282\n",
      "256/256 [==============================] - 50s 180ms/step - loss: 0.1981 - accuracy: 0.9482 - val_loss: 0.3007 - val_accuracy: 0.9295 - lr: 0.0080 - momentum: 0.8915\n",
      "Epoch 278/282\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2820 - accuracy: 0.9189 - val_loss: 0.2647 - val_accuracy: 0.9311 - lr: 0.0132 - momentum: 0.8506\n",
      "Epoch 279/282\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2488 - accuracy: 0.9272 - val_loss: 0.3855 - val_accuracy: 0.9471 - lr: 0.0108 - momentum: 0.8688\n",
      "Epoch 280/282\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.3068 - accuracy: 0.9248 - val_loss: 0.3390 - val_accuracy: 0.9006 - lr: 0.0062 - momentum: 0.9037\n",
      "Epoch 281/282\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2510 - accuracy: 0.9458 - val_loss: 0.3447 - val_accuracy: 0.9022 - lr: 0.0018 - momentum: 0.9367\n",
      "Epoch 282/282\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.1883 - accuracy: 0.9639 - val_loss: 0.3202 - val_accuracy: 0.9343 - lr: 5.3280e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9343\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3202\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m319.08 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m278.12 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m40.96 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [47] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m48\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 282)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01326\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 283/288\n",
      "256/256 [==============================] - 50s 180ms/step - loss: 0.2534 - accuracy: 0.9224 - val_loss: 0.2646 - val_accuracy: 0.9183 - lr: 0.0080 - momentum: 0.8915\n",
      "Epoch 284/288\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2299 - accuracy: 0.9365 - val_loss: 0.2172 - val_accuracy: 0.9391 - lr: 0.0132 - momentum: 0.8506\n",
      "Epoch 285/288\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.2070 - accuracy: 0.9473 - val_loss: 0.2389 - val_accuracy: 0.9327 - lr: 0.0108 - momentum: 0.8688\n",
      "Epoch 286/288\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2152 - accuracy: 0.9497 - val_loss: 0.2141 - val_accuracy: 0.9455 - lr: 0.0061 - momentum: 0.9037\n",
      "Epoch 287/288\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1948 - accuracy: 0.9541 - val_loss: 0.2333 - val_accuracy: 0.9327 - lr: 0.0018 - momentum: 0.9367\n",
      "Epoch 288/288\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.1655 - accuracy: 0.9590 - val_loss: 0.2491 - val_accuracy: 0.9327 - lr: 5.3040e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9327\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2491\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m318.92 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m277.33 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m41.59 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [48] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m49\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 288)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0132\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 289/294\n",
      "256/256 [==============================] - 50s 180ms/step - loss: 0.2384 - accuracy: 0.9297 - val_loss: 0.2766 - val_accuracy: 0.9359 - lr: 0.0079 - momentum: 0.8915\n",
      "Epoch 290/294\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2199 - accuracy: 0.9360 - val_loss: 0.2895 - val_accuracy: 0.9359 - lr: 0.0131 - momentum: 0.8506\n",
      "Epoch 291/294\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2179 - accuracy: 0.9390 - val_loss: 0.2628 - val_accuracy: 0.9311 - lr: 0.0107 - momentum: 0.8688\n",
      "Epoch 292/294\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.1770 - accuracy: 0.9595 - val_loss: 0.2369 - val_accuracy: 0.9391 - lr: 0.0061 - momentum: 0.9037\n",
      "Epoch 293/294\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1705 - accuracy: 0.9575 - val_loss: 0.2280 - val_accuracy: 0.9391 - lr: 0.0018 - momentum: 0.9367\n",
      "Epoch 294/294\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1195 - accuracy: 0.9751 - val_loss: 0.2580 - val_accuracy: 0.9391 - lr: 5.2800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9391\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2580\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m318.72 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m277.14 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m41.58 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [49] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m50\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 294)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01314\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 295/300\n",
      "256/256 [==============================] - 50s 180ms/step - loss: 0.2363 - accuracy: 0.9331 - val_loss: 0.2041 - val_accuracy: 0.9423 - lr: 0.0079 - momentum: 0.8915\n",
      "Epoch 296/300\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2320 - accuracy: 0.9438 - val_loss: 0.2156 - val_accuracy: 0.9263 - lr: 0.0131 - momentum: 0.8506\n",
      "Epoch 297/300\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2299 - accuracy: 0.9336 - val_loss: 0.3079 - val_accuracy: 0.9183 - lr: 0.0107 - momentum: 0.8688\n",
      "Epoch 298/300\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2252 - accuracy: 0.9497 - val_loss: 0.3596 - val_accuracy: 0.9423 - lr: 0.0061 - momentum: 0.9037\n",
      "Epoch 299/300\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1734 - accuracy: 0.9561 - val_loss: 0.2186 - val_accuracy: 0.9135 - lr: 0.0018 - momentum: 0.9367\n",
      "Epoch 300/300\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1446 - accuracy: 0.9653 - val_loss: 0.2723 - val_accuracy: 0.9375 - lr: 5.2560e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;31mERROR: Failed to load weights. Error: max() arg is an empty sequence\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9375\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2724\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m318.08 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m276.85 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m41.23 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [50] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m51\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 300)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01308\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 301/306\n",
      "256/256 [==============================] - 51s 183ms/step - loss: 0.2388 - accuracy: 0.9302 - val_loss: 0.2832 - val_accuracy: 0.9263 - lr: 0.0079 - momentum: 0.8915\n",
      "Epoch 302/306\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2805 - accuracy: 0.9116 - val_loss: 0.4591 - val_accuracy: 0.8974 - lr: 0.0130 - momentum: 0.8506\n",
      "Epoch 303/306\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2742 - accuracy: 0.9312 - val_loss: 0.3353 - val_accuracy: 0.9263 - lr: 0.0106 - momentum: 0.8688\n",
      "Epoch 304/306\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2330 - accuracy: 0.9390 - val_loss: 0.4355 - val_accuracy: 0.9087 - lr: 0.0061 - momentum: 0.9037\n",
      "Epoch 305/306\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1988 - accuracy: 0.9565 - val_loss: 0.5020 - val_accuracy: 0.9119 - lr: 0.0017 - momentum: 0.9367\n",
      "Epoch 306/306\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1722 - accuracy: 0.9629 - val_loss: 0.3481 - val_accuracy: 0.9295 - lr: 5.2320e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9295\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3481\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20990584790706635. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m320.33 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m278.49 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m41.83 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [51] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m52\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 306)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01302\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 307/312\n",
      "256/256 [==============================] - 51s 184ms/step - loss: 0.2609 - accuracy: 0.9229 - val_loss: 0.3252 - val_accuracy: 0.9311 - lr: 0.0078 - momentum: 0.8915\n",
      "Epoch 308/312\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.2811 - accuracy: 0.9346 - val_loss: 0.5813 - val_accuracy: 0.8622 - lr: 0.0129 - momentum: 0.8506\n",
      "Epoch 309/312\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2753 - accuracy: 0.9316 - val_loss: 0.2384 - val_accuracy: 0.9503 - lr: 0.0106 - momentum: 0.8688\n",
      "Epoch 310/312\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2208 - accuracy: 0.9458 - val_loss: 0.3539 - val_accuracy: 0.9135 - lr: 0.0060 - momentum: 0.9037\n",
      "Epoch 311/312\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1831 - accuracy: 0.9575 - val_loss: 0.2050 - val_accuracy: 0.9519 - lr: 0.0017 - momentum: 0.9367\n",
      "Epoch 312/312\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1277 - accuracy: 0.9702 - val_loss: 0.2321 - val_accuracy: 0.9439 - lr: 5.2080e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-311-0.9519.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9519\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2050\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model loss from \u001b[0m\u001b[0;32m0.20990584790706635 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.20500072836875916\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m325.79 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m279.91 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m45.89 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [52] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m53\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 312)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01296\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 313/318\n",
      "256/256 [==============================] - 50s 182ms/step - loss: 0.2367 - accuracy: 0.9307 - val_loss: 0.2165 - val_accuracy: 0.9423 - lr: 0.0078 - momentum: 0.8915\n",
      "Epoch 314/318\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2548 - accuracy: 0.9351 - val_loss: 0.2510 - val_accuracy: 0.9391 - lr: 0.0129 - momentum: 0.8506\n",
      "Epoch 315/318\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2100 - accuracy: 0.9438 - val_loss: 0.2889 - val_accuracy: 0.9391 - lr: 0.0105 - momentum: 0.8688\n",
      "Epoch 316/318\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2010 - accuracy: 0.9492 - val_loss: 0.2971 - val_accuracy: 0.9359 - lr: 0.0060 - momentum: 0.9037\n",
      "Epoch 317/318\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1639 - accuracy: 0.9683 - val_loss: 0.2362 - val_accuracy: 0.9487 - lr: 0.0017 - momentum: 0.9367\n",
      "Epoch 318/318\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1011 - accuracy: 0.9800 - val_loss: 0.3076 - val_accuracy: 0.9439 - lr: 5.1840e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9439\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3076\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m321.77 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m278.95 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m42.81 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [53] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m54\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 318)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33m└───Shuffling data...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0129\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 319/324\n",
      "256/256 [==============================] - 50s 180ms/step - loss: 0.1977 - accuracy: 0.9463 - val_loss: 0.3348 - val_accuracy: 0.9343 - lr: 0.0078 - momentum: 0.8915\n",
      "Epoch 320/324\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2123 - accuracy: 0.9409 - val_loss: 0.3236 - val_accuracy: 0.9471 - lr: 0.0128 - momentum: 0.8506\n",
      "Epoch 321/324\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2150 - accuracy: 0.9463 - val_loss: 0.2515 - val_accuracy: 0.9407 - lr: 0.0105 - momentum: 0.8688\n",
      "Epoch 322/324\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.2006 - accuracy: 0.9565 - val_loss: 0.4065 - val_accuracy: 0.9231 - lr: 0.0060 - momentum: 0.9037\n",
      "Epoch 323/324\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1589 - accuracy: 0.9658 - val_loss: 0.2775 - val_accuracy: 0.9455 - lr: 0.0017 - momentum: 0.9367\n",
      "Epoch 324/324\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1196 - accuracy: 0.9775 - val_loss: 0.3090 - val_accuracy: 0.9439 - lr: 5.1600e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9439\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3090\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m323.91 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m278.00 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m45.91 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [54] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m55\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 324)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01284\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 325/330\n",
      "256/256 [==============================] - 50s 181ms/step - loss: 0.2029 - accuracy: 0.9453 - val_loss: 0.2921 - val_accuracy: 0.9423 - lr: 0.0077 - momentum: 0.8915\n",
      "Epoch 326/330\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2446 - accuracy: 0.9370 - val_loss: 0.2964 - val_accuracy: 0.9471 - lr: 0.0128 - momentum: 0.8506\n",
      "Epoch 327/330\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.1971 - accuracy: 0.9600 - val_loss: 0.2869 - val_accuracy: 0.9439 - lr: 0.0104 - momentum: 0.8688\n",
      "Epoch 328/330\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1913 - accuracy: 0.9512 - val_loss: 0.2939 - val_accuracy: 0.9471 - lr: 0.0059 - momentum: 0.9037\n",
      "Epoch 329/330\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1584 - accuracy: 0.9697 - val_loss: 0.2358 - val_accuracy: 0.9487 - lr: 0.0017 - momentum: 0.9367\n",
      "Epoch 330/330\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1532 - accuracy: 0.9702 - val_loss: 0.3036 - val_accuracy: 0.9391 - lr: 5.1360e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9391\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3036\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m320.31 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m277.67 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m42.64 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [55] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m56\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 330)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01278\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 331/336\n",
      "256/256 [==============================] - 50s 180ms/step - loss: 0.2183 - accuracy: 0.9463 - val_loss: 0.3834 - val_accuracy: 0.9471 - lr: 0.0077 - momentum: 0.8915\n",
      "Epoch 332/336\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2782 - accuracy: 0.9302 - val_loss: 0.2965 - val_accuracy: 0.9199 - lr: 0.0127 - momentum: 0.8506\n",
      "Epoch 333/336\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2234 - accuracy: 0.9443 - val_loss: 0.3496 - val_accuracy: 0.9167 - lr: 0.0104 - momentum: 0.8688\n",
      "Epoch 334/336\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2153 - accuracy: 0.9507 - val_loss: 0.2646 - val_accuracy: 0.9487 - lr: 0.0059 - momentum: 0.9037\n",
      "Epoch 335/336\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1670 - accuracy: 0.9639 - val_loss: 0.2361 - val_accuracy: 0.9503 - lr: 0.0017 - momentum: 0.9367\n",
      "Epoch 336/336\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.1308 - accuracy: 0.9727 - val_loss: 0.2959 - val_accuracy: 0.9487 - lr: 5.1120e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9487\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2959\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m321.70 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m278.40 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m43.29 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [56] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m57\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 336)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01272\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 337/342\n",
      "256/256 [==============================] - 50s 179ms/step - loss: 0.1836 - accuracy: 0.9468 - val_loss: 0.3220 - val_accuracy: 0.9471 - lr: 0.0077 - momentum: 0.8915\n",
      "Epoch 338/342\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1932 - accuracy: 0.9541 - val_loss: 0.4142 - val_accuracy: 0.9423 - lr: 0.0126 - momentum: 0.8506\n",
      "Epoch 339/342\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2285 - accuracy: 0.9453 - val_loss: 0.3098 - val_accuracy: 0.9423 - lr: 0.0103 - momentum: 0.8688\n",
      "Epoch 340/342\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2177 - accuracy: 0.9565 - val_loss: 0.2746 - val_accuracy: 0.9407 - lr: 0.0059 - momentum: 0.9037\n",
      "Epoch 341/342\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1587 - accuracy: 0.9648 - val_loss: 0.2236 - val_accuracy: 0.9407 - lr: 0.0017 - momentum: 0.9367\n",
      "Epoch 342/342\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1290 - accuracy: 0.9688 - val_loss: 0.2223 - val_accuracy: 0.9471 - lr: 5.0880e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9471\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2223\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m321.11 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m277.05 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m44.06 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [57] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m58\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 342)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01266\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 343/348\n",
      "256/256 [==============================] - 50s 181ms/step - loss: 0.2228 - accuracy: 0.9375 - val_loss: 0.3681 - val_accuracy: 0.9487 - lr: 0.0076 - momentum: 0.8915\n",
      "Epoch 344/348\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2606 - accuracy: 0.9331 - val_loss: 0.2884 - val_accuracy: 0.9391 - lr: 0.0126 - momentum: 0.8506\n",
      "Epoch 345/348\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2535 - accuracy: 0.9443 - val_loss: 0.3066 - val_accuracy: 0.9391 - lr: 0.0103 - momentum: 0.8688\n",
      "Epoch 346/348\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2376 - accuracy: 0.9312 - val_loss: 0.4077 - val_accuracy: 0.9119 - lr: 0.0059 - momentum: 0.9037\n",
      "Epoch 347/348\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1648 - accuracy: 0.9604 - val_loss: 0.3524 - val_accuracy: 0.9231 - lr: 0.0017 - momentum: 0.9367\n",
      "Epoch 348/348\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1279 - accuracy: 0.9692 - val_loss: 0.3673 - val_accuracy: 0.9215 - lr: 5.0640e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9215\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3673\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m321.88 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m278.26 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m43.63 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [58] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m59\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 348)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0126\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 349/354\n",
      "256/256 [==============================] - 50s 181ms/step - loss: 0.2090 - accuracy: 0.9336 - val_loss: 0.2747 - val_accuracy: 0.9359 - lr: 0.0076 - momentum: 0.8915\n",
      "Epoch 350/354\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2300 - accuracy: 0.9370 - val_loss: 0.3635 - val_accuracy: 0.8990 - lr: 0.0125 - momentum: 0.8506\n",
      "Epoch 351/354\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1793 - accuracy: 0.9521 - val_loss: 0.3843 - val_accuracy: 0.9311 - lr: 0.0102 - momentum: 0.8688\n",
      "Epoch 352/354\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1660 - accuracy: 0.9639 - val_loss: 0.4227 - val_accuracy: 0.9183 - lr: 0.0058 - momentum: 0.9037\n",
      "Epoch 353/354\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1396 - accuracy: 0.9717 - val_loss: 0.3311 - val_accuracy: 0.9311 - lr: 0.0017 - momentum: 0.9367\n",
      "Epoch 354/354\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.0912 - accuracy: 0.9834 - val_loss: 0.4019 - val_accuracy: 0.9343 - lr: 5.0400e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9343\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.4019\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m322.12 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m278.08 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m44.05 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [59] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m60\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 354)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01254\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 355/360\n",
      "256/256 [==============================] - 50s 180ms/step - loss: 0.2260 - accuracy: 0.9287 - val_loss: 0.3412 - val_accuracy: 0.9375 - lr: 0.0075 - momentum: 0.8915\n",
      "Epoch 356/360\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2372 - accuracy: 0.9341 - val_loss: 0.2991 - val_accuracy: 0.9279 - lr: 0.0125 - momentum: 0.8506\n",
      "Epoch 357/360\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.2529 - accuracy: 0.9365 - val_loss: 0.3348 - val_accuracy: 0.9327 - lr: 0.0102 - momentum: 0.8688\n",
      "Epoch 358/360\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.2205 - accuracy: 0.9346 - val_loss: 0.6641 - val_accuracy: 0.9215 - lr: 0.0058 - momentum: 0.9037\n",
      "Epoch 359/360\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1792 - accuracy: 0.9556 - val_loss: 0.3750 - val_accuracy: 0.9375 - lr: 0.0017 - momentum: 0.9367\n",
      "Epoch 360/360\n",
      "256/256 [==============================] - 45s 176ms/step - loss: 0.1322 - accuracy: 0.9692 - val_loss: 0.3344 - val_accuracy: 0.9359 - lr: 5.0160e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9359\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3344\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m321.70 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m277.08 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m44.62 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [60] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m61\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 360)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01248\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 361/366\n",
      "256/256 [==============================] - 50s 181ms/step - loss: 0.2142 - accuracy: 0.9424 - val_loss: 0.3247 - val_accuracy: 0.9295 - lr: 0.0075 - momentum: 0.8915\n",
      "Epoch 362/366\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.2195 - accuracy: 0.9370 - val_loss: 0.2452 - val_accuracy: 0.9295 - lr: 0.0124 - momentum: 0.8506\n",
      "Epoch 363/366\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.2001 - accuracy: 0.9492 - val_loss: 0.2200 - val_accuracy: 0.9423 - lr: 0.0101 - momentum: 0.8688\n",
      "Epoch 364/366\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1986 - accuracy: 0.9551 - val_loss: 0.3045 - val_accuracy: 0.9407 - lr: 0.0058 - momentum: 0.9037\n",
      "Epoch 365/366\n",
      "256/256 [==============================] - 45s 177ms/step - loss: 0.1594 - accuracy: 0.9619 - val_loss: 0.2310 - val_accuracy: 0.9503 - lr: 0.0017 - momentum: 0.9367\n",
      "Epoch 366/366\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.1611 - accuracy: 0.9683 - val_loss: 0.2259 - val_accuracy: 0.9407 - lr: 4.9920e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9407\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2259\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m323.29 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m278.45 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m44.84 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [61] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m62\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 366)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01242\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 367/372\n",
      "256/256 [==============================] - 50s 181ms/step - loss: 0.1995 - accuracy: 0.9409 - val_loss: 0.2822 - val_accuracy: 0.9391 - lr: 0.0075 - momentum: 0.8915\n",
      "Epoch 368/372\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2105 - accuracy: 0.9443 - val_loss: 0.3644 - val_accuracy: 0.9279 - lr: 0.0124 - momentum: 0.8506\n",
      "Epoch 369/372\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2125 - accuracy: 0.9541 - val_loss: 0.4127 - val_accuracy: 0.9343 - lr: 0.0101 - momentum: 0.8688\n",
      "Epoch 370/372\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.2211 - accuracy: 0.9404 - val_loss: 0.3342 - val_accuracy: 0.9295 - lr: 0.0057 - momentum: 0.9037\n",
      "Epoch 371/372\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.1400 - accuracy: 0.9678 - val_loss: 0.3963 - val_accuracy: 0.9231 - lr: 0.0017 - momentum: 0.9367\n",
      "Epoch 372/372\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1065 - accuracy: 0.9736 - val_loss: 0.4282 - val_accuracy: 0.9279 - lr: 4.9680e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9279\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.4283\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m323.93 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m279.19 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m44.75 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [62] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m63\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 372)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01236\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 373/378\n",
      "256/256 [==============================] - 50s 180ms/step - loss: 0.1993 - accuracy: 0.9419 - val_loss: 0.6388 - val_accuracy: 0.8958 - lr: 0.0074 - momentum: 0.8915\n",
      "Epoch 374/378\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1768 - accuracy: 0.9512 - val_loss: 0.3051 - val_accuracy: 0.9359 - lr: 0.0123 - momentum: 0.8506\n",
      "Epoch 375/378\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1863 - accuracy: 0.9497 - val_loss: 0.5612 - val_accuracy: 0.8942 - lr: 0.0100 - momentum: 0.8688\n",
      "Epoch 376/378\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1359 - accuracy: 0.9736 - val_loss: 0.4201 - val_accuracy: 0.9215 - lr: 0.0057 - momentum: 0.9037\n",
      "Epoch 377/378\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.1530 - accuracy: 0.9727 - val_loss: 0.3494 - val_accuracy: 0.9359 - lr: 0.0016 - momentum: 0.9367\n",
      "Epoch 378/378\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1128 - accuracy: 0.9800 - val_loss: 0.4005 - val_accuracy: 0.9279 - lr: 4.9440e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9279\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.4006\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m324.05 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m278.63 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m45.42 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [63] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m64\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 378)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;31m- Debug DP Sample dir: \u001b[0m\u001b[0;32mSamples/TSR_SUB_400_y2023_m12_d22-h03_m51_s22\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0123\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 379/384\n",
      "256/256 [==============================] - 50s 181ms/step - loss: 0.2071 - accuracy: 0.9448 - val_loss: 0.2535 - val_accuracy: 0.9391 - lr: 0.0074 - momentum: 0.8915\n",
      "Epoch 380/384\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2124 - accuracy: 0.9375 - val_loss: 0.2774 - val_accuracy: 0.9519 - lr: 0.0122 - momentum: 0.8506\n",
      "Epoch 381/384\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1860 - accuracy: 0.9536 - val_loss: 0.3779 - val_accuracy: 0.9006 - lr: 0.0100 - momentum: 0.8688\n",
      "Epoch 382/384\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2226 - accuracy: 0.9448 - val_loss: 0.3410 - val_accuracy: 0.9022 - lr: 0.0057 - momentum: 0.9037\n",
      "Epoch 383/384\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.4106 - accuracy: 0.8672 - val_loss: 0.2579 - val_accuracy: 0.9119 - lr: 0.0016 - momentum: 0.9367\n",
      "Epoch 384/384\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.2415 - accuracy: 0.9312 - val_loss: 0.2493 - val_accuracy: 0.9359 - lr: 4.9200e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9359\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2493\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m337.92 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m280.58 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m57.34 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [64] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m65\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 384)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01224\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 385/390\n",
      "256/256 [==============================] - 50s 181ms/step - loss: 0.2728 - accuracy: 0.9106 - val_loss: 0.2036 - val_accuracy: 0.9439 - lr: 0.0074 - momentum: 0.8915\n",
      "Epoch 386/390\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.2622 - accuracy: 0.9248 - val_loss: 0.3522 - val_accuracy: 0.8878 - lr: 0.0122 - momentum: 0.8506\n",
      "Epoch 387/390\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2370 - accuracy: 0.9297 - val_loss: 0.2233 - val_accuracy: 0.9407 - lr: 0.0099 - momentum: 0.8688\n",
      "Epoch 388/390\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2396 - accuracy: 0.9438 - val_loss: 0.2375 - val_accuracy: 0.9343 - lr: 0.0057 - momentum: 0.9037\n",
      "Epoch 389/390\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1642 - accuracy: 0.9614 - val_loss: 0.2921 - val_accuracy: 0.9327 - lr: 0.0016 - momentum: 0.9367\n",
      "Epoch 390/390\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.1278 - accuracy: 0.9722 - val_loss: 0.2668 - val_accuracy: 0.9375 - lr: 4.8960e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;31mERROR: Failed to load weights. Error: max() arg is an empty sequence\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9375\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2668\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m325.07 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m279.13 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m45.94 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [65] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m66\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 390)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01218\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 391/396\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.2335 - accuracy: 0.9302 - val_loss: 0.4710 - val_accuracy: 0.9022 - lr: 0.0073 - momentum: 0.8915\n",
      "Epoch 392/396\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2452 - accuracy: 0.9243 - val_loss: 0.3078 - val_accuracy: 0.9327 - lr: 0.0121 - momentum: 0.8506\n",
      "Epoch 393/396\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2054 - accuracy: 0.9448 - val_loss: 0.2711 - val_accuracy: 0.9263 - lr: 0.0099 - momentum: 0.8688\n",
      "Epoch 394/396\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2032 - accuracy: 0.9512 - val_loss: 0.3266 - val_accuracy: 0.9279 - lr: 0.0056 - momentum: 0.9037\n",
      "Epoch 395/396\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1553 - accuracy: 0.9575 - val_loss: 0.3865 - val_accuracy: 0.9311 - lr: 0.0016 - momentum: 0.9367\n",
      "Epoch 396/396\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1363 - accuracy: 0.9634 - val_loss: 0.3360 - val_accuracy: 0.9327 - lr: 4.8720e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9327\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3360\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m327.52 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m281.07 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m46.44 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [66] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m67\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 396)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01212\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 397/402\n",
      "256/256 [==============================] - 50s 181ms/step - loss: 0.2119 - accuracy: 0.9331 - val_loss: 0.3073 - val_accuracy: 0.9183 - lr: 0.0073 - momentum: 0.8915\n",
      "Epoch 398/402\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2331 - accuracy: 0.9326 - val_loss: 0.4935 - val_accuracy: 0.8942 - lr: 0.0121 - momentum: 0.8506\n",
      "Epoch 399/402\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2253 - accuracy: 0.9390 - val_loss: 0.3324 - val_accuracy: 0.9279 - lr: 0.0098 - momentum: 0.8688\n",
      "Epoch 400/402\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1832 - accuracy: 0.9575 - val_loss: 0.3528 - val_accuracy: 0.9359 - lr: 0.0056 - momentum: 0.9037\n",
      "Epoch 401/402\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1368 - accuracy: 0.9673 - val_loss: 0.4640 - val_accuracy: 0.9295 - lr: 0.0016 - momentum: 0.9367\n",
      "Epoch 402/402\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1098 - accuracy: 0.9727 - val_loss: 0.4956 - val_accuracy: 0.9263 - lr: 4.8480e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9263\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.4957\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m327.68 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m280.88 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m46.79 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [67] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m68\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 402)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01206\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 403/408\n",
      "256/256 [==============================] - 50s 182ms/step - loss: 0.1981 - accuracy: 0.9429 - val_loss: 0.4115 - val_accuracy: 0.9311 - lr: 0.0073 - momentum: 0.8915\n",
      "Epoch 404/408\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.2037 - accuracy: 0.9409 - val_loss: 0.4208 - val_accuracy: 0.9327 - lr: 0.0120 - momentum: 0.8506\n",
      "Epoch 405/408\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2167 - accuracy: 0.9478 - val_loss: 0.3391 - val_accuracy: 0.9263 - lr: 0.0098 - momentum: 0.8688\n",
      "Epoch 406/408\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2090 - accuracy: 0.9482 - val_loss: 1.6039 - val_accuracy: 0.4167 - lr: 0.0056 - momentum: 0.9037\n",
      "Epoch 407/408\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2090 - accuracy: 0.9385 - val_loss: 0.3508 - val_accuracy: 0.9279 - lr: 0.0016 - momentum: 0.9367\n",
      "Epoch 408/408\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1647 - accuracy: 0.9629 - val_loss: 0.3060 - val_accuracy: 0.9279 - lr: 4.8240e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9279\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3060\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m325.31 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m279.16 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m46.15 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [68] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m69\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 408)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.012\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 409/414\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.2424 - accuracy: 0.9204 - val_loss: 0.2597 - val_accuracy: 0.9375 - lr: 0.0072 - momentum: 0.8915\n",
      "Epoch 410/414\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.2372 - accuracy: 0.9287 - val_loss: 0.4092 - val_accuracy: 0.9054 - lr: 0.0119 - momentum: 0.8506\n",
      "Epoch 411/414\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2232 - accuracy: 0.9429 - val_loss: 0.3071 - val_accuracy: 0.9375 - lr: 0.0097 - momentum: 0.8688\n",
      "Epoch 412/414\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1619 - accuracy: 0.9561 - val_loss: 0.3597 - val_accuracy: 0.9263 - lr: 0.0056 - momentum: 0.9037\n",
      "Epoch 413/414\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1335 - accuracy: 0.9707 - val_loss: 0.3930 - val_accuracy: 0.9022 - lr: 0.0016 - momentum: 0.9367\n",
      "Epoch 414/414\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1109 - accuracy: 0.9751 - val_loss: 0.3899 - val_accuracy: 0.9215 - lr: 4.8000e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9215\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3899\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m326.10 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m280.09 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m46.01 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [69] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m70\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 414)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01194\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 415/420\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.1985 - accuracy: 0.9414 - val_loss: 0.3671 - val_accuracy: 0.9391 - lr: 0.0072 - momentum: 0.8915\n",
      "Epoch 416/420\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.2515 - accuracy: 0.9468 - val_loss: 0.3416 - val_accuracy: 0.9439 - lr: 0.0119 - momentum: 0.8506\n",
      "Epoch 417/420\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.1785 - accuracy: 0.9507 - val_loss: 0.3911 - val_accuracy: 0.9503 - lr: 0.0097 - momentum: 0.8688\n",
      "Epoch 418/420\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1634 - accuracy: 0.9658 - val_loss: 0.2619 - val_accuracy: 0.9375 - lr: 0.0055 - momentum: 0.9037\n",
      "Epoch 419/420\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1609 - accuracy: 0.9644 - val_loss: 0.2554 - val_accuracy: 0.9391 - lr: 0.0016 - momentum: 0.9367\n",
      "Epoch 420/420\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.1210 - accuracy: 0.9775 - val_loss: 0.3009 - val_accuracy: 0.9423 - lr: 4.7760e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9423\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3009\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m328.70 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m282.09 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m46.61 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [70] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m71\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 420)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01188\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 421/426\n",
      "256/256 [==============================] - 50s 183ms/step - loss: 0.1919 - accuracy: 0.9419 - val_loss: 0.2697 - val_accuracy: 0.9311 - lr: 0.0072 - momentum: 0.8915\n",
      "Epoch 422/426\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2115 - accuracy: 0.9448 - val_loss: 0.5592 - val_accuracy: 0.9022 - lr: 0.0118 - momentum: 0.8506\n",
      "Epoch 423/426\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2710 - accuracy: 0.9258 - val_loss: 0.3152 - val_accuracy: 0.9167 - lr: 0.0096 - momentum: 0.8688\n",
      "Epoch 424/426\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1848 - accuracy: 0.9531 - val_loss: 0.2302 - val_accuracy: 0.9439 - lr: 0.0055 - momentum: 0.9037\n",
      "Epoch 425/426\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1284 - accuracy: 0.9697 - val_loss: 0.2284 - val_accuracy: 0.9343 - lr: 0.0016 - momentum: 0.9367\n",
      "Epoch 426/426\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1220 - accuracy: 0.9702 - val_loss: 0.2743 - val_accuracy: 0.9439 - lr: 4.7520e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9439\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2743\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m327.68 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m279.77 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m47.91 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [71] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m72\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 426)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33m└───Shuffling data...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01182\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 427/432\n",
      "256/256 [==============================] - 51s 183ms/step - loss: 0.2220 - accuracy: 0.9268 - val_loss: 0.2893 - val_accuracy: 0.9311 - lr: 0.0071 - momentum: 0.8915\n",
      "Epoch 428/432\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1984 - accuracy: 0.9468 - val_loss: 0.2627 - val_accuracy: 0.9391 - lr: 0.0118 - momentum: 0.8506\n",
      "Epoch 429/432\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1814 - accuracy: 0.9561 - val_loss: 0.2701 - val_accuracy: 0.9247 - lr: 0.0096 - momentum: 0.8688\n",
      "Epoch 430/432\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1460 - accuracy: 0.9683 - val_loss: 0.2917 - val_accuracy: 0.9215 - lr: 0.0055 - momentum: 0.9037\n",
      "Epoch 431/432\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1252 - accuracy: 0.9688 - val_loss: 0.2932 - val_accuracy: 0.9279 - lr: 0.0016 - momentum: 0.9367\n",
      "Epoch 432/432\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1040 - accuracy: 0.9785 - val_loss: 0.2979 - val_accuracy: 0.9343 - lr: 4.7280e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9343\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2979\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m330.52 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m280.69 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m49.83 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [72] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m73\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 432)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01176\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 433/438\n",
      "256/256 [==============================] - 50s 183ms/step - loss: 0.2199 - accuracy: 0.9404 - val_loss: 0.3522 - val_accuracy: 0.9295 - lr: 0.0071 - momentum: 0.8915\n",
      "Epoch 434/438\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1987 - accuracy: 0.9419 - val_loss: 0.2896 - val_accuracy: 0.9359 - lr: 0.0117 - momentum: 0.8506\n",
      "Epoch 435/438\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2374 - accuracy: 0.9331 - val_loss: 0.3696 - val_accuracy: 0.8942 - lr: 0.0095 - momentum: 0.8688\n",
      "Epoch 436/438\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2211 - accuracy: 0.9424 - val_loss: 0.2876 - val_accuracy: 0.9327 - lr: 0.0054 - momentum: 0.9037\n",
      "Epoch 437/438\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1664 - accuracy: 0.9590 - val_loss: 0.3804 - val_accuracy: 0.9135 - lr: 0.0016 - momentum: 0.9367\n",
      "Epoch 438/438\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1248 - accuracy: 0.9683 - val_loss: 0.4128 - val_accuracy: 0.9151 - lr: 4.7040e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9151\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.4128\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m327.69 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m280.16 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m47.53 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [73] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m74\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 438)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0117\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 439/444\n",
      "256/256 [==============================] - 50s 183ms/step - loss: 0.1985 - accuracy: 0.9458 - val_loss: 0.2751 - val_accuracy: 0.9391 - lr: 0.0070 - momentum: 0.8915\n",
      "Epoch 440/444\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2099 - accuracy: 0.9419 - val_loss: 0.3547 - val_accuracy: 0.9135 - lr: 0.0116 - momentum: 0.8506\n",
      "Epoch 441/444\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1777 - accuracy: 0.9487 - val_loss: 0.4030 - val_accuracy: 0.9119 - lr: 0.0095 - momentum: 0.8688\n",
      "Epoch 442/444\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2029 - accuracy: 0.9619 - val_loss: 0.3166 - val_accuracy: 0.9343 - lr: 0.0054 - momentum: 0.9037\n",
      "Epoch 443/444\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1744 - accuracy: 0.9619 - val_loss: 0.3691 - val_accuracy: 0.9311 - lr: 0.0016 - momentum: 0.9367\n",
      "Epoch 444/444\n",
      "256/256 [==============================] - 46s 177ms/step - loss: 0.1244 - accuracy: 0.9746 - val_loss: 0.4260 - val_accuracy: 0.9279 - lr: 4.6800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9279\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.4260\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m328.49 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m280.19 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m48.30 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [74] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m75\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 444)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01164\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 445/450\n",
      "256/256 [==============================] - 50s 182ms/step - loss: 0.1918 - accuracy: 0.9497 - val_loss: 0.2884 - val_accuracy: 0.9375 - lr: 0.0070 - momentum: 0.8915\n",
      "Epoch 446/450\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1896 - accuracy: 0.9512 - val_loss: 0.3865 - val_accuracy: 0.9295 - lr: 0.0116 - momentum: 0.8506\n",
      "Epoch 447/450\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1949 - accuracy: 0.9526 - val_loss: 0.3544 - val_accuracy: 0.9038 - lr: 0.0094 - momentum: 0.8688\n",
      "Epoch 448/450\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1458 - accuracy: 0.9697 - val_loss: 0.3064 - val_accuracy: 0.9311 - lr: 0.0054 - momentum: 0.9037\n",
      "Epoch 449/450\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1634 - accuracy: 0.9604 - val_loss: 0.2541 - val_accuracy: 0.9375 - lr: 0.0016 - momentum: 0.9367\n",
      "Epoch 450/450\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1089 - accuracy: 0.9751 - val_loss: 0.2729 - val_accuracy: 0.9247 - lr: 4.6560e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9247\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2729\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m328.95 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m280.78 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m48.17 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [75] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m76\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 450)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01158\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 451/456\n",
      "256/256 [==============================] - 50s 183ms/step - loss: 0.2100 - accuracy: 0.9326 - val_loss: 0.3282 - val_accuracy: 0.9215 - lr: 0.0070 - momentum: 0.8915\n",
      "Epoch 452/456\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2066 - accuracy: 0.9404 - val_loss: 0.3057 - val_accuracy: 0.9279 - lr: 0.0115 - momentum: 0.8506\n",
      "Epoch 453/456\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2336 - accuracy: 0.9370 - val_loss: 0.6003 - val_accuracy: 0.8910 - lr: 0.0094 - momentum: 0.8688\n",
      "Epoch 454/456\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2006 - accuracy: 0.9536 - val_loss: 0.2152 - val_accuracy: 0.9311 - lr: 0.0054 - momentum: 0.9037\n",
      "Epoch 455/456\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1549 - accuracy: 0.9683 - val_loss: 0.2208 - val_accuracy: 0.9327 - lr: 0.0015 - momentum: 0.9367\n",
      "Epoch 456/456\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1337 - accuracy: 0.9639 - val_loss: 0.2067 - val_accuracy: 0.9471 - lr: 4.6320e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9471\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2066\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.20500072836875916. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m329.90 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m281.11 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m48.79 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [76] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m77\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 456)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01152\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 457/462\n",
      "256/256 [==============================] - 51s 183ms/step - loss: 0.2102 - accuracy: 0.9434 - val_loss: 0.2762 - val_accuracy: 0.9263 - lr: 0.0069 - momentum: 0.8915\n",
      "Epoch 458/462\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2329 - accuracy: 0.9370 - val_loss: 0.3336 - val_accuracy: 0.9247 - lr: 0.0115 - momentum: 0.8506\n",
      "Epoch 459/462\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2184 - accuracy: 0.9478 - val_loss: 0.3217 - val_accuracy: 0.9391 - lr: 0.0094 - momentum: 0.8688\n",
      "Epoch 460/462\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.2258 - accuracy: 0.9360 - val_loss: 0.2642 - val_accuracy: 0.9359 - lr: 0.0053 - momentum: 0.9037\n",
      "Epoch 461/462\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1569 - accuracy: 0.9600 - val_loss: 0.1852 - val_accuracy: 0.9519 - lr: 0.0015 - momentum: 0.9367\n",
      "Epoch 462/462\n",
      "256/256 [==============================] - 46s 178ms/step - loss: 0.1207 - accuracy: 0.9722 - val_loss: 0.1820 - val_accuracy: 0.9503 - lr: 4.6080e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-461-0.9519.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9519\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.1852\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model loss from \u001b[0m\u001b[0;32m0.20500072836875916 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.1852283775806427\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m332.46 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m281.08 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m51.38 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [77] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m78\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 462)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01146\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 463/468\n",
      "256/256 [==============================] - 51s 186ms/step - loss: 0.2278 - accuracy: 0.9287 - val_loss: 0.1914 - val_accuracy: 0.9455 - lr: 0.0069 - momentum: 0.8915\n",
      "Epoch 464/468\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2232 - accuracy: 0.9263 - val_loss: 0.2165 - val_accuracy: 0.9247 - lr: 0.0114 - momentum: 0.8506\n",
      "Epoch 465/468\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1997 - accuracy: 0.9438 - val_loss: 0.2093 - val_accuracy: 0.9343 - lr: 0.0093 - momentum: 0.8688\n",
      "Epoch 466/468\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1686 - accuracy: 0.9585 - val_loss: 0.2183 - val_accuracy: 0.9471 - lr: 0.0053 - momentum: 0.9037\n",
      "Epoch 467/468\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.1469 - accuracy: 0.9624 - val_loss: 0.1976 - val_accuracy: 0.9423 - lr: 0.0015 - momentum: 0.9367\n",
      "Epoch 468/468\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1212 - accuracy: 0.9663 - val_loss: 0.1866 - val_accuracy: 0.9455 - lr: 4.5840e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9455\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.1866\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.1852283775806427. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m334.31 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m283.91 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m50.40 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [78] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m79\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 468)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0114\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 469/474\n",
      "256/256 [==============================] - 51s 183ms/step - loss: 0.1757 - accuracy: 0.9502 - val_loss: 0.2176 - val_accuracy: 0.9455 - lr: 0.0069 - momentum: 0.8915\n",
      "Epoch 470/474\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1973 - accuracy: 0.9526 - val_loss: 0.3784 - val_accuracy: 0.9391 - lr: 0.0113 - momentum: 0.8506\n",
      "Epoch 471/474\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1922 - accuracy: 0.9507 - val_loss: 0.1918 - val_accuracy: 0.9503 - lr: 0.0093 - momentum: 0.8688\n",
      "Epoch 472/474\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2003 - accuracy: 0.9595 - val_loss: 0.2401 - val_accuracy: 0.9503 - lr: 0.0053 - momentum: 0.9037\n",
      "Epoch 473/474\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1738 - accuracy: 0.9609 - val_loss: 0.1883 - val_accuracy: 0.9471 - lr: 0.0015 - momentum: 0.9367\n",
      "Epoch 474/474\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1348 - accuracy: 0.9717 - val_loss: 0.1817 - val_accuracy: 0.9471 - lr: 4.5600e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-471-0.9503.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9503\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.1918\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9567307829856873. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.1852283775806427. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m333.90 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m282.81 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m51.10 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [79] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m80\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 474)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01134\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 475/480\n",
      "256/256 [==============================] - 51s 186ms/step - loss: 0.2342 - accuracy: 0.9380 - val_loss: 0.2145 - val_accuracy: 0.9503 - lr: 0.0068 - momentum: 0.8915\n",
      "Epoch 476/480\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2019 - accuracy: 0.9473 - val_loss: 0.2357 - val_accuracy: 0.9295 - lr: 0.0113 - momentum: 0.8506\n",
      "Epoch 477/480\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1574 - accuracy: 0.9604 - val_loss: 0.2965 - val_accuracy: 0.9423 - lr: 0.0092 - momentum: 0.8688\n",
      "Epoch 478/480\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1408 - accuracy: 0.9653 - val_loss: 0.1871 - val_accuracy: 0.9599 - lr: 0.0052 - momentum: 0.9037\n",
      "Epoch 479/480\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1129 - accuracy: 0.9771 - val_loss: 0.2315 - val_accuracy: 0.9439 - lr: 0.0015 - momentum: 0.9367\n",
      "Epoch 480/480\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1149 - accuracy: 0.9790 - val_loss: 0.1957 - val_accuracy: 0.9455 - lr: 4.5360e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-478-0.9599.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9599\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.1871\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model accuracy from \u001b[0m\u001b[0;32m0.9567307829856873 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.9599359035491943\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.1852283775806427. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m336.16 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m283.56 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m52.61 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [80] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m81\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 480)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01128\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 481/486\n",
      "256/256 [==============================] - 52s 186ms/step - loss: 0.1831 - accuracy: 0.9502 - val_loss: 0.1883 - val_accuracy: 0.9487 - lr: 0.0068 - momentum: 0.8915\n",
      "Epoch 482/486\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.2238 - accuracy: 0.9370 - val_loss: 0.1762 - val_accuracy: 0.9567 - lr: 0.0112 - momentum: 0.8506\n",
      "Epoch 483/486\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1968 - accuracy: 0.9546 - val_loss: 0.2420 - val_accuracy: 0.9375 - lr: 0.0092 - momentum: 0.8688\n",
      "Epoch 484/486\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1566 - accuracy: 0.9639 - val_loss: 0.2655 - val_accuracy: 0.9263 - lr: 0.0052 - momentum: 0.9037\n",
      "Epoch 485/486\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1233 - accuracy: 0.9722 - val_loss: 0.3470 - val_accuracy: 0.9071 - lr: 0.0015 - momentum: 0.9367\n",
      "Epoch 486/486\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.0971 - accuracy: 0.9810 - val_loss: 0.3051 - val_accuracy: 0.9295 - lr: 4.5120e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-482-0.9567.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9567\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.1762\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model loss from \u001b[0m\u001b[0;32m0.1852283775806427 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.17617985606193542\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m338.26 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m283.66 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m54.60 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [81] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m82\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 486)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01122\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 487/492\n",
      "256/256 [==============================] - 51s 186ms/step - loss: 0.2008 - accuracy: 0.9443 - val_loss: 0.1760 - val_accuracy: 0.9519 - lr: 0.0068 - momentum: 0.8915\n",
      "Epoch 488/492\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1629 - accuracy: 0.9619 - val_loss: 0.2700 - val_accuracy: 0.9327 - lr: 0.0112 - momentum: 0.8506\n",
      "Epoch 489/492\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1441 - accuracy: 0.9648 - val_loss: 0.2800 - val_accuracy: 0.9327 - lr: 0.0091 - momentum: 0.8688\n",
      "Epoch 490/492\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1798 - accuracy: 0.9600 - val_loss: 0.5160 - val_accuracy: 0.9135 - lr: 0.0052 - momentum: 0.9037\n",
      "Epoch 491/492\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1522 - accuracy: 0.9668 - val_loss: 0.3251 - val_accuracy: 0.9295 - lr: 0.0015 - momentum: 0.9367\n",
      "Epoch 492/492\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1104 - accuracy: 0.9761 - val_loss: 0.3280 - val_accuracy: 0.9279 - lr: 4.4880e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-487-0.9519.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9519\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.1760\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model loss from \u001b[0m\u001b[0;32m0.17617985606193542 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.17603926360607147\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m337.12 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m282.51 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m54.60 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [82] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m83\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 492)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01116\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 493/498\n",
      "256/256 [==============================] - 51s 187ms/step - loss: 0.2007 - accuracy: 0.9419 - val_loss: 0.2081 - val_accuracy: 0.9327 - lr: 0.0067 - momentum: 0.8915\n",
      "Epoch 494/498\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2041 - accuracy: 0.9438 - val_loss: 0.2344 - val_accuracy: 0.9295 - lr: 0.0111 - momentum: 0.8506\n",
      "Epoch 495/498\n",
      "256/256 [==============================] - 47s 183ms/step - loss: 0.1820 - accuracy: 0.9580 - val_loss: 0.1937 - val_accuracy: 0.9503 - lr: 0.0091 - momentum: 0.8688\n",
      "Epoch 496/498\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1421 - accuracy: 0.9673 - val_loss: 0.2166 - val_accuracy: 0.9439 - lr: 0.0052 - momentum: 0.9037\n",
      "Epoch 497/498\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1428 - accuracy: 0.9653 - val_loss: 0.1948 - val_accuracy: 0.9455 - lr: 0.0015 - momentum: 0.9367\n",
      "Epoch 498/498\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1332 - accuracy: 0.9702 - val_loss: 0.2214 - val_accuracy: 0.9439 - lr: 4.4640e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9439\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2214\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m336.41 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m283.57 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m52.84 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [83] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m84\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 498)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0111\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 499/504\n",
      "256/256 [==============================] - 51s 184ms/step - loss: 0.2168 - accuracy: 0.9409 - val_loss: 0.2371 - val_accuracy: 0.9423 - lr: 0.0067 - momentum: 0.8915\n",
      "Epoch 500/504\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2798 - accuracy: 0.9038 - val_loss: 1.5811 - val_accuracy: 0.3862 - lr: 0.0110 - momentum: 0.8506\n",
      "Epoch 501/504\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.4885 - accuracy: 0.8281 - val_loss: 0.3287 - val_accuracy: 0.9071 - lr: 0.0090 - momentum: 0.8688\n",
      "Epoch 502/504\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.3234 - accuracy: 0.9023 - val_loss: 0.4095 - val_accuracy: 0.8622 - lr: 0.0051 - momentum: 0.9037\n",
      "Epoch 503/504\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2540 - accuracy: 0.9277 - val_loss: 0.2722 - val_accuracy: 0.9343 - lr: 0.0015 - momentum: 0.9367\n",
      "Epoch 504/504\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2126 - accuracy: 0.9404 - val_loss: 0.2674 - val_accuracy: 0.9279 - lr: 4.4400e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9279\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2674\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m333.93 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m281.70 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m52.23 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [84] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m85\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 504)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01104\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 505/510\n",
      "256/256 [==============================] - 51s 184ms/step - loss: 0.2841 - accuracy: 0.9102 - val_loss: 0.2492 - val_accuracy: 0.9295 - lr: 0.0066 - momentum: 0.8915\n",
      "Epoch 506/510\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2734 - accuracy: 0.9165 - val_loss: 0.2569 - val_accuracy: 0.9231 - lr: 0.0110 - momentum: 0.8506\n",
      "Epoch 507/510\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2439 - accuracy: 0.9253 - val_loss: 0.3360 - val_accuracy: 0.8894 - lr: 0.0090 - momentum: 0.8688\n",
      "Epoch 508/510\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2390 - accuracy: 0.9292 - val_loss: 0.2527 - val_accuracy: 0.9151 - lr: 0.0051 - momentum: 0.9037\n",
      "Epoch 509/510\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1864 - accuracy: 0.9482 - val_loss: 0.3553 - val_accuracy: 0.8878 - lr: 0.0015 - momentum: 0.9367\n",
      "Epoch 510/510\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1744 - accuracy: 0.9438 - val_loss: 0.3095 - val_accuracy: 0.8958 - lr: 4.4160e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.8958\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3095\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m334.09 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m282.77 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m51.33 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [85] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m86\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 510)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01098\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 511/516\n",
      "256/256 [==============================] - 51s 184ms/step - loss: 0.2299 - accuracy: 0.9194 - val_loss: 0.3775 - val_accuracy: 0.8942 - lr: 0.0066 - momentum: 0.8915\n",
      "Epoch 512/516\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2336 - accuracy: 0.9224 - val_loss: 0.4209 - val_accuracy: 0.8766 - lr: 0.0109 - momentum: 0.8506\n",
      "Epoch 513/516\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2213 - accuracy: 0.9326 - val_loss: 0.3701 - val_accuracy: 0.8910 - lr: 0.0089 - momentum: 0.8688\n",
      "Epoch 514/516\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2196 - accuracy: 0.9316 - val_loss: 0.3015 - val_accuracy: 0.9119 - lr: 0.0051 - momentum: 0.9037\n",
      "Epoch 515/516\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1547 - accuracy: 0.9551 - val_loss: 0.4705 - val_accuracy: 0.8958 - lr: 0.0015 - momentum: 0.9367\n",
      "Epoch 516/516\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1172 - accuracy: 0.9673 - val_loss: 0.5644 - val_accuracy: 0.8862 - lr: 4.3920e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.8862\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.5644\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m333.32 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m282.27 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m51.05 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [86] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m87\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 516)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01092\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 517/522\n",
      "256/256 [==============================] - 51s 184ms/step - loss: 0.2431 - accuracy: 0.9302 - val_loss: 0.4163 - val_accuracy: 0.8974 - lr: 0.0066 - momentum: 0.8915\n",
      "Epoch 518/522\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2538 - accuracy: 0.9268 - val_loss: 0.3237 - val_accuracy: 0.9231 - lr: 0.0109 - momentum: 0.8506\n",
      "Epoch 519/522\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2172 - accuracy: 0.9468 - val_loss: 0.5438 - val_accuracy: 0.8590 - lr: 0.0089 - momentum: 0.8688\n",
      "Epoch 520/522\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2061 - accuracy: 0.9468 - val_loss: 0.2792 - val_accuracy: 0.9103 - lr: 0.0051 - momentum: 0.9037\n",
      "Epoch 521/522\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1580 - accuracy: 0.9570 - val_loss: 0.2834 - val_accuracy: 0.9119 - lr: 0.0015 - momentum: 0.9367\n",
      "Epoch 522/522\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1259 - accuracy: 0.9702 - val_loss: 0.3188 - val_accuracy: 0.9119 - lr: 4.3680e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9119\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3188\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m334.25 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m282.96 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m51.29 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [87] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m88\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 522)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01086\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 523/528\n",
      "256/256 [==============================] - 51s 184ms/step - loss: 0.1763 - accuracy: 0.9492 - val_loss: 0.2541 - val_accuracy: 0.9295 - lr: 0.0065 - momentum: 0.8915\n",
      "Epoch 524/528\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2082 - accuracy: 0.9346 - val_loss: 0.2674 - val_accuracy: 0.9151 - lr: 0.0108 - momentum: 0.8506\n",
      "Epoch 525/528\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1956 - accuracy: 0.9502 - val_loss: 0.2429 - val_accuracy: 0.9375 - lr: 0.0088 - momentum: 0.8688\n",
      "Epoch 526/528\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1450 - accuracy: 0.9629 - val_loss: 0.3454 - val_accuracy: 0.9231 - lr: 0.0050 - momentum: 0.9037\n",
      "Epoch 527/528\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1203 - accuracy: 0.9775 - val_loss: 0.3013 - val_accuracy: 0.9167 - lr: 0.0014 - momentum: 0.9367\n",
      "Epoch 528/528\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.0980 - accuracy: 0.9795 - val_loss: 0.2988 - val_accuracy: 0.9199 - lr: 4.3440e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9199\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2988\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m334.91 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m282.87 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m52.04 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [88] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m89\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 528)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0108\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 529/534\n",
      "256/256 [==============================] - 51s 184ms/step - loss: 0.2229 - accuracy: 0.9336 - val_loss: 0.2800 - val_accuracy: 0.9375 - lr: 0.0065 - momentum: 0.8915\n",
      "Epoch 530/534\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2037 - accuracy: 0.9399 - val_loss: 0.2988 - val_accuracy: 0.9343 - lr: 0.0107 - momentum: 0.8506\n",
      "Epoch 531/534\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1945 - accuracy: 0.9443 - val_loss: 0.3587 - val_accuracy: 0.9038 - lr: 0.0088 - momentum: 0.8688\n",
      "Epoch 532/534\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1608 - accuracy: 0.9604 - val_loss: 0.2243 - val_accuracy: 0.9407 - lr: 0.0050 - momentum: 0.9037\n",
      "Epoch 533/534\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1717 - accuracy: 0.9551 - val_loss: 0.3388 - val_accuracy: 0.9167 - lr: 0.0014 - momentum: 0.9367\n",
      "Epoch 534/534\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1270 - accuracy: 0.9658 - val_loss: 0.2928 - val_accuracy: 0.9279 - lr: 4.3200e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9279\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2928\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m335.29 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m283.12 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m52.17 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [89] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m90\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 534)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33m└───Shuffling data...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01074\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 535/540\n",
      "256/256 [==============================] - 51s 184ms/step - loss: 0.2553 - accuracy: 0.9204 - val_loss: 0.2754 - val_accuracy: 0.9038 - lr: 0.0065 - momentum: 0.8915\n",
      "Epoch 536/540\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2352 - accuracy: 0.9268 - val_loss: 0.2348 - val_accuracy: 0.9311 - lr: 0.0107 - momentum: 0.8506\n",
      "Epoch 537/540\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2315 - accuracy: 0.9336 - val_loss: 0.2594 - val_accuracy: 0.9167 - lr: 0.0087 - momentum: 0.8688\n",
      "Epoch 538/540\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1771 - accuracy: 0.9561 - val_loss: 0.2493 - val_accuracy: 0.9311 - lr: 0.0050 - momentum: 0.9037\n",
      "Epoch 539/540\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1767 - accuracy: 0.9531 - val_loss: 0.3012 - val_accuracy: 0.9103 - lr: 0.0014 - momentum: 0.9367\n",
      "Epoch 540/540\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1363 - accuracy: 0.9658 - val_loss: 0.3091 - val_accuracy: 0.9231 - lr: 4.2960e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9231\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3091\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m338.08 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m282.91 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m55.17 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [90] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m91\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 540)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01068\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 541/546\n",
      "256/256 [==============================] - 51s 184ms/step - loss: 0.2324 - accuracy: 0.9355 - val_loss: 0.2321 - val_accuracy: 0.9311 - lr: 0.0064 - momentum: 0.8915\n",
      "Epoch 542/546\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2370 - accuracy: 0.9365 - val_loss: 0.2630 - val_accuracy: 0.9247 - lr: 0.0106 - momentum: 0.8506\n",
      "Epoch 543/546\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2311 - accuracy: 0.9336 - val_loss: 0.3506 - val_accuracy: 0.9167 - lr: 0.0087 - momentum: 0.8688\n",
      "Epoch 544/546\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2093 - accuracy: 0.9463 - val_loss: 0.2880 - val_accuracy: 0.9311 - lr: 0.0049 - momentum: 0.9037\n",
      "Epoch 545/546\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1566 - accuracy: 0.9629 - val_loss: 0.2458 - val_accuracy: 0.9263 - lr: 0.0014 - momentum: 0.9367\n",
      "Epoch 546/546\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1261 - accuracy: 0.9678 - val_loss: 0.2360 - val_accuracy: 0.9263 - lr: 4.2720e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9263\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2360\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m334.53 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m282.56 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m51.97 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [91] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m92\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 546)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01062\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 547/552\n",
      "256/256 [==============================] - 51s 183ms/step - loss: 0.1908 - accuracy: 0.9429 - val_loss: 0.2946 - val_accuracy: 0.9311 - lr: 0.0064 - momentum: 0.8915\n",
      "Epoch 548/552\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2102 - accuracy: 0.9370 - val_loss: 0.2310 - val_accuracy: 0.9199 - lr: 0.0106 - momentum: 0.8506\n",
      "Epoch 549/552\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1688 - accuracy: 0.9536 - val_loss: 0.2412 - val_accuracy: 0.9247 - lr: 0.0086 - momentum: 0.8688\n",
      "Epoch 550/552\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1506 - accuracy: 0.9614 - val_loss: 0.2306 - val_accuracy: 0.9279 - lr: 0.0049 - momentum: 0.9037\n",
      "Epoch 551/552\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2527 - accuracy: 0.9248 - val_loss: 0.2805 - val_accuracy: 0.9135 - lr: 0.0014 - momentum: 0.9367\n",
      "Epoch 552/552\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2524 - accuracy: 0.9219 - val_loss: 0.2080 - val_accuracy: 0.9439 - lr: 4.2480e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9439\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2080\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m335.50 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m281.77 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m53.73 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [92] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m93\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 552)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01056\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 553/558\n",
      "256/256 [==============================] - 51s 183ms/step - loss: 0.2802 - accuracy: 0.9033 - val_loss: 0.3070 - val_accuracy: 0.8702 - lr: 0.0064 - momentum: 0.8915\n",
      "Epoch 554/558\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2819 - accuracy: 0.9077 - val_loss: 0.3020 - val_accuracy: 0.9022 - lr: 0.0105 - momentum: 0.8506\n",
      "Epoch 555/558\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.2129 - accuracy: 0.9341 - val_loss: 0.2712 - val_accuracy: 0.9087 - lr: 0.0086 - momentum: 0.8688\n",
      "Epoch 556/558\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1881 - accuracy: 0.9492 - val_loss: 0.2646 - val_accuracy: 0.9231 - lr: 0.0049 - momentum: 0.9037\n",
      "Epoch 557/558\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1644 - accuracy: 0.9575 - val_loss: 0.2279 - val_accuracy: 0.9247 - lr: 0.0014 - momentum: 0.9367\n",
      "Epoch 558/558\n",
      "256/256 [==============================] - 46s 179ms/step - loss: 0.1322 - accuracy: 0.9648 - val_loss: 0.2427 - val_accuracy: 0.9247 - lr: 4.2240e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9247\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2427\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m335.69 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m282.18 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m53.52 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [93] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m94\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 558)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0105\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 559/564\n",
      "256/256 [==============================] - 51s 184ms/step - loss: 0.2274 - accuracy: 0.9258 - val_loss: 0.2463 - val_accuracy: 0.9247 - lr: 0.0063 - momentum: 0.8915\n",
      "Epoch 560/564\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2199 - accuracy: 0.9316 - val_loss: 0.3196 - val_accuracy: 0.9231 - lr: 0.0104 - momentum: 0.8506\n",
      "Epoch 561/564\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1923 - accuracy: 0.9512 - val_loss: 0.3223 - val_accuracy: 0.9263 - lr: 0.0085 - momentum: 0.8688\n",
      "Epoch 562/564\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1664 - accuracy: 0.9526 - val_loss: 0.3450 - val_accuracy: 0.9295 - lr: 0.0049 - momentum: 0.9037\n",
      "Epoch 563/564\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1356 - accuracy: 0.9688 - val_loss: 0.3262 - val_accuracy: 0.9215 - lr: 0.0014 - momentum: 0.9367\n",
      "Epoch 564/564\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1128 - accuracy: 0.9707 - val_loss: 0.4118 - val_accuracy: 0.9167 - lr: 4.2000e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9167\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.4117\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m336.89 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m282.83 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m54.06 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [94] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m95\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 564)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01044\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 565/570\n",
      "256/256 [==============================] - 51s 184ms/step - loss: 0.1628 - accuracy: 0.9497 - val_loss: 0.3142 - val_accuracy: 0.9215 - lr: 0.0063 - momentum: 0.8915\n",
      "Epoch 566/570\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1794 - accuracy: 0.9512 - val_loss: 0.2638 - val_accuracy: 0.9199 - lr: 0.0104 - momentum: 0.8506\n",
      "Epoch 567/570\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1562 - accuracy: 0.9600 - val_loss: 0.2997 - val_accuracy: 0.9183 - lr: 0.0085 - momentum: 0.8688\n",
      "Epoch 568/570\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1372 - accuracy: 0.9653 - val_loss: 0.3107 - val_accuracy: 0.9103 - lr: 0.0048 - momentum: 0.9037\n",
      "Epoch 569/570\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1054 - accuracy: 0.9741 - val_loss: 0.3363 - val_accuracy: 0.9247 - lr: 0.0014 - momentum: 0.9367\n",
      "Epoch 570/570\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.0834 - accuracy: 0.9814 - val_loss: 0.3877 - val_accuracy: 0.9215 - lr: 4.1760e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9215\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3878\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m335.70 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m282.71 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m53.00 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [95] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m96\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 570)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01038\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 571/576\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.1892 - accuracy: 0.9360 - val_loss: 0.3353 - val_accuracy: 0.9263 - lr: 0.0062 - momentum: 0.8915\n",
      "Epoch 572/576\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2201 - accuracy: 0.9375 - val_loss: 0.4019 - val_accuracy: 0.9263 - lr: 0.0103 - momentum: 0.8506\n",
      "Epoch 573/576\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.2044 - accuracy: 0.9517 - val_loss: 0.4528 - val_accuracy: 0.9103 - lr: 0.0084 - momentum: 0.8688\n",
      "Epoch 574/576\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1589 - accuracy: 0.9634 - val_loss: 0.3456 - val_accuracy: 0.9167 - lr: 0.0048 - momentum: 0.9037\n",
      "Epoch 575/576\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1979 - accuracy: 0.9438 - val_loss: 0.2332 - val_accuracy: 0.9247 - lr: 0.0014 - momentum: 0.9367\n",
      "Epoch 576/576\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.2101 - accuracy: 0.9341 - val_loss: 0.2202 - val_accuracy: 0.9327 - lr: 4.1520e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9327\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2202\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m337.49 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m283.96 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m53.53 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [96] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m97\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 576)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01032\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 577/582\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.2547 - accuracy: 0.9233 - val_loss: 0.2767 - val_accuracy: 0.9295 - lr: 0.0062 - momentum: 0.8915\n",
      "Epoch 578/582\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.2499 - accuracy: 0.9272 - val_loss: 0.2187 - val_accuracy: 0.9279 - lr: 0.0103 - momentum: 0.8506\n",
      "Epoch 579/582\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2153 - accuracy: 0.9429 - val_loss: 0.2452 - val_accuracy: 0.9359 - lr: 0.0084 - momentum: 0.8688\n",
      "Epoch 580/582\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1696 - accuracy: 0.9551 - val_loss: 0.4149 - val_accuracy: 0.9167 - lr: 0.0048 - momentum: 0.9037\n",
      "Epoch 581/582\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1423 - accuracy: 0.9639 - val_loss: 0.3355 - val_accuracy: 0.9279 - lr: 0.0014 - momentum: 0.9367\n",
      "Epoch 582/582\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.0996 - accuracy: 0.9785 - val_loss: 0.3712 - val_accuracy: 0.9215 - lr: 4.1280e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9215\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3712\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m337.11 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m283.64 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m53.47 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [97] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m98\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 582)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01026\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 583/588\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.2201 - accuracy: 0.9385 - val_loss: 0.2735 - val_accuracy: 0.9151 - lr: 0.0062 - momentum: 0.8915\n",
      "Epoch 584/588\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2809 - accuracy: 0.9204 - val_loss: 0.2670 - val_accuracy: 0.9199 - lr: 0.0102 - momentum: 0.8506\n",
      "Epoch 585/588\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1999 - accuracy: 0.9458 - val_loss: 0.2282 - val_accuracy: 0.9279 - lr: 0.0083 - momentum: 0.8688\n",
      "Epoch 586/588\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.1734 - accuracy: 0.9565 - val_loss: 0.2593 - val_accuracy: 0.9311 - lr: 0.0047 - momentum: 0.9037\n",
      "Epoch 587/588\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1565 - accuracy: 0.9629 - val_loss: 0.3165 - val_accuracy: 0.9343 - lr: 0.0014 - momentum: 0.9367\n",
      "Epoch 588/588\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1175 - accuracy: 0.9751 - val_loss: 0.3327 - val_accuracy: 0.9295 - lr: 4.1040e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9295\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3327\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m336.90 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m283.22 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m53.68 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [98] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m99\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 588)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0102\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 589/594\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.2144 - accuracy: 0.9351 - val_loss: 0.2413 - val_accuracy: 0.9263 - lr: 0.0061 - momentum: 0.8915\n",
      "Epoch 590/594\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.2054 - accuracy: 0.9375 - val_loss: 0.6813 - val_accuracy: 0.8446 - lr: 0.0101 - momentum: 0.8506\n",
      "Epoch 591/594\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1723 - accuracy: 0.9551 - val_loss: 0.4419 - val_accuracy: 0.8958 - lr: 0.0083 - momentum: 0.8688\n",
      "Epoch 592/594\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1436 - accuracy: 0.9658 - val_loss: 0.4394 - val_accuracy: 0.9087 - lr: 0.0047 - momentum: 0.9037\n",
      "Epoch 593/594\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.1841 - accuracy: 0.9478 - val_loss: 0.2847 - val_accuracy: 0.9071 - lr: 0.0014 - momentum: 0.9367\n",
      "Epoch 594/594\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.1491 - accuracy: 0.9604 - val_loss: 0.2885 - val_accuracy: 0.9183 - lr: 4.0800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9183\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2885\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m338.08 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m284.22 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m53.86 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [99] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m100\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 594)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01014\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 595/600\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.2367 - accuracy: 0.9297 - val_loss: 0.3142 - val_accuracy: 0.9167 - lr: 0.0061 - momentum: 0.8915\n",
      "Epoch 596/600\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.2108 - accuracy: 0.9380 - val_loss: 0.3346 - val_accuracy: 0.9279 - lr: 0.0101 - momentum: 0.8506\n",
      "Epoch 597/600\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.1840 - accuracy: 0.9536 - val_loss: 0.2564 - val_accuracy: 0.9199 - lr: 0.0082 - momentum: 0.8688\n",
      "Epoch 598/600\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1475 - accuracy: 0.9644 - val_loss: 0.2859 - val_accuracy: 0.9343 - lr: 0.0047 - momentum: 0.9037\n",
      "Epoch 599/600\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1354 - accuracy: 0.9688 - val_loss: 0.2552 - val_accuracy: 0.9327 - lr: 0.0014 - momentum: 0.9367\n",
      "Epoch 600/600\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.1124 - accuracy: 0.9771 - val_loss: 0.2574 - val_accuracy: 0.9279 - lr: 4.0560e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9279\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2574\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m338.89 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m284.02 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m54.87 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [100] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m101\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 600)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01008\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 601/606\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.1830 - accuracy: 0.9463 - val_loss: 0.3112 - val_accuracy: 0.9359 - lr: 0.0061 - momentum: 0.8915\n",
      "Epoch 602/606\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.3670 - accuracy: 0.8677 - val_loss: 0.4187 - val_accuracy: 0.8702 - lr: 0.0100 - momentum: 0.8506\n",
      "Epoch 603/606\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.2610 - accuracy: 0.9189 - val_loss: 0.2511 - val_accuracy: 0.9183 - lr: 0.0082 - momentum: 0.8688\n",
      "Epoch 604/606\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.2041 - accuracy: 0.9419 - val_loss: 0.2197 - val_accuracy: 0.9279 - lr: 0.0047 - momentum: 0.9037\n",
      "Epoch 605/606\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1508 - accuracy: 0.9624 - val_loss: 0.3026 - val_accuracy: 0.9135 - lr: 0.0013 - momentum: 0.9367\n",
      "Epoch 606/606\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.1358 - accuracy: 0.9619 - val_loss: 0.2853 - val_accuracy: 0.9135 - lr: 4.0320e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9135\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2853\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m339.30 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m284.24 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m55.06 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [101] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m102\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 606)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.01002\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 607/612\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.2251 - accuracy: 0.9243 - val_loss: 0.3275 - val_accuracy: 0.9183 - lr: 0.0060 - momentum: 0.8915\n",
      "Epoch 608/612\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.1922 - accuracy: 0.9419 - val_loss: 0.2651 - val_accuracy: 0.9199 - lr: 0.0100 - momentum: 0.8506\n",
      "Epoch 609/612\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.2009 - accuracy: 0.9463 - val_loss: 0.3290 - val_accuracy: 0.9167 - lr: 0.0081 - momentum: 0.8688\n",
      "Epoch 610/612\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.1772 - accuracy: 0.9561 - val_loss: 0.2485 - val_accuracy: 0.9215 - lr: 0.0046 - momentum: 0.9037\n",
      "Epoch 611/612\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.1503 - accuracy: 0.9648 - val_loss: 0.2716 - val_accuracy: 0.9295 - lr: 0.0013 - momentum: 0.9367\n",
      "Epoch 612/612\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.1122 - accuracy: 0.9736 - val_loss: 0.2813 - val_accuracy: 0.9231 - lr: 4.0080e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9215\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2813\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m340.10 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m283.82 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m56.28 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [102] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m103\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 612)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00996\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 613/618\n",
      "256/256 [==============================] - 51s 184ms/step - loss: 0.1833 - accuracy: 0.9429 - val_loss: 0.2361 - val_accuracy: 0.9199 - lr: 0.0060 - momentum: 0.8915\n",
      "Epoch 614/618\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.2008 - accuracy: 0.9365 - val_loss: 0.2739 - val_accuracy: 0.9215 - lr: 0.0099 - momentum: 0.8506\n",
      "Epoch 615/618\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.1891 - accuracy: 0.9556 - val_loss: 0.2914 - val_accuracy: 0.9215 - lr: 0.0081 - momentum: 0.8688\n",
      "Epoch 616/618\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.2023 - accuracy: 0.9424 - val_loss: 0.4254 - val_accuracy: 0.8990 - lr: 0.0046 - momentum: 0.9037\n",
      "Epoch 617/618\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1599 - accuracy: 0.9634 - val_loss: 0.3748 - val_accuracy: 0.9135 - lr: 0.0013 - momentum: 0.9367\n",
      "Epoch 618/618\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.1402 - accuracy: 0.9629 - val_loss: 0.2984 - val_accuracy: 0.9247 - lr: 3.9840e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9247\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2983\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m340.30 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m284.35 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m55.96 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [103] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m104\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 618)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0099\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 619/624\n",
      "256/256 [==============================] - 51s 184ms/step - loss: 0.2028 - accuracy: 0.9375 - val_loss: 0.3109 - val_accuracy: 0.9311 - lr: 0.0060 - momentum: 0.8915\n",
      "Epoch 620/624\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.1986 - accuracy: 0.9390 - val_loss: 0.3335 - val_accuracy: 0.9311 - lr: 0.0098 - momentum: 0.8506\n",
      "Epoch 621/624\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1618 - accuracy: 0.9521 - val_loss: 0.2746 - val_accuracy: 0.9343 - lr: 0.0080 - momentum: 0.8688\n",
      "Epoch 622/624\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.1339 - accuracy: 0.9639 - val_loss: 0.2910 - val_accuracy: 0.9423 - lr: 0.0046 - momentum: 0.9037\n",
      "Epoch 623/624\n",
      "256/256 [==============================] - 46s 180ms/step - loss: 0.1103 - accuracy: 0.9736 - val_loss: 0.3011 - val_accuracy: 0.9327 - lr: 0.0013 - momentum: 0.9367\n",
      "Epoch 624/624\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.1004 - accuracy: 0.9727 - val_loss: 0.2698 - val_accuracy: 0.9423 - lr: 3.9600e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9423\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2698\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m340.71 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m284.80 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m55.91 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [104] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m105\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 624)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00984\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 625/630\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.1806 - accuracy: 0.9463 - val_loss: 0.3013 - val_accuracy: 0.9327 - lr: 0.0059 - momentum: 0.8915\n",
      "Epoch 626/630\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.1939 - accuracy: 0.9399 - val_loss: 0.3433 - val_accuracy: 0.9359 - lr: 0.0098 - momentum: 0.8506\n",
      "Epoch 627/630\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1779 - accuracy: 0.9580 - val_loss: 0.2583 - val_accuracy: 0.9375 - lr: 0.0080 - momentum: 0.8688\n",
      "Epoch 628/630\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.1552 - accuracy: 0.9614 - val_loss: 0.2010 - val_accuracy: 0.9423 - lr: 0.0046 - momentum: 0.9037\n",
      "Epoch 629/630\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.1256 - accuracy: 0.9731 - val_loss: 0.2472 - val_accuracy: 0.9407 - lr: 0.0013 - momentum: 0.9367\n",
      "Epoch 630/630\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1038 - accuracy: 0.9736 - val_loss: 0.2743 - val_accuracy: 0.9343 - lr: 3.9360e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9343\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2743\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m341.65 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m285.25 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m56.39 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [105] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m106\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 630)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00978\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 631/636\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.1986 - accuracy: 0.9355 - val_loss: 0.3703 - val_accuracy: 0.9279 - lr: 0.0059 - momentum: 0.8915\n",
      "Epoch 632/636\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.2135 - accuracy: 0.9341 - val_loss: 0.3344 - val_accuracy: 0.9263 - lr: 0.0097 - momentum: 0.8506\n",
      "Epoch 633/636\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.1732 - accuracy: 0.9536 - val_loss: 0.3282 - val_accuracy: 0.9295 - lr: 0.0079 - momentum: 0.8688\n",
      "Epoch 634/636\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.1595 - accuracy: 0.9546 - val_loss: 0.2195 - val_accuracy: 0.9423 - lr: 0.0045 - momentum: 0.9037\n",
      "Epoch 635/636\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.1178 - accuracy: 0.9712 - val_loss: 0.3095 - val_accuracy: 0.9279 - lr: 0.0013 - momentum: 0.9367\n",
      "Epoch 636/636\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.0843 - accuracy: 0.9805 - val_loss: 0.2921 - val_accuracy: 0.9343 - lr: 3.9120e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9343\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2922\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m341.52 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m284.79 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m56.73 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [106] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m107\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 636)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00972\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 637/642\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.2098 - accuracy: 0.9419 - val_loss: 0.2439 - val_accuracy: 0.9359 - lr: 0.0059 - momentum: 0.8915\n",
      "Epoch 638/642\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.1870 - accuracy: 0.9443 - val_loss: 0.1919 - val_accuracy: 0.9455 - lr: 0.0097 - momentum: 0.8506\n",
      "Epoch 639/642\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.2163 - accuracy: 0.9434 - val_loss: 0.3369 - val_accuracy: 0.9343 - lr: 0.0079 - momentum: 0.8688\n",
      "Epoch 640/642\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1658 - accuracy: 0.9595 - val_loss: 0.3353 - val_accuracy: 0.9199 - lr: 0.0045 - momentum: 0.9037\n",
      "Epoch 641/642\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.1276 - accuracy: 0.9683 - val_loss: 0.3116 - val_accuracy: 0.9375 - lr: 0.0013 - momentum: 0.9367\n",
      "Epoch 642/642\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.0968 - accuracy: 0.9810 - val_loss: 0.2878 - val_accuracy: 0.9359 - lr: 3.8880e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9359\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2878\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m342.12 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m285.32 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m56.80 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [107] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m108\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 642)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33m└───Shuffling data...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00966\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 643/648\n",
      "256/256 [==============================] - 51s 185ms/step - loss: 0.1801 - accuracy: 0.9463 - val_loss: 0.1890 - val_accuracy: 0.9359 - lr: 0.0058 - momentum: 0.8915\n",
      "Epoch 644/648\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1856 - accuracy: 0.9507 - val_loss: 0.2684 - val_accuracy: 0.9359 - lr: 0.0096 - momentum: 0.8506\n",
      "Epoch 645/648\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.1530 - accuracy: 0.9663 - val_loss: 0.2430 - val_accuracy: 0.9311 - lr: 0.0078 - momentum: 0.8688\n",
      "Epoch 646/648\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1236 - accuracy: 0.9707 - val_loss: 0.3698 - val_accuracy: 0.9311 - lr: 0.0045 - momentum: 0.9037\n",
      "Epoch 647/648\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.0929 - accuracy: 0.9790 - val_loss: 0.3974 - val_accuracy: 0.9167 - lr: 0.0013 - momentum: 0.9367\n",
      "Epoch 648/648\n",
      "256/256 [==============================] - 47s 181ms/step - loss: 0.0803 - accuracy: 0.9814 - val_loss: 0.3989 - val_accuracy: 0.9343 - lr: 3.8640e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9343\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3989\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17603926360607147. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m343.31 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m285.08 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m58.22 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [108] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m109\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 648)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0096\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 649/654\n",
      "256/256 [==============================] - 51s 186ms/step - loss: 0.1925 - accuracy: 0.9478 - val_loss: 0.2649 - val_accuracy: 0.9375 - lr: 0.0058 - momentum: 0.8915\n",
      "Epoch 650/654\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1847 - accuracy: 0.9517 - val_loss: 0.2350 - val_accuracy: 0.9295 - lr: 0.0095 - momentum: 0.8506\n",
      "Epoch 651/654\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1808 - accuracy: 0.9531 - val_loss: 0.1877 - val_accuracy: 0.9471 - lr: 0.0078 - momentum: 0.8688\n",
      "Epoch 652/654\n",
      "256/256 [==============================] - 48s 185ms/step - loss: 0.1492 - accuracy: 0.9639 - val_loss: 0.1736 - val_accuracy: 0.9519 - lr: 0.0044 - momentum: 0.9037\n",
      "Epoch 653/654\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1449 - accuracy: 0.9619 - val_loss: 0.2009 - val_accuracy: 0.9295 - lr: 0.0013 - momentum: 0.9367\n",
      "Epoch 654/654\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1233 - accuracy: 0.9648 - val_loss: 0.2104 - val_accuracy: 0.9183 - lr: 3.8400e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-652-0.9519.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9519\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.1736\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model loss from \u001b[0m\u001b[0;32m0.17603926360607147 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.17357933521270752\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m346.39 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m286.87 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m59.52 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [109] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m110\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 654)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00954\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 655/660\n",
      "256/256 [==============================] - 52s 189ms/step - loss: 0.2077 - accuracy: 0.9434 - val_loss: 0.2075 - val_accuracy: 0.9455 - lr: 0.0057 - momentum: 0.8915\n",
      "Epoch 656/660\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.2784 - accuracy: 0.9106 - val_loss: 0.1934 - val_accuracy: 0.9375 - lr: 0.0095 - momentum: 0.8506\n",
      "Epoch 657/660\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1754 - accuracy: 0.9463 - val_loss: 0.1810 - val_accuracy: 0.9439 - lr: 0.0077 - momentum: 0.8688\n",
      "Epoch 658/660\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1393 - accuracy: 0.9590 - val_loss: 0.2485 - val_accuracy: 0.9359 - lr: 0.0044 - momentum: 0.9037\n",
      "Epoch 659/660\n",
      "256/256 [==============================] - 48s 186ms/step - loss: 0.1253 - accuracy: 0.9663 - val_loss: 0.2008 - val_accuracy: 0.9503 - lr: 0.0013 - momentum: 0.9367\n",
      "Epoch 660/660\n",
      "256/256 [==============================] - 47s 183ms/step - loss: 0.1082 - accuracy: 0.9707 - val_loss: 0.2412 - val_accuracy: 0.9343 - lr: 3.8160e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9343\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2412\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m346.15 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m288.55 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m57.61 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [110] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m111\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 660)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00948\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 661/666\n",
      "256/256 [==============================] - 52s 186ms/step - loss: 0.1970 - accuracy: 0.9434 - val_loss: 0.2500 - val_accuracy: 0.9135 - lr: 0.0057 - momentum: 0.8915\n",
      "Epoch 662/666\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1853 - accuracy: 0.9438 - val_loss: 0.3575 - val_accuracy: 0.9054 - lr: 0.0094 - momentum: 0.8506\n",
      "Epoch 663/666\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.2017 - accuracy: 0.9419 - val_loss: 0.2965 - val_accuracy: 0.9311 - lr: 0.0077 - momentum: 0.8688\n",
      "Epoch 664/666\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1682 - accuracy: 0.9619 - val_loss: 0.3504 - val_accuracy: 0.9071 - lr: 0.0044 - momentum: 0.9037\n",
      "Epoch 665/666\n",
      "256/256 [==============================] - 47s 183ms/step - loss: 0.1360 - accuracy: 0.9634 - val_loss: 0.3270 - val_accuracy: 0.9215 - lr: 0.0013 - momentum: 0.9367\n",
      "Epoch 666/666\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1031 - accuracy: 0.9771 - val_loss: 0.3206 - val_accuracy: 0.9279 - lr: 3.7920e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9279\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3206\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m346.50 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m286.51 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m59.99 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [111] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m112\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 666)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00942\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 667/672\n",
      "256/256 [==============================] - 52s 187ms/step - loss: 0.2001 - accuracy: 0.9443 - val_loss: 0.2240 - val_accuracy: 0.9407 - lr: 0.0057 - momentum: 0.8915\n",
      "Epoch 668/672\n",
      "256/256 [==============================] - 48s 185ms/step - loss: 0.1905 - accuracy: 0.9531 - val_loss: 0.1934 - val_accuracy: 0.9535 - lr: 0.0094 - momentum: 0.8506\n",
      "Epoch 669/672\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1603 - accuracy: 0.9541 - val_loss: 0.2721 - val_accuracy: 0.9375 - lr: 0.0076 - momentum: 0.8688\n",
      "Epoch 670/672\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1281 - accuracy: 0.9673 - val_loss: 0.2933 - val_accuracy: 0.9295 - lr: 0.0044 - momentum: 0.9037\n",
      "Epoch 671/672\n",
      "256/256 [==============================] - 47s 183ms/step - loss: 0.1257 - accuracy: 0.9678 - val_loss: 0.2563 - val_accuracy: 0.9327 - lr: 0.0013 - momentum: 0.9367\n",
      "Epoch 672/672\n",
      "256/256 [==============================] - 47s 183ms/step - loss: 0.0931 - accuracy: 0.9790 - val_loss: 0.2896 - val_accuracy: 0.9327 - lr: 3.7680e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9327\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2896\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m345.11 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m287.78 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m57.33 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [112] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m113\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 672)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00936\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 673/678\n",
      "256/256 [==============================] - 52s 186ms/step - loss: 0.1816 - accuracy: 0.9497 - val_loss: 0.2687 - val_accuracy: 0.9375 - lr: 0.0056 - momentum: 0.8915\n",
      "Epoch 674/678\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1864 - accuracy: 0.9448 - val_loss: 0.2001 - val_accuracy: 0.9359 - lr: 0.0093 - momentum: 0.8506\n",
      "Epoch 675/678\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1929 - accuracy: 0.9448 - val_loss: 0.2183 - val_accuracy: 0.9503 - lr: 0.0076 - momentum: 0.8688\n",
      "Epoch 676/678\n",
      "256/256 [==============================] - 47s 183ms/step - loss: 0.1564 - accuracy: 0.9639 - val_loss: 0.2089 - val_accuracy: 0.9487 - lr: 0.0043 - momentum: 0.9037\n",
      "Epoch 677/678\n",
      "256/256 [==============================] - 47s 183ms/step - loss: 0.1309 - accuracy: 0.9658 - val_loss: 0.1883 - val_accuracy: 0.9455 - lr: 0.0012 - momentum: 0.9367\n",
      "Epoch 678/678\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1132 - accuracy: 0.9746 - val_loss: 0.2309 - val_accuracy: 0.9343 - lr: 3.7440e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9343\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2309\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m345.87 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m287.05 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m58.82 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [113] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m114\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 678)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0093\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 679/684\n",
      "256/256 [==============================] - 52s 186ms/step - loss: 0.2029 - accuracy: 0.9429 - val_loss: 1004.5974 - val_accuracy: 0.6250 - lr: 0.0056 - momentum: 0.8915\n",
      "Epoch 680/684\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.2141 - accuracy: 0.9326 - val_loss: 0.3285 - val_accuracy: 0.9343 - lr: 0.0092 - momentum: 0.8506\n",
      "Epoch 681/684\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1780 - accuracy: 0.9585 - val_loss: 0.4111 - val_accuracy: 0.8942 - lr: 0.0075 - momentum: 0.8688\n",
      "Epoch 682/684\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1443 - accuracy: 0.9648 - val_loss: 0.2738 - val_accuracy: 0.9343 - lr: 0.0043 - momentum: 0.9037\n",
      "Epoch 683/684\n",
      "256/256 [==============================] - 47s 183ms/step - loss: 0.1348 - accuracy: 0.9658 - val_loss: 0.2881 - val_accuracy: 0.9199 - lr: 0.0012 - momentum: 0.9367\n",
      "Epoch 684/684\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1383 - accuracy: 0.9629 - val_loss: 0.3661 - val_accuracy: 0.9071 - lr: 3.7200e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9071\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3661\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m345.52 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m286.82 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m58.70 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [114] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m115\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 684)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00924\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 685/690\n",
      "256/256 [==============================] - 52s 187ms/step - loss: 0.1926 - accuracy: 0.9409 - val_loss: 0.4415 - val_accuracy: 0.9038 - lr: 0.0056 - momentum: 0.8915\n",
      "Epoch 686/690\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1677 - accuracy: 0.9531 - val_loss: 0.2239 - val_accuracy: 0.9343 - lr: 0.0092 - momentum: 0.8506\n",
      "Epoch 687/690\n",
      "256/256 [==============================] - 47s 183ms/step - loss: 0.1899 - accuracy: 0.9478 - val_loss: 0.2276 - val_accuracy: 0.9311 - lr: 0.0075 - momentum: 0.8688\n",
      "Epoch 688/690\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1573 - accuracy: 0.9590 - val_loss: 0.2471 - val_accuracy: 0.9215 - lr: 0.0043 - momentum: 0.9037\n",
      "Epoch 689/690\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.1212 - accuracy: 0.9702 - val_loss: 0.2242 - val_accuracy: 0.9311 - lr: 0.0012 - momentum: 0.9367\n",
      "Epoch 690/690\n",
      "256/256 [==============================] - 47s 183ms/step - loss: 0.1065 - accuracy: 0.9731 - val_loss: 0.2341 - val_accuracy: 0.9311 - lr: 3.6960e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9311\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2341\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m345.10 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m287.11 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m57.98 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [115] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m116\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 690)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00918\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 691/696\n",
      "256/256 [==============================] - 60s 212ms/step - loss: 0.1553 - accuracy: 0.9570 - val_loss: 0.3812 - val_accuracy: 0.9167 - lr: 0.0055 - momentum: 0.8915\n",
      "Epoch 692/696\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.1734 - accuracy: 0.9541 - val_loss: 0.1841 - val_accuracy: 0.9455 - lr: 0.0091 - momentum: 0.8506\n",
      "Epoch 693/696\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.1747 - accuracy: 0.9482 - val_loss: 0.1765 - val_accuracy: 0.9503 - lr: 0.0075 - momentum: 0.8688\n",
      "Epoch 694/696\n",
      "256/256 [==============================] - 53s 207ms/step - loss: 0.1481 - accuracy: 0.9590 - val_loss: 0.2868 - val_accuracy: 0.9311 - lr: 0.0042 - momentum: 0.9037\n",
      "Epoch 695/696\n",
      "256/256 [==============================] - 53s 205ms/step - loss: 0.1279 - accuracy: 0.9707 - val_loss: 0.2949 - val_accuracy: 0.9327 - lr: 0.0012 - momentum: 0.9367\n",
      "Epoch 696/696\n",
      "256/256 [==============================] - 53s 205ms/step - loss: 0.1000 - accuracy: 0.9746 - val_loss: 0.2569 - val_accuracy: 0.9487 - lr: 3.6720e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9487\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2569\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m395.56 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m325.05 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m70.51 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [116] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m117\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 696)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00912\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 697/702\n",
      "256/256 [==============================] - 59s 210ms/step - loss: 0.1846 - accuracy: 0.9502 - val_loss: 0.1894 - val_accuracy: 0.9519 - lr: 0.0055 - momentum: 0.8915\n",
      "Epoch 698/702\n",
      "256/256 [==============================] - 53s 205ms/step - loss: 0.1809 - accuracy: 0.9497 - val_loss: 0.2153 - val_accuracy: 0.9423 - lr: 0.0091 - momentum: 0.8506\n",
      "Epoch 699/702\n",
      "256/256 [==============================] - 53s 208ms/step - loss: 0.1674 - accuracy: 0.9624 - val_loss: 0.2563 - val_accuracy: 0.9471 - lr: 0.0074 - momentum: 0.8688\n",
      "Epoch 700/702\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.1207 - accuracy: 0.9731 - val_loss: 0.2883 - val_accuracy: 0.9359 - lr: 0.0042 - momentum: 0.9037\n",
      "Epoch 701/702\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.1315 - accuracy: 0.9707 - val_loss: 0.2695 - val_accuracy: 0.9343 - lr: 0.0012 - momentum: 0.9367\n",
      "Epoch 702/702\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.0929 - accuracy: 0.9819 - val_loss: 0.2316 - val_accuracy: 0.9487 - lr: 3.6480e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9487\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2316\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m397.21 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m323.14 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m74.07 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [117] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m118\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 702)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00906\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 703/708\n",
      "256/256 [==============================] - 59s 212ms/step - loss: 0.1751 - accuracy: 0.9478 - val_loss: 0.4427 - val_accuracy: 0.8494 - lr: 0.0055 - momentum: 0.8915\n",
      "Epoch 704/708\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.2010 - accuracy: 0.9326 - val_loss: 0.2286 - val_accuracy: 0.9439 - lr: 0.0090 - momentum: 0.8506\n",
      "Epoch 705/708\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.1860 - accuracy: 0.9443 - val_loss: 0.2116 - val_accuracy: 0.9423 - lr: 0.0074 - momentum: 0.8688\n",
      "Epoch 706/708\n",
      "256/256 [==============================] - 53s 207ms/step - loss: 0.1386 - accuracy: 0.9614 - val_loss: 0.2094 - val_accuracy: 0.9407 - lr: 0.0042 - momentum: 0.9037\n",
      "Epoch 707/708\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.1317 - accuracy: 0.9600 - val_loss: 0.2283 - val_accuracy: 0.9455 - lr: 0.0012 - momentum: 0.9367\n",
      "Epoch 708/708\n",
      "256/256 [==============================] - 53s 205ms/step - loss: 0.1034 - accuracy: 0.9746 - val_loss: 0.2400 - val_accuracy: 0.9455 - lr: 3.6240e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9455\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2400\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m401.07 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m325.12 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m75.95 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [118] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m119\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 708)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.009\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 709/714\n",
      "256/256 [==============================] - 59s 212ms/step - loss: 0.1538 - accuracy: 0.9619 - val_loss: 0.2188 - val_accuracy: 0.9359 - lr: 0.0054 - momentum: 0.8915\n",
      "Epoch 710/714\n",
      "256/256 [==============================] - 53s 207ms/step - loss: 0.2128 - accuracy: 0.9355 - val_loss: 0.2977 - val_accuracy: 0.9135 - lr: 0.0089 - momentum: 0.8506\n",
      "Epoch 711/714\n",
      "256/256 [==============================] - 53s 207ms/step - loss: 0.2005 - accuracy: 0.9409 - val_loss: 0.1948 - val_accuracy: 0.9407 - lr: 0.0073 - momentum: 0.8688\n",
      "Epoch 712/714\n",
      "256/256 [==============================] - 53s 204ms/step - loss: 0.1602 - accuracy: 0.9561 - val_loss: 0.2424 - val_accuracy: 0.9407 - lr: 0.0042 - momentum: 0.9037\n",
      "Epoch 713/714\n",
      "256/256 [==============================] - 53s 208ms/step - loss: 0.1633 - accuracy: 0.9546 - val_loss: 0.3531 - val_accuracy: 0.9119 - lr: 0.0012 - momentum: 0.9367\n",
      "Epoch 714/714\n",
      "256/256 [==============================] - 53s 207ms/step - loss: 0.1262 - accuracy: 0.9648 - val_loss: 0.3895 - val_accuracy: 0.9087 - lr: 3.6000e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9087\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3879\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m399.51 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m325.86 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m73.65 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [119] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m120\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 714)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00894\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 715/720\n",
      "256/256 [==============================] - 60s 213ms/step - loss: 0.2853 - accuracy: 0.8921 - val_loss: 44.1765 - val_accuracy: 0.4968 - lr: 0.0054 - momentum: 0.8915\n",
      "Epoch 716/720\n",
      "256/256 [==============================] - 53s 207ms/step - loss: 0.4900 - accuracy: 0.7964 - val_loss: 0.3934 - val_accuracy: 0.8510 - lr: 0.0089 - momentum: 0.8506\n",
      "Epoch 717/720\n",
      "256/256 [==============================] - 53s 207ms/step - loss: 0.3208 - accuracy: 0.8809 - val_loss: 0.4923 - val_accuracy: 0.8077 - lr: 0.0073 - momentum: 0.8688\n",
      "Epoch 718/720\n",
      "256/256 [==============================] - 53s 204ms/step - loss: 0.2574 - accuracy: 0.9146 - val_loss: 0.2903 - val_accuracy: 0.9087 - lr: 0.0041 - momentum: 0.9037\n",
      "Epoch 719/720\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.2284 - accuracy: 0.9263 - val_loss: 0.2491 - val_accuracy: 0.9247 - lr: 0.0012 - momentum: 0.9367\n",
      "Epoch 720/720\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.1839 - accuracy: 0.9434 - val_loss: 0.2490 - val_accuracy: 0.9215 - lr: 3.5760e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9215\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2490\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m401.69 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m325.27 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m76.42 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [120] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m121\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 720)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00888\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 721/726\n",
      "256/256 [==============================] - 60s 213ms/step - loss: 0.2467 - accuracy: 0.9219 - val_loss: 0.2657 - val_accuracy: 0.9167 - lr: 0.0053 - momentum: 0.8915\n",
      "Epoch 722/726\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.2420 - accuracy: 0.9209 - val_loss: 0.5724 - val_accuracy: 0.7051 - lr: 0.0088 - momentum: 0.8506\n",
      "Epoch 723/726\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.2383 - accuracy: 0.9258 - val_loss: 0.2247 - val_accuracy: 0.9215 - lr: 0.0072 - momentum: 0.8688\n",
      "Epoch 724/726\n",
      "256/256 [==============================] - 53s 207ms/step - loss: 0.1874 - accuracy: 0.9458 - val_loss: 0.2328 - val_accuracy: 0.9295 - lr: 0.0041 - momentum: 0.9037\n",
      "Epoch 725/726\n",
      "256/256 [==============================] - 53s 207ms/step - loss: 0.1532 - accuracy: 0.9512 - val_loss: 0.2350 - val_accuracy: 0.9247 - lr: 0.0012 - momentum: 0.9367\n",
      "Epoch 726/726\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.1403 - accuracy: 0.9590 - val_loss: 0.2735 - val_accuracy: 0.9215 - lr: 3.5520e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9215\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2735\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m400.42 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m325.23 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m75.19 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [121] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m122\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 726)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00882\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 727/732\n",
      "256/256 [==============================] - 60s 215ms/step - loss: 0.2011 - accuracy: 0.9351 - val_loss: 0.2269 - val_accuracy: 0.9167 - lr: 0.0053 - momentum: 0.8915\n",
      "Epoch 728/732\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.2072 - accuracy: 0.9307 - val_loss: 0.2523 - val_accuracy: 0.9343 - lr: 0.0088 - momentum: 0.8506\n",
      "Epoch 729/732\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.4140 - accuracy: 0.8306 - val_loss: 0.2999 - val_accuracy: 0.8910 - lr: 0.0072 - momentum: 0.8688\n",
      "Epoch 730/732\n",
      "256/256 [==============================] - 53s 204ms/step - loss: 0.2811 - accuracy: 0.9058 - val_loss: 0.2931 - val_accuracy: 0.9231 - lr: 0.0041 - momentum: 0.9037\n",
      "Epoch 731/732\n",
      "256/256 [==============================] - 53s 204ms/step - loss: 0.1958 - accuracy: 0.9380 - val_loss: 0.2793 - val_accuracy: 0.9135 - lr: 0.0012 - momentum: 0.9367\n",
      "Epoch 732/732\n",
      "256/256 [==============================] - 53s 204ms/step - loss: 0.1798 - accuracy: 0.9507 - val_loss: 0.2849 - val_accuracy: 0.9247 - lr: 3.5280e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9247\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2849\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m400.06 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m324.16 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m75.90 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [122] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m123\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 732)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00876\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 733/738\n",
      "256/256 [==============================] - 60s 214ms/step - loss: 0.2367 - accuracy: 0.9277 - val_loss: 0.3106 - val_accuracy: 0.9215 - lr: 0.0053 - momentum: 0.8915\n",
      "Epoch 734/738\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.2412 - accuracy: 0.9282 - val_loss: 0.2260 - val_accuracy: 0.9327 - lr: 0.0087 - momentum: 0.8506\n",
      "Epoch 735/738\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.2291 - accuracy: 0.9307 - val_loss: 0.2937 - val_accuracy: 0.9022 - lr: 0.0071 - momentum: 0.8688\n",
      "Epoch 736/738\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.2225 - accuracy: 0.9292 - val_loss: 0.2502 - val_accuracy: 0.9231 - lr: 0.0041 - momentum: 0.9037\n",
      "Epoch 737/738\n",
      "256/256 [==============================] - 52s 203ms/step - loss: 0.2293 - accuracy: 0.9238 - val_loss: 0.1966 - val_accuracy: 0.9247 - lr: 0.0012 - momentum: 0.9367\n",
      "Epoch 738/738\n",
      "256/256 [==============================] - 48s 188ms/step - loss: 0.2015 - accuracy: 0.9307 - val_loss: 0.2094 - val_accuracy: 0.9247 - lr: 3.5040e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9247\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2095\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m393.92 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m317.95 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m75.97 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [123] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m124\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 738)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0087\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 739/744\n",
      "256/256 [==============================] - 55s 195ms/step - loss: 0.2290 - accuracy: 0.9224 - val_loss: 0.2004 - val_accuracy: 0.9295 - lr: 0.0052 - momentum: 0.8915\n",
      "Epoch 740/744\n",
      "256/256 [==============================] - 48s 188ms/step - loss: 0.2007 - accuracy: 0.9385 - val_loss: 0.2318 - val_accuracy: 0.9295 - lr: 0.0087 - momentum: 0.8506\n",
      "Epoch 741/744\n",
      "256/256 [==============================] - 49s 189ms/step - loss: 0.1886 - accuracy: 0.9404 - val_loss: 0.2851 - val_accuracy: 0.9247 - lr: 0.0071 - momentum: 0.8688\n",
      "Epoch 742/744\n",
      "256/256 [==============================] - 48s 189ms/step - loss: 0.1909 - accuracy: 0.9404 - val_loss: 0.2319 - val_accuracy: 0.9231 - lr: 0.0040 - momentum: 0.9037\n",
      "Epoch 743/744\n",
      "256/256 [==============================] - 49s 190ms/step - loss: 0.1498 - accuracy: 0.9536 - val_loss: 0.2112 - val_accuracy: 0.9247 - lr: 0.0012 - momentum: 0.9367\n",
      "Epoch 744/744\n",
      "256/256 [==============================] - 53s 205ms/step - loss: 0.1260 - accuracy: 0.9624 - val_loss: 0.2279 - val_accuracy: 0.9247 - lr: 3.4800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9247\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2279\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m373.91 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m302.21 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m71.70 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [124] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m125\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 744)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00864\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 745/750\n",
      "256/256 [==============================] - 59s 212ms/step - loss: 0.2364 - accuracy: 0.9268 - val_loss: 0.1946 - val_accuracy: 0.9295 - lr: 0.0052 - momentum: 0.8915\n",
      "Epoch 746/750\n",
      "256/256 [==============================] - 52s 203ms/step - loss: 0.2108 - accuracy: 0.9292 - val_loss: 0.1880 - val_accuracy: 0.9423 - lr: 0.0086 - momentum: 0.8506\n",
      "Epoch 747/750\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.1923 - accuracy: 0.9429 - val_loss: 0.2489 - val_accuracy: 0.9311 - lr: 0.0070 - momentum: 0.8688\n",
      "Epoch 748/750\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.1763 - accuracy: 0.9443 - val_loss: 0.2070 - val_accuracy: 0.9359 - lr: 0.0040 - momentum: 0.9037\n",
      "Epoch 749/750\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1504 - accuracy: 0.9595 - val_loss: 0.2190 - val_accuracy: 0.9295 - lr: 0.0012 - momentum: 0.9367\n",
      "Epoch 750/750\n",
      "256/256 [==============================] - 51s 200ms/step - loss: 0.1180 - accuracy: 0.9673 - val_loss: 0.1815 - val_accuracy: 0.9375 - lr: 3.4560e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9375\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.1815\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m396.27 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m320.17 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m76.10 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [125] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m126\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 750)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33m└───Shuffling data...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00858\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 751/756\n",
      "256/256 [==============================] - 59s 211ms/step - loss: 0.1875 - accuracy: 0.9395 - val_loss: 0.1891 - val_accuracy: 0.9391 - lr: 0.0052 - momentum: 0.8915\n",
      "Epoch 752/756\n",
      "256/256 [==============================] - 54s 209ms/step - loss: 0.1770 - accuracy: 0.9521 - val_loss: 0.2006 - val_accuracy: 0.9391 - lr: 0.0085 - momentum: 0.8506\n",
      "Epoch 753/756\n",
      "256/256 [==============================] - 54s 209ms/step - loss: 0.1858 - accuracy: 0.9434 - val_loss: 0.3627 - val_accuracy: 0.9119 - lr: 0.0070 - momentum: 0.8688\n",
      "Epoch 754/756\n",
      "256/256 [==============================] - 54s 209ms/step - loss: 0.1482 - accuracy: 0.9648 - val_loss: 0.2054 - val_accuracy: 0.9279 - lr: 0.0040 - momentum: 0.9037\n",
      "Epoch 755/756\n",
      "256/256 [==============================] - 54s 210ms/step - loss: 0.1425 - accuracy: 0.9614 - val_loss: 0.2056 - val_accuracy: 0.9343 - lr: 0.0011 - momentum: 0.9367\n",
      "Epoch 756/756\n",
      "256/256 [==============================] - 54s 208ms/step - loss: 0.1070 - accuracy: 0.9736 - val_loss: 0.2019 - val_accuracy: 0.9407 - lr: 3.4320e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9407\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2019\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m408.38 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m328.23 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m80.15 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [126] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m127\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 756)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00852\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 757/762\n",
      "256/256 [==============================] - 60s 215ms/step - loss: 0.1892 - accuracy: 0.9414 - val_loss: 0.1697 - val_accuracy: 0.9439 - lr: 0.0051 - momentum: 0.8915\n",
      "Epoch 758/762\n",
      "256/256 [==============================] - 53s 205ms/step - loss: 0.1981 - accuracy: 0.9399 - val_loss: 0.2210 - val_accuracy: 0.9359 - lr: 0.0085 - momentum: 0.8506\n",
      "Epoch 759/762\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.1458 - accuracy: 0.9624 - val_loss: 0.2387 - val_accuracy: 0.9327 - lr: 0.0069 - momentum: 0.8688\n",
      "Epoch 760/762\n",
      "256/256 [==============================] - 53s 208ms/step - loss: 0.1451 - accuracy: 0.9604 - val_loss: 0.4134 - val_accuracy: 0.7933 - lr: 0.0039 - momentum: 0.9037\n",
      "Epoch 761/762\n",
      "256/256 [==============================] - 52s 200ms/step - loss: 0.1456 - accuracy: 0.9644 - val_loss: 0.1825 - val_accuracy: 0.9455 - lr: 0.0011 - momentum: 0.9367\n",
      "Epoch 762/762\n",
      "256/256 [==============================] - 49s 190ms/step - loss: 0.1163 - accuracy: 0.9712 - val_loss: 0.2020 - val_accuracy: 0.9423 - lr: 3.4080e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;31mERROR: Failed to load weights. Error: max() arg is an empty sequence\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9423\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2020\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m397.31 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m319.57 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m77.74 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [127] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m128\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 762)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;31m- Debug DP Sample dir: \u001b[0m\u001b[0;32mSamples/TSR_SUB_400_y2023_m12_d22-h10_m02_s37\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00846\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 763/768\n",
      "256/256 [==============================] - 59s 216ms/step - loss: 0.2035 - accuracy: 0.9404 - val_loss: 0.1979 - val_accuracy: 0.9343 - lr: 0.0051 - momentum: 0.8915\n",
      "Epoch 764/768\n",
      "256/256 [==============================] - 53s 205ms/step - loss: 0.1976 - accuracy: 0.9448 - val_loss: 0.1832 - val_accuracy: 0.9343 - lr: 0.0084 - momentum: 0.8506\n",
      "Epoch 765/768\n",
      "256/256 [==============================] - 53s 207ms/step - loss: 0.1549 - accuracy: 0.9556 - val_loss: 0.2422 - val_accuracy: 0.9327 - lr: 0.0069 - momentum: 0.8688\n",
      "Epoch 766/768\n",
      "256/256 [==============================] - 53s 205ms/step - loss: 0.1255 - accuracy: 0.9629 - val_loss: 0.2552 - val_accuracy: 0.9343 - lr: 0.0039 - momentum: 0.9037\n",
      "Epoch 767/768\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.1143 - accuracy: 0.9697 - val_loss: 0.2569 - val_accuracy: 0.9327 - lr: 0.0011 - momentum: 0.9367\n",
      "Epoch 768/768\n",
      "256/256 [==============================] - 53s 205ms/step - loss: 0.0877 - accuracy: 0.9805 - val_loss: 0.2370 - val_accuracy: 0.9295 - lr: 3.3840e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9295\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2370\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m412.87 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m323.62 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m89.24 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [128] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m129\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 768)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0084\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 769/774\n",
      "256/256 [==============================] - 60s 216ms/step - loss: 0.1933 - accuracy: 0.9355 - val_loss: 0.2169 - val_accuracy: 0.9359 - lr: 0.0051 - momentum: 0.8915\n",
      "Epoch 770/774\n",
      "256/256 [==============================] - 50s 196ms/step - loss: 0.2025 - accuracy: 0.9351 - val_loss: 0.2068 - val_accuracy: 0.9423 - lr: 0.0084 - momentum: 0.8506\n",
      "Epoch 771/774\n",
      "256/256 [==============================] - 48s 187ms/step - loss: 0.1770 - accuracy: 0.9531 - val_loss: 0.2168 - val_accuracy: 0.9247 - lr: 0.0068 - momentum: 0.8688\n",
      "Epoch 772/774\n",
      "256/256 [==============================] - 49s 190ms/step - loss: 0.3436 - accuracy: 0.8853 - val_loss: 0.3420 - val_accuracy: 0.8878 - lr: 0.0039 - momentum: 0.9037\n",
      "Epoch 773/774\n",
      "256/256 [==============================] - 48s 187ms/step - loss: 0.2669 - accuracy: 0.9131 - val_loss: 0.2616 - val_accuracy: 0.9183 - lr: 0.0011 - momentum: 0.9367\n",
      "Epoch 774/774\n",
      "256/256 [==============================] - 49s 190ms/step - loss: 0.2383 - accuracy: 0.9199 - val_loss: 0.2597 - val_accuracy: 0.9167 - lr: 3.3600e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9167\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2597\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m382.79 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m305.30 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m77.49 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [129] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m130\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 774)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00834\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 775/780\n",
      "256/256 [==============================] - 59s 210ms/step - loss: 0.2757 - accuracy: 0.9082 - val_loss: 0.2797 - val_accuracy: 0.9006 - lr: 0.0050 - momentum: 0.8915\n",
      "Epoch 776/780\n",
      "256/256 [==============================] - 53s 205ms/step - loss: 0.2456 - accuracy: 0.9180 - val_loss: 0.2568 - val_accuracy: 0.9231 - lr: 0.0083 - momentum: 0.8506\n",
      "Epoch 777/780\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.2160 - accuracy: 0.9365 - val_loss: 0.2810 - val_accuracy: 0.9151 - lr: 0.0068 - momentum: 0.8688\n",
      "Epoch 778/780\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.2165 - accuracy: 0.9390 - val_loss: 0.2546 - val_accuracy: 0.9279 - lr: 0.0039 - momentum: 0.9037\n",
      "Epoch 779/780\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.2193 - accuracy: 0.9268 - val_loss: 0.2000 - val_accuracy: 0.9359 - lr: 0.0011 - momentum: 0.9367\n",
      "Epoch 780/780\n",
      "256/256 [==============================] - 52s 203ms/step - loss: 0.2017 - accuracy: 0.9326 - val_loss: 0.2180 - val_accuracy: 0.9327 - lr: 3.3360e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9327\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2180\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m398.75 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m320.87 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m77.88 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [130] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m131\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 780)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00828\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 781/786\n",
      "256/256 [==============================] - 59s 213ms/step - loss: 0.2444 - accuracy: 0.9175 - val_loss: 0.2351 - val_accuracy: 0.9167 - lr: 0.0050 - momentum: 0.8915\n",
      "Epoch 782/786\n",
      "256/256 [==============================] - 54s 208ms/step - loss: 0.2478 - accuracy: 0.9160 - val_loss: 0.2355 - val_accuracy: 0.9471 - lr: 0.0082 - momentum: 0.8506\n",
      "Epoch 783/786\n",
      "256/256 [==============================] - 53s 205ms/step - loss: 0.2046 - accuracy: 0.9370 - val_loss: 0.2596 - val_accuracy: 0.9311 - lr: 0.0067 - momentum: 0.8688\n",
      "Epoch 784/786\n",
      "256/256 [==============================] - 54s 208ms/step - loss: 0.1665 - accuracy: 0.9526 - val_loss: 0.2696 - val_accuracy: 0.9215 - lr: 0.0038 - momentum: 0.9037\n",
      "Epoch 785/786\n",
      "256/256 [==============================] - 53s 207ms/step - loss: 0.1558 - accuracy: 0.9595 - val_loss: 0.2078 - val_accuracy: 0.9327 - lr: 0.0011 - momentum: 0.9367\n",
      "Epoch 786/786\n",
      "256/256 [==============================] - 49s 190ms/step - loss: 0.1371 - accuracy: 0.9609 - val_loss: 0.2095 - val_accuracy: 0.9359 - lr: 3.3120e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9359\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2095\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m397.98 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m322.15 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m75.83 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [131] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m132\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 786)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00822\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 787/792\n",
      "256/256 [==============================] - 54s 193ms/step - loss: 0.2220 - accuracy: 0.9248 - val_loss: 0.2208 - val_accuracy: 0.9343 - lr: 0.0049 - momentum: 0.8915\n",
      "Epoch 788/792\n",
      "256/256 [==============================] - 49s 190ms/step - loss: 0.2266 - accuracy: 0.9312 - val_loss: 0.2058 - val_accuracy: 0.9423 - lr: 0.0082 - momentum: 0.8506\n",
      "Epoch 789/792\n",
      "256/256 [==============================] - 49s 191ms/step - loss: 0.1998 - accuracy: 0.9409 - val_loss: 0.2647 - val_accuracy: 0.9423 - lr: 0.0067 - momentum: 0.8688\n",
      "Epoch 790/792\n",
      "256/256 [==============================] - 53s 208ms/step - loss: 0.1623 - accuracy: 0.9556 - val_loss: 0.2243 - val_accuracy: 0.9263 - lr: 0.0038 - momentum: 0.9037\n",
      "Epoch 791/792\n",
      "256/256 [==============================] - 53s 204ms/step - loss: 0.1516 - accuracy: 0.9526 - val_loss: 0.2844 - val_accuracy: 0.9263 - lr: 0.0011 - momentum: 0.9367\n",
      "Epoch 792/792\n",
      "256/256 [==============================] - 52s 203ms/step - loss: 0.1290 - accuracy: 0.9707 - val_loss: 0.2513 - val_accuracy: 0.9215 - lr: 3.2880e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9215\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2513\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m379.30 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m310.64 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m68.66 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [132] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m133\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 792)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00816\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 793/798\n",
      "256/256 [==============================] - 61s 216ms/step - loss: 0.2119 - accuracy: 0.9331 - val_loss: 0.3260 - val_accuracy: 0.8766 - lr: 0.0049 - momentum: 0.8915\n",
      "Epoch 794/798\n",
      "256/256 [==============================] - 53s 207ms/step - loss: 0.3403 - accuracy: 0.8740 - val_loss: 0.6583 - val_accuracy: 0.8253 - lr: 0.0081 - momentum: 0.8506\n",
      "Epoch 795/798\n",
      "256/256 [==============================] - 54s 208ms/step - loss: 0.2756 - accuracy: 0.9053 - val_loss: 0.4088 - val_accuracy: 0.8734 - lr: 0.0066 - momentum: 0.8688\n",
      "Epoch 796/798\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.2565 - accuracy: 0.9126 - val_loss: 0.3918 - val_accuracy: 0.8718 - lr: 0.0038 - momentum: 0.9037\n",
      "Epoch 797/798\n",
      "256/256 [==============================] - 53s 207ms/step - loss: 0.2101 - accuracy: 0.9331 - val_loss: 0.2888 - val_accuracy: 0.8926 - lr: 0.0011 - momentum: 0.9367\n",
      "Epoch 798/798\n",
      "256/256 [==============================] - 53s 207ms/step - loss: 0.2070 - accuracy: 0.9312 - val_loss: 0.4085 - val_accuracy: 0.8638 - lr: 3.2640e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.8638\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.4084\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m407.68 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m327.65 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m80.03 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [133] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m134\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 798)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0081\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 799/804\n",
      "256/256 [==============================] - 60s 216ms/step - loss: 0.2220 - accuracy: 0.9224 - val_loss: 0.3605 - val_accuracy: 0.8798 - lr: 0.0049 - momentum: 0.8915\n",
      "Epoch 800/804\n",
      "256/256 [==============================] - 53s 207ms/step - loss: 0.2107 - accuracy: 0.9326 - val_loss: 0.3003 - val_accuracy: 0.9151 - lr: 0.0081 - momentum: 0.8506\n",
      "Epoch 801/804\n",
      "256/256 [==============================] - 53s 207ms/step - loss: 0.1872 - accuracy: 0.9463 - val_loss: 0.3125 - val_accuracy: 0.9231 - lr: 0.0066 - momentum: 0.8688\n",
      "Epoch 802/804\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.1649 - accuracy: 0.9531 - val_loss: 0.3394 - val_accuracy: 0.8942 - lr: 0.0037 - momentum: 0.9037\n",
      "Epoch 803/804\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.1556 - accuracy: 0.9551 - val_loss: 0.3374 - val_accuracy: 0.9054 - lr: 0.0011 - momentum: 0.9367\n",
      "Epoch 804/804\n",
      "256/256 [==============================] - 53s 208ms/step - loss: 0.1234 - accuracy: 0.9688 - val_loss: 0.3288 - val_accuracy: 0.9199 - lr: 3.2400e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9199\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3288\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m409.81 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m326.56 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m83.25 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [134] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m135\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 804)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00804\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 805/810\n",
      "256/256 [==============================] - 60s 214ms/step - loss: 0.2366 - accuracy: 0.9248 - val_loss: 0.3447 - val_accuracy: 0.9215 - lr: 0.0048 - momentum: 0.8915\n",
      "Epoch 806/810\n",
      "256/256 [==============================] - 53s 205ms/step - loss: 0.2777 - accuracy: 0.8984 - val_loss: 0.2728 - val_accuracy: 0.9151 - lr: 0.0080 - momentum: 0.8506\n",
      "Epoch 807/810\n",
      "256/256 [==============================] - 53s 205ms/step - loss: 0.2414 - accuracy: 0.9204 - val_loss: 0.3262 - val_accuracy: 0.8958 - lr: 0.0065 - momentum: 0.8688\n",
      "Epoch 808/810\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.2167 - accuracy: 0.9307 - val_loss: 0.2234 - val_accuracy: 0.9263 - lr: 0.0037 - momentum: 0.9037\n",
      "Epoch 809/810\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.1971 - accuracy: 0.9326 - val_loss: 0.2884 - val_accuracy: 0.9054 - lr: 0.0011 - momentum: 0.9367\n",
      "Epoch 810/810\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.1471 - accuracy: 0.9546 - val_loss: 0.2752 - val_accuracy: 0.9119 - lr: 3.2160e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9119\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2752\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m407.36 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m324.62 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m82.73 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [135] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m136\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 810)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00798\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 811/816\n",
      "256/256 [==============================] - 61s 216ms/step - loss: 0.2347 - accuracy: 0.9282 - val_loss: 0.2873 - val_accuracy: 0.9071 - lr: 0.0048 - momentum: 0.8915\n",
      "Epoch 812/816\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.2065 - accuracy: 0.9375 - val_loss: 0.2347 - val_accuracy: 0.9183 - lr: 0.0079 - momentum: 0.8506\n",
      "Epoch 813/816\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.2057 - accuracy: 0.9370 - val_loss: 0.2539 - val_accuracy: 0.9343 - lr: 0.0065 - momentum: 0.8688\n",
      "Epoch 814/816\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.2019 - accuracy: 0.9434 - val_loss: 0.3075 - val_accuracy: 0.9407 - lr: 0.0037 - momentum: 0.9037\n",
      "Epoch 815/816\n",
      "256/256 [==============================] - 52s 203ms/step - loss: 0.1648 - accuracy: 0.9531 - val_loss: 0.2347 - val_accuracy: 0.9215 - lr: 0.0011 - momentum: 0.9367\n",
      "Epoch 816/816\n",
      "256/256 [==============================] - 52s 203ms/step - loss: 0.1220 - accuracy: 0.9692 - val_loss: 0.2458 - val_accuracy: 0.9247 - lr: 3.1920e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9247\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2458\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m406.44 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m323.27 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m83.17 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [136] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m137\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 816)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00792\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 817/822\n",
      "256/256 [==============================] - 60s 213ms/step - loss: 0.2104 - accuracy: 0.9326 - val_loss: 0.2660 - val_accuracy: 0.9263 - lr: 0.0048 - momentum: 0.8915\n",
      "Epoch 818/822\n",
      "256/256 [==============================] - 53s 205ms/step - loss: 0.1870 - accuracy: 0.9385 - val_loss: 0.2892 - val_accuracy: 0.9006 - lr: 0.0079 - momentum: 0.8506\n",
      "Epoch 819/822\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.1621 - accuracy: 0.9551 - val_loss: 0.4086 - val_accuracy: 0.8974 - lr: 0.0064 - momentum: 0.8688\n",
      "Epoch 820/822\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.1636 - accuracy: 0.9507 - val_loss: 0.3320 - val_accuracy: 0.9119 - lr: 0.0037 - momentum: 0.9037\n",
      "Epoch 821/822\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1256 - accuracy: 0.9653 - val_loss: 0.4950 - val_accuracy: 0.8686 - lr: 0.0011 - momentum: 0.9367\n",
      "Epoch 822/822\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.1094 - accuracy: 0.9712 - val_loss: 0.3723 - val_accuracy: 0.9071 - lr: 3.1680e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9071\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3723\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m408.01 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m322.74 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m85.27 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [137] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m138\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 822)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00786\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 823/828\n",
      "256/256 [==============================] - 59s 210ms/step - loss: 0.2042 - accuracy: 0.9351 - val_loss: 0.2513 - val_accuracy: 0.9231 - lr: 0.0047 - momentum: 0.8915\n",
      "Epoch 824/828\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1963 - accuracy: 0.9404 - val_loss: 0.2151 - val_accuracy: 0.9215 - lr: 0.0078 - momentum: 0.8506\n",
      "Epoch 825/828\n",
      "256/256 [==============================] - 51s 200ms/step - loss: 0.1588 - accuracy: 0.9556 - val_loss: 0.2643 - val_accuracy: 0.9343 - lr: 0.0064 - momentum: 0.8688\n",
      "Epoch 826/828\n",
      "256/256 [==============================] - 50s 196ms/step - loss: 0.1577 - accuracy: 0.9526 - val_loss: 0.2359 - val_accuracy: 0.9359 - lr: 0.0036 - momentum: 0.9037\n",
      "Epoch 827/828\n",
      "256/256 [==============================] - 51s 198ms/step - loss: 0.1204 - accuracy: 0.9688 - val_loss: 0.2857 - val_accuracy: 0.9119 - lr: 0.0010 - momentum: 0.9367\n",
      "Epoch 828/828\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1020 - accuracy: 0.9736 - val_loss: 0.2874 - val_accuracy: 0.9247 - lr: 3.1440e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9247\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2874\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m392.15 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m315.63 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m76.52 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [138] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m139\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 828)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0078\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 829/834\n",
      "256/256 [==============================] - 58s 209ms/step - loss: 0.2252 - accuracy: 0.9282 - val_loss: 0.5006 - val_accuracy: 0.8654 - lr: 0.0047 - momentum: 0.8915\n",
      "Epoch 830/834\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.2011 - accuracy: 0.9297 - val_loss: 0.2682 - val_accuracy: 0.9231 - lr: 0.0078 - momentum: 0.8506\n",
      "Epoch 831/834\n",
      "256/256 [==============================] - 51s 198ms/step - loss: 0.2673 - accuracy: 0.9067 - val_loss: 1.2017 - val_accuracy: 0.6282 - lr: 0.0063 - momentum: 0.8688\n",
      "Epoch 832/834\n",
      "256/256 [==============================] - 51s 197ms/step - loss: 0.5645 - accuracy: 0.7729 - val_loss: 0.4006 - val_accuracy: 0.8702 - lr: 0.0036 - momentum: 0.9037\n",
      "Epoch 833/834\n",
      "256/256 [==============================] - 52s 200ms/step - loss: 0.4609 - accuracy: 0.8315 - val_loss: 0.3658 - val_accuracy: 0.8686 - lr: 0.0010 - momentum: 0.9367\n",
      "Epoch 834/834\n",
      "256/256 [==============================] - 51s 198ms/step - loss: 0.3920 - accuracy: 0.8564 - val_loss: 0.3374 - val_accuracy: 0.8862 - lr: 3.1200e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.8862\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3374\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m392.03 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m315.15 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m76.88 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [139] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m140\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 834)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00774\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 835/840\n",
      "256/256 [==============================] - 59s 211ms/step - loss: 0.4071 - accuracy: 0.8442 - val_loss: 0.5462 - val_accuracy: 0.8125 - lr: 0.0047 - momentum: 0.8915\n",
      "Epoch 836/840\n",
      "256/256 [==============================] - 51s 199ms/step - loss: 0.3941 - accuracy: 0.8521 - val_loss: 0.3742 - val_accuracy: 0.8670 - lr: 0.0077 - momentum: 0.8506\n",
      "Epoch 837/840\n",
      "256/256 [==============================] - 51s 198ms/step - loss: 0.4309 - accuracy: 0.8438 - val_loss: 0.5963 - val_accuracy: 0.7869 - lr: 0.0063 - momentum: 0.8688\n",
      "Epoch 838/840\n",
      "256/256 [==============================] - 51s 197ms/step - loss: 0.4239 - accuracy: 0.8477 - val_loss: 0.5772 - val_accuracy: 0.8157 - lr: 0.0036 - momentum: 0.9037\n",
      "Epoch 839/840\n",
      "256/256 [==============================] - 51s 199ms/step - loss: 0.3448 - accuracy: 0.8794 - val_loss: 0.2869 - val_accuracy: 0.9135 - lr: 0.0010 - momentum: 0.9367\n",
      "Epoch 840/840\n",
      "256/256 [==============================] - 52s 200ms/step - loss: 0.2944 - accuracy: 0.9062 - val_loss: 0.2743 - val_accuracy: 0.9167 - lr: 3.0960e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9167\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2743\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m393.14 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m315.23 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m77.91 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [140] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m141\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 840)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00768\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 841/846\n",
      "256/256 [==============================] - 58s 207ms/step - loss: 0.3251 - accuracy: 0.8877 - val_loss: 0.3009 - val_accuracy: 0.9183 - lr: 0.0046 - momentum: 0.8915\n",
      "Epoch 842/846\n",
      "256/256 [==============================] - 51s 198ms/step - loss: 0.3769 - accuracy: 0.8794 - val_loss: 0.3459 - val_accuracy: 0.8974 - lr: 0.0076 - momentum: 0.8506\n",
      "Epoch 843/846\n",
      "256/256 [==============================] - 51s 198ms/step - loss: 0.3452 - accuracy: 0.8965 - val_loss: 0.3477 - val_accuracy: 0.9054 - lr: 0.0062 - momentum: 0.8688\n",
      "Epoch 844/846\n",
      "256/256 [==============================] - 52s 200ms/step - loss: 0.2915 - accuracy: 0.9136 - val_loss: 0.2519 - val_accuracy: 0.9295 - lr: 0.0036 - momentum: 0.9037\n",
      "Epoch 845/846\n",
      "256/256 [==============================] - 50s 193ms/step - loss: 0.2381 - accuracy: 0.9258 - val_loss: 0.2369 - val_accuracy: 0.9311 - lr: 0.0010 - momentum: 0.9367\n",
      "Epoch 846/846\n",
      "256/256 [==============================] - 49s 189ms/step - loss: 0.2109 - accuracy: 0.9414 - val_loss: 0.2506 - val_accuracy: 0.9311 - lr: 3.0720e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9311\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2506\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m387.85 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m310.67 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m77.18 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [141] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m142\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 846)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00762\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 847/852\n",
      "256/256 [==============================] - 54s 194ms/step - loss: 0.2725 - accuracy: 0.9126 - val_loss: 0.2212 - val_accuracy: 0.9359 - lr: 0.0046 - momentum: 0.8915\n",
      "Epoch 848/852\n",
      "256/256 [==============================] - 49s 191ms/step - loss: 0.2889 - accuracy: 0.9131 - val_loss: 0.2270 - val_accuracy: 0.9295 - lr: 0.0076 - momentum: 0.8506\n",
      "Epoch 849/852\n",
      "256/256 [==============================] - 49s 189ms/step - loss: 0.2461 - accuracy: 0.9268 - val_loss: 0.2459 - val_accuracy: 0.9327 - lr: 0.0062 - momentum: 0.8688\n",
      "Epoch 850/852\n",
      "256/256 [==============================] - 48s 189ms/step - loss: 0.2066 - accuracy: 0.9380 - val_loss: 0.2523 - val_accuracy: 0.9327 - lr: 0.0035 - momentum: 0.9037\n",
      "Epoch 851/852\n",
      "256/256 [==============================] - 47s 185ms/step - loss: 0.1830 - accuracy: 0.9414 - val_loss: 0.2316 - val_accuracy: 0.9311 - lr: 0.0010 - momentum: 0.9367\n",
      "Epoch 852/852\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1674 - accuracy: 0.9458 - val_loss: 0.2271 - val_accuracy: 0.9327 - lr: 3.0480e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9327\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2271\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m363.85 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m295.67 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m68.18 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [142] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m143\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 852)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00756\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 853/858\n",
      "256/256 [==============================] - 53s 192ms/step - loss: 0.2458 - accuracy: 0.9297 - val_loss: 0.2311 - val_accuracy: 0.9279 - lr: 0.0046 - momentum: 0.8915\n",
      "Epoch 854/858\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.2483 - accuracy: 0.9233 - val_loss: 0.2261 - val_accuracy: 0.9375 - lr: 0.0075 - momentum: 0.8506\n",
      "Epoch 855/858\n",
      "256/256 [==============================] - 48s 186ms/step - loss: 0.2358 - accuracy: 0.9312 - val_loss: 0.2399 - val_accuracy: 0.9343 - lr: 0.0061 - momentum: 0.8688\n",
      "Epoch 856/858\n",
      "256/256 [==============================] - 48s 189ms/step - loss: 0.2128 - accuracy: 0.9414 - val_loss: 0.2444 - val_accuracy: 0.9327 - lr: 0.0035 - momentum: 0.9037\n",
      "Epoch 857/858\n",
      "256/256 [==============================] - 49s 189ms/step - loss: 0.1592 - accuracy: 0.9570 - val_loss: 0.2456 - val_accuracy: 0.9263 - lr: 0.0010 - momentum: 0.9367\n",
      "Epoch 858/858\n",
      "256/256 [==============================] - 50s 194ms/step - loss: 0.1472 - accuracy: 0.9644 - val_loss: 0.2409 - val_accuracy: 0.9295 - lr: 3.0240e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9295\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2409\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m360.68 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m295.91 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m64.77 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [143] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m144\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 858)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33m└───Shuffling data...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0075\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 859/864\n",
      "256/256 [==============================] - 59s 208ms/step - loss: 0.2294 - accuracy: 0.9341 - val_loss: 0.1979 - val_accuracy: 0.9343 - lr: 0.0045 - momentum: 0.8915\n",
      "Epoch 860/864\n",
      "256/256 [==============================] - 51s 199ms/step - loss: 0.2352 - accuracy: 0.9272 - val_loss: 0.2006 - val_accuracy: 0.9407 - lr: 0.0075 - momentum: 0.8506\n",
      "Epoch 861/864\n",
      "256/256 [==============================] - 52s 200ms/step - loss: 0.1986 - accuracy: 0.9482 - val_loss: 0.3057 - val_accuracy: 0.9263 - lr: 0.0061 - momentum: 0.8688\n",
      "Epoch 862/864\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1940 - accuracy: 0.9497 - val_loss: 0.2065 - val_accuracy: 0.9359 - lr: 0.0035 - momentum: 0.9037\n",
      "Epoch 863/864\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1684 - accuracy: 0.9565 - val_loss: 0.2131 - val_accuracy: 0.9311 - lr: 0.0010 - momentum: 0.9367\n",
      "Epoch 864/864\n",
      "256/256 [==============================] - 52s 203ms/step - loss: 0.1330 - accuracy: 0.9673 - val_loss: 0.2187 - val_accuracy: 0.9311 - lr: 3.0000e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9311\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2187\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m404.78 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m317.77 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m87.01 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [144] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m145\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 864)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00744\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 865/870\n",
      "256/256 [==============================] - 58s 206ms/step - loss: 0.1902 - accuracy: 0.9414 - val_loss: 0.1884 - val_accuracy: 0.9423 - lr: 0.0045 - momentum: 0.8915\n",
      "Epoch 866/870\n",
      "256/256 [==============================] - 51s 199ms/step - loss: 0.1868 - accuracy: 0.9487 - val_loss: 0.2014 - val_accuracy: 0.9439 - lr: 0.0074 - momentum: 0.8506\n",
      "Epoch 867/870\n",
      "256/256 [==============================] - 50s 194ms/step - loss: 0.1852 - accuracy: 0.9482 - val_loss: 0.2021 - val_accuracy: 0.9503 - lr: 0.0060 - momentum: 0.8688\n",
      "Epoch 868/870\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1867 - accuracy: 0.9487 - val_loss: 0.2061 - val_accuracy: 0.9471 - lr: 0.0034 - momentum: 0.9037\n",
      "Epoch 869/870\n",
      "256/256 [==============================] - 47s 183ms/step - loss: 0.1382 - accuracy: 0.9663 - val_loss: 0.1945 - val_accuracy: 0.9455 - lr: 9.9307e-04 - momentum: 0.9367\n",
      "Epoch 870/870\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1195 - accuracy: 0.9727 - val_loss: 0.1933 - val_accuracy: 0.9487 - lr: 2.9760e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9487\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.1932\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m383.59 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m301.06 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m82.54 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [145] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m146\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 870)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00738\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 871/876\n",
      "256/256 [==============================] - 53s 190ms/step - loss: 0.2385 - accuracy: 0.9312 - val_loss: 0.2251 - val_accuracy: 0.9471 - lr: 0.0044 - momentum: 0.8915\n",
      "Epoch 872/876\n",
      "256/256 [==============================] - 48s 185ms/step - loss: 0.2060 - accuracy: 0.9443 - val_loss: 0.2112 - val_accuracy: 0.9391 - lr: 0.0073 - momentum: 0.8506\n",
      "Epoch 873/876\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1947 - accuracy: 0.9448 - val_loss: 0.2370 - val_accuracy: 0.9199 - lr: 0.0060 - momentum: 0.8688\n",
      "Epoch 874/876\n",
      "256/256 [==============================] - 47s 185ms/step - loss: 0.1689 - accuracy: 0.9561 - val_loss: 0.2176 - val_accuracy: 0.9375 - lr: 0.0034 - momentum: 0.9037\n",
      "Epoch 875/876\n",
      "256/256 [==============================] - 47s 185ms/step - loss: 0.1463 - accuracy: 0.9683 - val_loss: 0.2465 - val_accuracy: 0.9295 - lr: 9.8506e-04 - momentum: 0.9367\n",
      "Epoch 876/876\n",
      "256/256 [==============================] - 48s 185ms/step - loss: 0.1103 - accuracy: 0.9756 - val_loss: 0.2424 - val_accuracy: 0.9311 - lr: 2.9520e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9311\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2424\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m358.30 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m290.84 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m67.46 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [146] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m147\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 876)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00732\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 877/882\n",
      "256/256 [==============================] - 53s 190ms/step - loss: 0.1922 - accuracy: 0.9502 - val_loss: 0.2412 - val_accuracy: 0.9199 - lr: 0.0044 - momentum: 0.8915\n",
      "Epoch 878/882\n",
      "256/256 [==============================] - 48s 185ms/step - loss: 0.2124 - accuracy: 0.9438 - val_loss: 0.2525 - val_accuracy: 0.9295 - lr: 0.0073 - momentum: 0.8506\n",
      "Epoch 879/882\n",
      "256/256 [==============================] - 47s 183ms/step - loss: 0.1583 - accuracy: 0.9619 - val_loss: 0.3578 - val_accuracy: 0.8926 - lr: 0.0059 - momentum: 0.8688\n",
      "Epoch 880/882\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1665 - accuracy: 0.9575 - val_loss: 0.2298 - val_accuracy: 0.9295 - lr: 0.0034 - momentum: 0.9037\n",
      "Epoch 881/882\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1529 - accuracy: 0.9653 - val_loss: 0.2570 - val_accuracy: 0.9151 - lr: 9.7706e-04 - momentum: 0.9367\n",
      "Epoch 882/882\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1145 - accuracy: 0.9746 - val_loss: 0.2322 - val_accuracy: 0.9263 - lr: 2.9280e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9263\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2321\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m358.25 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m289.63 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m68.62 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [147] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m148\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 882)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00726\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 883/888\n",
      "256/256 [==============================] - 53s 190ms/step - loss: 0.1818 - accuracy: 0.9512 - val_loss: 0.1857 - val_accuracy: 0.9471 - lr: 0.0044 - momentum: 0.8915\n",
      "Epoch 884/888\n",
      "256/256 [==============================] - 47s 185ms/step - loss: 0.1545 - accuracy: 0.9580 - val_loss: 0.1797 - val_accuracy: 0.9391 - lr: 0.0072 - momentum: 0.8506\n",
      "Epoch 885/888\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1570 - accuracy: 0.9570 - val_loss: 0.2584 - val_accuracy: 0.9295 - lr: 0.0059 - momentum: 0.8688\n",
      "Epoch 886/888\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1446 - accuracy: 0.9707 - val_loss: 0.2854 - val_accuracy: 0.9247 - lr: 0.0034 - momentum: 0.9037\n",
      "Epoch 887/888\n",
      "256/256 [==============================] - 47s 185ms/step - loss: 0.1206 - accuracy: 0.9746 - val_loss: 0.3014 - val_accuracy: 0.9231 - lr: 9.6905e-04 - momentum: 0.9367\n",
      "Epoch 888/888\n",
      "256/256 [==============================] - 48s 186ms/step - loss: 0.1080 - accuracy: 0.9780 - val_loss: 0.2755 - val_accuracy: 0.9263 - lr: 2.9040e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9263\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2755\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m358.31 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m290.80 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m67.51 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [148] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m149\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 888)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0072\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 889/894\n",
      "256/256 [==============================] - 53s 190ms/step - loss: 0.1893 - accuracy: 0.9492 - val_loss: 0.2723 - val_accuracy: 0.9054 - lr: 0.0043 - momentum: 0.8915\n",
      "Epoch 890/894\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1712 - accuracy: 0.9531 - val_loss: 0.2447 - val_accuracy: 0.9231 - lr: 0.0072 - momentum: 0.8506\n",
      "Epoch 891/894\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1505 - accuracy: 0.9634 - val_loss: 0.3448 - val_accuracy: 0.9199 - lr: 0.0058 - momentum: 0.8688\n",
      "Epoch 892/894\n",
      "256/256 [==============================] - 48s 185ms/step - loss: 0.1553 - accuracy: 0.9678 - val_loss: 0.2513 - val_accuracy: 0.9247 - lr: 0.0033 - momentum: 0.9037\n",
      "Epoch 893/894\n",
      "256/256 [==============================] - 47s 185ms/step - loss: 0.1215 - accuracy: 0.9746 - val_loss: 0.2670 - val_accuracy: 0.9295 - lr: 9.6104e-04 - momentum: 0.9367\n",
      "Epoch 894/894\n",
      "256/256 [==============================] - 48s 185ms/step - loss: 0.1034 - accuracy: 0.9785 - val_loss: 0.2846 - val_accuracy: 0.9231 - lr: 2.8800e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9231\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2846\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m358.66 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m290.78 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m67.88 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [149] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m150\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 894)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00714\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 895/900\n",
      "256/256 [==============================] - 53s 190ms/step - loss: 0.2128 - accuracy: 0.9429 - val_loss: 0.2021 - val_accuracy: 0.9311 - lr: 0.0043 - momentum: 0.8915\n",
      "Epoch 896/900\n",
      "256/256 [==============================] - 48s 186ms/step - loss: 0.2065 - accuracy: 0.9424 - val_loss: 0.2557 - val_accuracy: 0.9279 - lr: 0.0071 - momentum: 0.8506\n",
      "Epoch 897/900\n",
      "256/256 [==============================] - 48s 185ms/step - loss: 0.1962 - accuracy: 0.9541 - val_loss: 0.2593 - val_accuracy: 0.9263 - lr: 0.0058 - momentum: 0.8688\n",
      "Epoch 898/900\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1682 - accuracy: 0.9556 - val_loss: 0.2827 - val_accuracy: 0.9327 - lr: 0.0033 - momentum: 0.9037\n",
      "Epoch 899/900\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1401 - accuracy: 0.9653 - val_loss: 0.2612 - val_accuracy: 0.9423 - lr: 9.5303e-04 - momentum: 0.9367\n",
      "Epoch 900/900\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1219 - accuracy: 0.9756 - val_loss: 0.2339 - val_accuracy: 0.9407 - lr: 2.8560e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9407\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2339\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m357.14 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m290.53 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m66.61 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [150] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m151\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 900)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00708\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 901/906\n",
      "256/256 [==============================] - 53s 190ms/step - loss: 0.1875 - accuracy: 0.9478 - val_loss: 0.1808 - val_accuracy: 0.9423 - lr: 0.0043 - momentum: 0.8915\n",
      "Epoch 902/906\n",
      "256/256 [==============================] - 47s 185ms/step - loss: 0.1839 - accuracy: 0.9482 - val_loss: 0.2252 - val_accuracy: 0.9407 - lr: 0.0070 - momentum: 0.8506\n",
      "Epoch 903/906\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1723 - accuracy: 0.9536 - val_loss: 0.1964 - val_accuracy: 0.9375 - lr: 0.0057 - momentum: 0.8688\n",
      "Epoch 904/906\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1340 - accuracy: 0.9668 - val_loss: 0.1814 - val_accuracy: 0.9439 - lr: 0.0033 - momentum: 0.9037\n",
      "Epoch 905/906\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1214 - accuracy: 0.9761 - val_loss: 0.1984 - val_accuracy: 0.9455 - lr: 9.4502e-04 - momentum: 0.9367\n",
      "Epoch 906/906\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1035 - accuracy: 0.9785 - val_loss: 0.2109 - val_accuracy: 0.9487 - lr: 2.8320e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9487\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2109\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m357.67 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m290.32 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m67.35 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [151] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m152\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 906)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00702\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 907/912\n",
      "256/256 [==============================] - 53s 191ms/step - loss: 0.1797 - accuracy: 0.9473 - val_loss: 0.1992 - val_accuracy: 0.9359 - lr: 0.0042 - momentum: 0.8915\n",
      "Epoch 908/912\n",
      "256/256 [==============================] - 48s 186ms/step - loss: 0.2243 - accuracy: 0.9380 - val_loss: 0.3800 - val_accuracy: 0.8862 - lr: 0.0070 - momentum: 0.8506\n",
      "Epoch 909/912\n",
      "256/256 [==============================] - 48s 185ms/step - loss: 0.1808 - accuracy: 0.9541 - val_loss: 0.3202 - val_accuracy: 0.8862 - lr: 0.0057 - momentum: 0.8688\n",
      "Epoch 910/912\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1874 - accuracy: 0.9575 - val_loss: 0.2607 - val_accuracy: 0.9167 - lr: 0.0032 - momentum: 0.9037\n",
      "Epoch 911/912\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1450 - accuracy: 0.9648 - val_loss: 0.2543 - val_accuracy: 0.9231 - lr: 9.3701e-04 - momentum: 0.9367\n",
      "Epoch 912/912\n",
      "256/256 [==============================] - 47s 185ms/step - loss: 0.1166 - accuracy: 0.9756 - val_loss: 0.2351 - val_accuracy: 0.9215 - lr: 2.8080e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9215\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2351\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m359.03 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m291.23 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m67.80 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [152] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m153\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 912)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00696\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 913/918\n",
      "256/256 [==============================] - 53s 191ms/step - loss: 0.1839 - accuracy: 0.9429 - val_loss: 0.2318 - val_accuracy: 0.9311 - lr: 0.0042 - momentum: 0.8915\n",
      "Epoch 914/918\n",
      "256/256 [==============================] - 48s 186ms/step - loss: 0.1900 - accuracy: 0.9507 - val_loss: 0.3164 - val_accuracy: 0.8974 - lr: 0.0069 - momentum: 0.8506\n",
      "Epoch 915/918\n",
      "256/256 [==============================] - 47s 183ms/step - loss: 0.1744 - accuracy: 0.9526 - val_loss: 0.2606 - val_accuracy: 0.9247 - lr: 0.0056 - momentum: 0.8688\n",
      "Epoch 916/918\n",
      "256/256 [==============================] - 47s 183ms/step - loss: 0.1593 - accuracy: 0.9648 - val_loss: 0.2437 - val_accuracy: 0.9295 - lr: 0.0032 - momentum: 0.9037\n",
      "Epoch 917/918\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1487 - accuracy: 0.9639 - val_loss: 0.2522 - val_accuracy: 0.9295 - lr: 9.2900e-04 - momentum: 0.9367\n",
      "Epoch 918/918\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1176 - accuracy: 0.9712 - val_loss: 0.2343 - val_accuracy: 0.9295 - lr: 2.7840e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9295\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2343\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m359.93 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m290.49 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m69.44 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [153] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m154\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 918)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0069\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 919/924\n",
      "256/256 [==============================] - 53s 190ms/step - loss: 0.1535 - accuracy: 0.9590 - val_loss: 0.2527 - val_accuracy: 0.9311 - lr: 0.0042 - momentum: 0.8915\n",
      "Epoch 920/924\n",
      "256/256 [==============================] - 48s 185ms/step - loss: 0.1513 - accuracy: 0.9580 - val_loss: 0.2025 - val_accuracy: 0.9295 - lr: 0.0069 - momentum: 0.8506\n",
      "Epoch 921/924\n",
      "256/256 [==============================] - 47s 183ms/step - loss: 0.1419 - accuracy: 0.9658 - val_loss: 0.2556 - val_accuracy: 0.9311 - lr: 0.0056 - momentum: 0.8688\n",
      "Epoch 922/924\n",
      "256/256 [==============================] - 48s 185ms/step - loss: 0.1130 - accuracy: 0.9761 - val_loss: 0.3402 - val_accuracy: 0.9263 - lr: 0.0032 - momentum: 0.9037\n",
      "Epoch 923/924\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1196 - accuracy: 0.9697 - val_loss: 0.2615 - val_accuracy: 0.9279 - lr: 9.2099e-04 - momentum: 0.9367\n",
      "Epoch 924/924\n",
      "256/256 [==============================] - 48s 185ms/step - loss: 0.0966 - accuracy: 0.9810 - val_loss: 0.2652 - val_accuracy: 0.9263 - lr: 2.7600e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9263\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2652\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m357.64 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m290.81 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m66.83 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [154] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m155\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 924)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00684\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 925/930\n",
      "256/256 [==============================] - 52s 189ms/step - loss: 0.1810 - accuracy: 0.9482 - val_loss: 0.2610 - val_accuracy: 0.9263 - lr: 0.0041 - momentum: 0.8915\n",
      "Epoch 926/930\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1863 - accuracy: 0.9463 - val_loss: 0.2579 - val_accuracy: 0.9087 - lr: 0.0068 - momentum: 0.8506\n",
      "Epoch 927/930\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1628 - accuracy: 0.9546 - val_loss: 0.2591 - val_accuracy: 0.9231 - lr: 0.0056 - momentum: 0.8688\n",
      "Epoch 928/930\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1435 - accuracy: 0.9648 - val_loss: 0.2720 - val_accuracy: 0.9087 - lr: 0.0032 - momentum: 0.9037\n",
      "Epoch 929/930\n",
      "256/256 [==============================] - 48s 185ms/step - loss: 0.1092 - accuracy: 0.9751 - val_loss: 0.2923 - val_accuracy: 0.8974 - lr: 9.1299e-04 - momentum: 0.9367\n",
      "Epoch 930/930\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.0997 - accuracy: 0.9790 - val_loss: 0.2254 - val_accuracy: 0.9407 - lr: 2.7360e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9407\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2254\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m357.55 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m289.99 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m67.56 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [155] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m156\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 930)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00678\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 931/936\n",
      "256/256 [==============================] - 53s 190ms/step - loss: 0.1535 - accuracy: 0.9546 - val_loss: 0.2177 - val_accuracy: 0.9375 - lr: 0.0041 - momentum: 0.8915\n",
      "Epoch 932/936\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1550 - accuracy: 0.9580 - val_loss: 0.2012 - val_accuracy: 0.9327 - lr: 0.0067 - momentum: 0.8506\n",
      "Epoch 933/936\n",
      "256/256 [==============================] - 47s 185ms/step - loss: 0.1364 - accuracy: 0.9683 - val_loss: 0.2958 - val_accuracy: 0.9151 - lr: 0.0055 - momentum: 0.8688\n",
      "Epoch 934/936\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1499 - accuracy: 0.9624 - val_loss: 0.2019 - val_accuracy: 0.9391 - lr: 0.0031 - momentum: 0.9037\n",
      "Epoch 935/936\n",
      "256/256 [==============================] - 47s 185ms/step - loss: 0.1129 - accuracy: 0.9731 - val_loss: 0.1907 - val_accuracy: 0.9407 - lr: 9.0498e-04 - momentum: 0.9367\n",
      "Epoch 936/936\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.0797 - accuracy: 0.9839 - val_loss: 0.1984 - val_accuracy: 0.9359 - lr: 2.7120e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9359\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.1984\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m357.65 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m290.11 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m67.53 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [156] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m157\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 936)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00672\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 937/942\n",
      "256/256 [==============================] - 53s 190ms/step - loss: 0.1605 - accuracy: 0.9551 - val_loss: 0.2294 - val_accuracy: 0.9247 - lr: 0.0040 - momentum: 0.8915\n",
      "Epoch 938/942\n",
      "256/256 [==============================] - 48s 185ms/step - loss: 0.1710 - accuracy: 0.9541 - val_loss: 0.2811 - val_accuracy: 0.9071 - lr: 0.0067 - momentum: 0.8506\n",
      "Epoch 939/942\n",
      "256/256 [==============================] - 47s 185ms/step - loss: 0.1501 - accuracy: 0.9595 - val_loss: 0.3557 - val_accuracy: 0.8926 - lr: 0.0055 - momentum: 0.8688\n",
      "Epoch 940/942\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1245 - accuracy: 0.9717 - val_loss: 0.2582 - val_accuracy: 0.9295 - lr: 0.0031 - momentum: 0.9037\n",
      "Epoch 941/942\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1033 - accuracy: 0.9775 - val_loss: 0.2379 - val_accuracy: 0.9215 - lr: 8.9697e-04 - momentum: 0.9367\n",
      "Epoch 942/942\n",
      "256/256 [==============================] - 47s 185ms/step - loss: 0.0856 - accuracy: 0.9810 - val_loss: 0.2351 - val_accuracy: 0.9247 - lr: 2.6880e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9247\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2351\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m358.30 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m290.37 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m67.93 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [157] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m158\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 942)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00666\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 943/948\n",
      "256/256 [==============================] - 53s 190ms/step - loss: 0.1583 - accuracy: 0.9531 - val_loss: 0.2100 - val_accuracy: 0.9359 - lr: 0.0040 - momentum: 0.8915\n",
      "Epoch 944/948\n",
      "256/256 [==============================] - 48s 185ms/step - loss: 0.1790 - accuracy: 0.9458 - val_loss: 0.1943 - val_accuracy: 0.9391 - lr: 0.0066 - momentum: 0.8506\n",
      "Epoch 945/948\n",
      "256/256 [==============================] - 47s 185ms/step - loss: 0.1494 - accuracy: 0.9648 - val_loss: 0.2878 - val_accuracy: 0.9247 - lr: 0.0054 - momentum: 0.8688\n",
      "Epoch 946/948\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.1229 - accuracy: 0.9722 - val_loss: 0.2283 - val_accuracy: 0.9343 - lr: 0.0031 - momentum: 0.9037\n",
      "Epoch 947/948\n",
      "256/256 [==============================] - 52s 203ms/step - loss: 0.1069 - accuracy: 0.9702 - val_loss: 0.2433 - val_accuracy: 0.9311 - lr: 8.8896e-04 - momentum: 0.9367\n",
      "Epoch 948/948\n",
      "256/256 [==============================] - 51s 199ms/step - loss: 0.0785 - accuracy: 0.9819 - val_loss: 0.2274 - val_accuracy: 0.9375 - lr: 2.6640e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9375\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2274\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m368.01 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m299.01 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m69.01 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [158] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m159\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 948)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0066\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 949/954\n",
      "256/256 [==============================] - 54s 195ms/step - loss: 0.1777 - accuracy: 0.9463 - val_loss: 0.1946 - val_accuracy: 0.9359 - lr: 0.0040 - momentum: 0.8915\n",
      "Epoch 950/954\n",
      "256/256 [==============================] - 49s 192ms/step - loss: 0.1593 - accuracy: 0.9468 - val_loss: 0.2057 - val_accuracy: 0.9279 - lr: 0.0066 - momentum: 0.8506\n",
      "Epoch 951/954\n",
      "256/256 [==============================] - 49s 189ms/step - loss: 0.1463 - accuracy: 0.9575 - val_loss: 0.2334 - val_accuracy: 0.9263 - lr: 0.0054 - momentum: 0.8688\n",
      "Epoch 952/954\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.1408 - accuracy: 0.9639 - val_loss: 0.1937 - val_accuracy: 0.9423 - lr: 0.0031 - momentum: 0.9037\n",
      "Epoch 953/954\n",
      "256/256 [==============================] - 50s 195ms/step - loss: 0.1109 - accuracy: 0.9722 - val_loss: 0.1936 - val_accuracy: 0.9487 - lr: 8.8095e-04 - momentum: 0.9367\n",
      "Epoch 954/954\n",
      "256/256 [==============================] - 49s 192ms/step - loss: 0.0957 - accuracy: 0.9761 - val_loss: 0.1874 - val_accuracy: 0.9439 - lr: 2.6400e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9439\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.1875\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m382.23 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m304.44 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m77.79 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [159] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m160\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 954)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00654\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 955/960\n",
      "256/256 [==============================] - 55s 196ms/step - loss: 0.1584 - accuracy: 0.9546 - val_loss: 0.1774 - val_accuracy: 0.9423 - lr: 0.0039 - momentum: 0.8915\n",
      "Epoch 956/960\n",
      "256/256 [==============================] - 49s 191ms/step - loss: 0.1720 - accuracy: 0.9541 - val_loss: 0.2030 - val_accuracy: 0.9391 - lr: 0.0065 - momentum: 0.8506\n",
      "Epoch 957/960\n",
      "256/256 [==============================] - 49s 190ms/step - loss: 0.1447 - accuracy: 0.9614 - val_loss: 0.2177 - val_accuracy: 0.9439 - lr: 0.0053 - momentum: 0.8688\n",
      "Epoch 958/960\n",
      "256/256 [==============================] - 50s 195ms/step - loss: 0.1295 - accuracy: 0.9697 - val_loss: 0.1932 - val_accuracy: 0.9535 - lr: 0.0030 - momentum: 0.9037\n",
      "Epoch 959/960\n",
      "256/256 [==============================] - 49s 191ms/step - loss: 0.0828 - accuracy: 0.9819 - val_loss: 0.1683 - val_accuracy: 0.9519 - lr: 8.7294e-04 - momentum: 0.9367\n",
      "Epoch 960/960\n",
      "256/256 [==============================] - 49s 191ms/step - loss: 0.0809 - accuracy: 0.9839 - val_loss: 0.1904 - val_accuracy: 0.9503 - lr: 2.6160e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-958-0.9535.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9535\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.1932\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.17357933521270752. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m375.79 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m301.20 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m74.58 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [160] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m161\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 960)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00648\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 961/966\n",
      "256/256 [==============================] - 56s 202ms/step - loss: 0.1814 - accuracy: 0.9434 - val_loss: 0.1690 - val_accuracy: 0.9471 - lr: 0.0039 - momentum: 0.8915\n",
      "Epoch 962/966\n",
      "256/256 [==============================] - 49s 191ms/step - loss: 0.1495 - accuracy: 0.9526 - val_loss: 0.1827 - val_accuracy: 0.9455 - lr: 0.0064 - momentum: 0.8506\n",
      "Epoch 963/966\n",
      "256/256 [==============================] - 50s 193ms/step - loss: 0.1986 - accuracy: 0.9487 - val_loss: 0.2563 - val_accuracy: 0.9311 - lr: 0.0053 - momentum: 0.8688\n",
      "Epoch 964/966\n",
      "256/256 [==============================] - 50s 192ms/step - loss: 0.1490 - accuracy: 0.9629 - val_loss: 0.2136 - val_accuracy: 0.9407 - lr: 0.0030 - momentum: 0.9037\n",
      "Epoch 965/966\n",
      "256/256 [==============================] - 49s 193ms/step - loss: 0.1169 - accuracy: 0.9727 - val_loss: 0.2108 - val_accuracy: 0.9407 - lr: 8.6493e-04 - momentum: 0.9367\n",
      "Epoch 966/966\n",
      "256/256 [==============================] - 49s 192ms/step - loss: 0.0937 - accuracy: 0.9819 - val_loss: 0.1931 - val_accuracy: 0.9439 - lr: 2.5920e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-961-0.9471.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9471\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.1691\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model loss from \u001b[0m\u001b[0;32m0.17357933521270752 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.169065922498703\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m378.84 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m303.75 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m75.08 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [161] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m162\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 966)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33m└───Shuffling data...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00642\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 967/972\n",
      "256/256 [==============================] - 55s 200ms/step - loss: 0.1477 - accuracy: 0.9551 - val_loss: 0.1649 - val_accuracy: 0.9535 - lr: 0.0039 - momentum: 0.8915\n",
      "Epoch 968/972\n",
      "256/256 [==============================] - 49s 191ms/step - loss: 0.1534 - accuracy: 0.9585 - val_loss: 0.2445 - val_accuracy: 0.9343 - lr: 0.0064 - momentum: 0.8506\n",
      "Epoch 969/972\n",
      "256/256 [==============================] - 49s 191ms/step - loss: 0.1222 - accuracy: 0.9688 - val_loss: 0.2051 - val_accuracy: 0.9407 - lr: 0.0052 - momentum: 0.8688\n",
      "Epoch 970/972\n",
      "256/256 [==============================] - 49s 191ms/step - loss: 0.0908 - accuracy: 0.9731 - val_loss: 0.2130 - val_accuracy: 0.9455 - lr: 0.0030 - momentum: 0.9037\n",
      "Epoch 971/972\n",
      "256/256 [==============================] - 52s 203ms/step - loss: 0.0757 - accuracy: 0.9839 - val_loss: 0.2308 - val_accuracy: 0.9407 - lr: 8.5693e-04 - momentum: 0.9367\n",
      "Epoch 972/972\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.0693 - accuracy: 0.9839 - val_loss: 0.2432 - val_accuracy: 0.9375 - lr: 2.5680e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0;33mLoading the best weights...\u001b[0m\n",
      "\u001b[0;33mLoading weights from file cache\\model_SUB_checkpoint-967-0.9535.h5...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9535\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.1649\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mImproved model loss from \u001b[0m\u001b[0;32m0.169065922498703 \u001b[0m\u001b[0;33mto \u001b[0m\u001b[0;32m0.16494178771972656\u001b[0m\u001b[0;33m. \u001b[0m\u001b[0;96mSaving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;36mSaving full model H5 format...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m387.15 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m307.80 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m79.35 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [162] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m163\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 972)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00636\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 973/978\n",
      "256/256 [==============================] - 59s 211ms/step - loss: 0.1391 - accuracy: 0.9644 - val_loss: 0.1754 - val_accuracy: 0.9519 - lr: 0.0038 - momentum: 0.8915\n",
      "Epoch 974/978\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.1578 - accuracy: 0.9590 - val_loss: 0.2103 - val_accuracy: 0.9551 - lr: 0.0063 - momentum: 0.8506\n",
      "Epoch 975/978\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.1195 - accuracy: 0.9692 - val_loss: 0.2345 - val_accuracy: 0.9423 - lr: 0.0052 - momentum: 0.8688\n",
      "Epoch 976/978\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1059 - accuracy: 0.9751 - val_loss: 0.1932 - val_accuracy: 0.9391 - lr: 0.0029 - momentum: 0.9037\n",
      "Epoch 977/978\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.1070 - accuracy: 0.9751 - val_loss: 0.1781 - val_accuracy: 0.9487 - lr: 8.4892e-04 - momentum: 0.9367\n",
      "Epoch 978/978\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.0749 - accuracy: 0.9854 - val_loss: 0.2030 - val_accuracy: 0.9503 - lr: 2.5440e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9503\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2030\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.16494178771972656. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m415.92 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m320.24 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m95.68 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [163] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m164\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 978)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.0063\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 979/984\n",
      "256/256 [==============================] - 59s 210ms/step - loss: 0.1456 - accuracy: 0.9565 - val_loss: 0.2190 - val_accuracy: 0.9535 - lr: 0.0038 - momentum: 0.8915\n",
      "Epoch 980/984\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.1325 - accuracy: 0.9653 - val_loss: 0.1992 - val_accuracy: 0.9391 - lr: 0.0063 - momentum: 0.8506\n",
      "Epoch 981/984\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.1214 - accuracy: 0.9707 - val_loss: 0.2051 - val_accuracy: 0.9375 - lr: 0.0051 - momentum: 0.8688\n",
      "Epoch 982/984\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1205 - accuracy: 0.9751 - val_loss: 0.2231 - val_accuracy: 0.9471 - lr: 0.0029 - momentum: 0.9037\n",
      "Epoch 983/984\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.0890 - accuracy: 0.9795 - val_loss: 0.2044 - val_accuracy: 0.9471 - lr: 8.4091e-04 - momentum: 0.9367\n",
      "Epoch 984/984\n",
      "256/256 [==============================] - 51s 201ms/step - loss: 0.0708 - accuracy: 0.9883 - val_loss: 0.2096 - val_accuracy: 0.9455 - lr: 2.5200e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9455\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2096\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.16494178771972656. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m409.42 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m318.39 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m91.03 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [164] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m165\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 984)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00624\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 985/990\n",
      "256/256 [==============================] - 58s 208ms/step - loss: 0.1737 - accuracy: 0.9521 - val_loss: 0.1887 - val_accuracy: 0.9407 - lr: 0.0038 - momentum: 0.8915\n",
      "Epoch 986/990\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.1476 - accuracy: 0.9619 - val_loss: 0.1742 - val_accuracy: 0.9455 - lr: 0.0062 - momentum: 0.8506\n",
      "Epoch 987/990\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1491 - accuracy: 0.9639 - val_loss: 0.2558 - val_accuracy: 0.9199 - lr: 0.0051 - momentum: 0.8688\n",
      "Epoch 988/990\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1552 - accuracy: 0.9575 - val_loss: 0.3135 - val_accuracy: 0.9295 - lr: 0.0029 - momentum: 0.9037\n",
      "Epoch 989/990\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.1227 - accuracy: 0.9668 - val_loss: 0.2008 - val_accuracy: 0.9375 - lr: 8.3290e-04 - momentum: 0.9367\n",
      "Epoch 990/990\n",
      "256/256 [==============================] - 51s 201ms/step - loss: 0.0954 - accuracy: 0.9785 - val_loss: 0.2084 - val_accuracy: 0.9343 - lr: 2.4960e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9343\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2084\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.16494178771972656. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m406.11 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m317.79 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m88.32 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [165] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m166\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 990)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00618\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 991/996\n",
      "256/256 [==============================] - 59s 210ms/step - loss: 0.1662 - accuracy: 0.9536 - val_loss: 0.2017 - val_accuracy: 0.9199 - lr: 0.0037 - momentum: 0.8915\n",
      "Epoch 992/996\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1472 - accuracy: 0.9614 - val_loss: 0.2142 - val_accuracy: 0.9311 - lr: 0.0061 - momentum: 0.8506\n",
      "Epoch 993/996\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1322 - accuracy: 0.9668 - val_loss: 0.2460 - val_accuracy: 0.9231 - lr: 0.0050 - momentum: 0.8688\n",
      "Epoch 994/996\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1330 - accuracy: 0.9668 - val_loss: 0.2677 - val_accuracy: 0.9279 - lr: 0.0029 - momentum: 0.9037\n",
      "Epoch 995/996\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.0932 - accuracy: 0.9805 - val_loss: 0.2991 - val_accuracy: 0.9167 - lr: 8.2489e-04 - momentum: 0.9367\n",
      "Epoch 996/996\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.0812 - accuracy: 0.9844 - val_loss: 0.3042 - val_accuracy: 0.9119 - lr: 2.4720e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9119\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3042\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.16494178771972656. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m409.32 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m318.27 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m91.05 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [166] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m167\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 996)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00612\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 997/1002\n",
      "256/256 [==============================] - 59s 208ms/step - loss: 0.1784 - accuracy: 0.9492 - val_loss: 0.2218 - val_accuracy: 0.9343 - lr: 0.0037 - momentum: 0.8915\n",
      "Epoch 998/1002\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1566 - accuracy: 0.9502 - val_loss: 0.3255 - val_accuracy: 0.9119 - lr: 0.0061 - momentum: 0.8506\n",
      "Epoch 999/1002\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1466 - accuracy: 0.9648 - val_loss: 0.2080 - val_accuracy: 0.9391 - lr: 0.0050 - momentum: 0.8688\n",
      "Epoch 1000/1002\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1251 - accuracy: 0.9663 - val_loss: 0.2467 - val_accuracy: 0.9327 - lr: 0.0028 - momentum: 0.9037\n",
      "Epoch 1001/1002\n",
      "256/256 [==============================] - 52s 200ms/step - loss: 0.0961 - accuracy: 0.9751 - val_loss: 0.2129 - val_accuracy: 0.9359 - lr: 8.1688e-04 - momentum: 0.9367\n",
      "Epoch 1002/1002\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.0840 - accuracy: 0.9834 - val_loss: 0.2096 - val_accuracy: 0.9439 - lr: 2.4480e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9439\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2096\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.16494178771972656. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m409.98 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m317.79 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m92.18 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [167] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m168\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 1002)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00606\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 1003/1008\n",
      "256/256 [==============================] - 59s 209ms/step - loss: 0.1514 - accuracy: 0.9526 - val_loss: 0.1762 - val_accuracy: 0.9471 - lr: 0.0036 - momentum: 0.8915\n",
      "Epoch 1004/1008\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1407 - accuracy: 0.9614 - val_loss: 0.2518 - val_accuracy: 0.9375 - lr: 0.0060 - momentum: 0.8506\n",
      "Epoch 1005/1008\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1072 - accuracy: 0.9761 - val_loss: 0.2631 - val_accuracy: 0.9071 - lr: 0.0049 - momentum: 0.8688\n",
      "Epoch 1006/1008\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.0974 - accuracy: 0.9775 - val_loss: 0.2442 - val_accuracy: 0.9151 - lr: 0.0028 - momentum: 0.9037\n",
      "Epoch 1007/1008\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.0867 - accuracy: 0.9819 - val_loss: 0.3030 - val_accuracy: 0.9231 - lr: 8.0887e-04 - momentum: 0.9367\n",
      "Epoch 1008/1008\n",
      "256/256 [==============================] - 52s 200ms/step - loss: 0.0640 - accuracy: 0.9897 - val_loss: 0.3008 - val_accuracy: 0.9279 - lr: 2.4240e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9279\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3008\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.16494178771972656. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m412.44 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m318.26 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m94.17 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [168] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m169\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 1008)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.006\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 1009/1014\n",
      "256/256 [==============================] - 58s 207ms/step - loss: 0.1399 - accuracy: 0.9663 - val_loss: 0.2912 - val_accuracy: 0.9231 - lr: 0.0036 - momentum: 0.8915\n",
      "Epoch 1010/1014\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.1143 - accuracy: 0.9697 - val_loss: 0.2185 - val_accuracy: 0.9295 - lr: 0.0060 - momentum: 0.8506\n",
      "Epoch 1011/1014\n",
      "256/256 [==============================] - 51s 199ms/step - loss: 0.1123 - accuracy: 0.9707 - val_loss: 0.2335 - val_accuracy: 0.9359 - lr: 0.0049 - momentum: 0.8688\n",
      "Epoch 1012/1014\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.0941 - accuracy: 0.9829 - val_loss: 0.2705 - val_accuracy: 0.9103 - lr: 0.0028 - momentum: 0.9037\n",
      "Epoch 1013/1014\n",
      "256/256 [==============================] - 51s 200ms/step - loss: 0.0757 - accuracy: 0.9839 - val_loss: 0.2618 - val_accuracy: 0.9247 - lr: 8.0087e-04 - momentum: 0.9367\n",
      "Epoch 1014/1014\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.0629 - accuracy: 0.9834 - val_loss: 0.2533 - val_accuracy: 0.9327 - lr: 2.4000e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9327\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2533\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.16494178771972656. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m411.90 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m317.42 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m94.48 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [169] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m170\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 1014)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00594\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 1015/1020\n",
      "256/256 [==============================] - 59s 209ms/step - loss: 0.1347 - accuracy: 0.9634 - val_loss: 0.1971 - val_accuracy: 0.9247 - lr: 0.0036 - momentum: 0.8915\n",
      "Epoch 1016/1020\n",
      "256/256 [==============================] - 52s 200ms/step - loss: 0.1258 - accuracy: 0.9663 - val_loss: 0.2425 - val_accuracy: 0.9455 - lr: 0.0059 - momentum: 0.8506\n",
      "Epoch 1017/1020\n",
      "256/256 [==============================] - 51s 200ms/step - loss: 0.1098 - accuracy: 0.9702 - val_loss: 0.2983 - val_accuracy: 0.9375 - lr: 0.0048 - momentum: 0.8688\n",
      "Epoch 1018/1020\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.0974 - accuracy: 0.9761 - val_loss: 0.2847 - val_accuracy: 0.9455 - lr: 0.0027 - momentum: 0.9037\n",
      "Epoch 1019/1020\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.0752 - accuracy: 0.9863 - val_loss: 0.2719 - val_accuracy: 0.9343 - lr: 7.9286e-04 - momentum: 0.9367\n",
      "Epoch 1020/1020\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.0758 - accuracy: 0.9839 - val_loss: 0.2827 - val_accuracy: 0.9359 - lr: 2.3760e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9359\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2827\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.16494178771972656. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m416.61 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m318.33 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m98.28 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [170] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m171\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 1020)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00588\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 1021/1026\n",
      "256/256 [==============================] - 59s 210ms/step - loss: 0.1609 - accuracy: 0.9561 - val_loss: 0.2086 - val_accuracy: 0.9375 - lr: 0.0035 - momentum: 0.8915\n",
      "Epoch 1022/1026\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.1205 - accuracy: 0.9653 - val_loss: 0.2133 - val_accuracy: 0.9407 - lr: 0.0058 - momentum: 0.8506\n",
      "Epoch 1023/1026\n",
      "256/256 [==============================] - 52s 202ms/step - loss: 0.1043 - accuracy: 0.9717 - val_loss: 0.2414 - val_accuracy: 0.9359 - lr: 0.0048 - momentum: 0.8688\n",
      "Epoch 1024/1026\n",
      "256/256 [==============================] - 51s 200ms/step - loss: 0.0886 - accuracy: 0.9766 - val_loss: 0.3067 - val_accuracy: 0.9439 - lr: 0.0027 - momentum: 0.9037\n",
      "Epoch 1025/1026\n",
      "256/256 [==============================] - 52s 200ms/step - loss: 0.1194 - accuracy: 0.9795 - val_loss: 0.2449 - val_accuracy: 0.9375 - lr: 7.8485e-04 - momentum: 0.9367\n",
      "Epoch 1026/1026\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.0854 - accuracy: 0.9888 - val_loss: 0.2389 - val_accuracy: 0.9391 - lr: 2.3520e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9391\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.2389\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.16494178771972656. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m410.94 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m318.36 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m92.58 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [171] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m172\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 1026)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n",
      "\u001b[0;33m- Normalizing Image Data...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training OneCycleLr::maxlr to \u001b[0m\u001b[0;32m[0.00582\u001b[0m\u001b[0;31m\u001b[0m\u001b[0;32m]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mSetting training subset epoch.c to \u001b[0m\u001b[0;32m[6]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;32mTraining on subset...\u001b[0m\n",
      "Epoch 1027/1032\n",
      "256/256 [==============================] - 59s 210ms/step - loss: 0.1825 - accuracy: 0.9546 - val_loss: 0.2403 - val_accuracy: 0.9359 - lr: 0.0035 - momentum: 0.8915\n",
      "Epoch 1028/1032\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1548 - accuracy: 0.9604 - val_loss: 0.2380 - val_accuracy: 0.9311 - lr: 0.0058 - momentum: 0.8506\n",
      "Epoch 1029/1032\n",
      "256/256 [==============================] - 52s 201ms/step - loss: 0.1368 - accuracy: 0.9678 - val_loss: 0.2339 - val_accuracy: 0.9295 - lr: 0.0047 - momentum: 0.8688\n",
      "Epoch 1030/1032\n",
      "256/256 [==============================] - 52s 203ms/step - loss: 0.1160 - accuracy: 0.9746 - val_loss: 0.3557 - val_accuracy: 0.9151 - lr: 0.0027 - momentum: 0.9037\n",
      "Epoch 1031/1032\n",
      "256/256 [==============================] - 51s 198ms/step - loss: 0.0921 - accuracy: 0.9800 - val_loss: 0.4045 - val_accuracy: 0.8910 - lr: 7.7684e-04 - momentum: 0.9367\n",
      "Epoch 1032/1032\n",
      "256/256 [==============================] - 51s 197ms/step - loss: 0.0839 - accuracy: 0.9849 - val_loss: 0.3134 - val_accuracy: 0.9135 - lr: 2.3280e-08 - momentum: 0.9500\n",
      "\u001b[0;32mSubset training done.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mNot loading weights\u001b[0m\u001b[0;32m[\u001b[0m\u001b[0;94mBSR:\u001b[0m\u001b[0;33macc{95.6756}, \u001b[0m\u001b[0;33mloss{0.0111}\u001b[0m\u001b[0;95m|\u001b[0m\u001b[0;94mBTR:\u001b[0m\u001b[0;32macc{97.5646}, loss{0.0020}]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test acc: \u001b[0m\u001b[0;32m0.9135\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mModel Test loss: \u001b[0m\u001b[0;32m0.3134\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel accuracy did not improve from 0.9599359035491943. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;91mModel loss did not improve from 0.16494178771972656. Not saving model.\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(FULL): \u001b[0m\u001b[0;32m408.48 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(SUBo): \u001b[0m\u001b[0;32m316.99 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTime taken for epoch(OTHERo): \u001b[0m\u001b[0;32m91.49 \u001b[0m\u001b[0;36msec\u001b[0m\n",
      "\u001b[0;36m<---------------------------------------|Epoch [172] END|--------------------------------------->\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0mEpoch: \u001b[0m\u001b[0;36m173\u001b[0m\u001b[0m/\u001b[0m\u001b[0;32m384 (TSEC: 1032)\u001b[0m\u001b[0;34m | \u001b[0m\u001b[0;32m[Fine tuning]\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0;33mTaking a subset of \u001b[0m\u001b[0;32m[|2048|AdvSubset:True]\u001b[0m\u001b[0;33m...\u001b[0m\n",
      "\u001b[0;33mPreparing train data...\u001b[0m\n",
      "\u001b[0;33m- Augmenting Image Data...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "# Garbage Collection (memory)\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "# CONF <-------------------------------------------------------------------------->\n",
    "# Hyperparameters for training the model:\n",
    "max_epoch = 384 # max_epoch: Maximum number of epochs to train for. Use >=256 for full fine-tuning of large models.\n",
    "subset_epoch = 6 # subset_epoch: Number of epochs to train each subset.\n",
    "subset_epoch_FT = 6 # subset_epoch_FT: subset_epoch after pre-training epochs.\n",
    "PL_epoch = 24 # PL_epoch: Number of pre-training epochs. Use >=24 for large models or 0/1 for fine-tuning only.\n",
    "subset_size = 2048 # subset_size: Size of each training subset. Common values: 512, 1024, 2048, 4096.\n",
    "Conf_batch_size_REV2 = 8 # Conf_batch_size_REV2: Batch size.\n",
    "MAX_LR = 0.0147 # MAX_LR: Maximum learning rate.\n",
    "DEC_LR = 0.00006 # DEC_LR: Learning rate decay.\n",
    "MIN_LR = 0.0005 # MIN_LR: Minimum learning rate.\n",
    "OneCycleLr_UFTS = False # OneCycleLr_UFTS: Set the OneCycleLr max epochs to the estimated full training SUB epochs. (DEC_LR and MIN_LR dont have any effect if True)\n",
    "Debug_OUTPUT_DPS = True # Debug_OUTPUT_DPS: Output debug image samples if True.\n",
    "Debug_OUTPUT_DPS_freq = 64 # Debug_OUTPUT_DPS_freq: Debug image output frequency(epoch).\n",
    "TerminateOnHighTemp_M = True # TerminateOnHighTemp_M: Terminate training on high GPU temp to prevent damage.\n",
    "SAVE_FULLM = True # SAVE_FULLM: Save full model if True.\n",
    "USE_REV2_DP = False # USE_REV2_DP: Use Rev2 data preprocessing if True.\n",
    "AdvSubsetC = True  # AdvSubsetC: Use advanced subset sampling to prevent overfitting if True.\n",
    "AdvSubsetC_SHR = 18 # AdvSubsetC_SHR: Parameter for advanced subset sampling (shuffling data after n epochs).\n",
    "load_SUB_BRW = True # load_SUB_BRW: Load previous subset weights to speed up training if True. May reduce max accuracy.\n",
    "load_SUB_BRW_MODE = 'val_accuracy' # load_SUB_BRW_MODE: Previous subset weights loading mode - 'val_accuracy' or 'val_loss'.\n",
    "load_SUB_BRW_LMODE = 0 # load_SUB_BRW_LMODE: Previous subset weights loading mode parameter (1 for only on imp and !1 for normal mode (for subset_epoch > 6 normal mode is better)).\n",
    "load_SUB_BRW_LMODE_FN = True # load_SUB_BRW_LMODE_FN: Set load_SUB_BRW_LMODE=1 during fine-tuning if True.\n",
    "ModelCheckpoint_mode = 'auto' # ModelCheckpoint_mode: 'auto', 'min', or 'max' - how to monitor ModelCheckpoint.\n",
    "ModelCheckpoint_Reset_TO = 0.6251 # ModelCheckpoint_Reset_TO: Reset ModelCheckpoint monitor to this value, e.g. 0 or float('inf').\n",
    "Auto_clear_cache = True # Auto_clear_cache: Clear cache during training if True to reduce memory usage.\n",
    "Use_ES_ONSUBT = False # Use_ES_ONSUBT: Early stopping per subset (deprecated).\n",
    "EarlyStopping_P = 5 # EarlyStopping_P: Early stopping patience (deprecated).\n",
    "Use_tensorboard_profiler = False # Use_tensorboard_profiler: Enable tensorboard profiler.\n",
    "Use_extended_tensorboard = True # Use_extended_tensorboard: Enable extended tensorboard.\n",
    "BEST_RSN = 'PAI_model_T' # Best model save name prefix.\n",
    "ALWAYS_REFIT = False # Refit ImageDataGenerator evey time.\n",
    "IMAGE_GEN_PATH = 'Data\\\\image_SUB_generator.pkl'\n",
    "# CONF END <---------------------------------------------------------------------->\n",
    "#VAR\n",
    "Total_SUB_epoch_C = 0 # TO FIX TensorBoard\n",
    "CU_LR = MAX_LR\n",
    "all_histories = []\n",
    "chosen_indices = []\n",
    "subset_sizes = []\n",
    "best_acc = 0\n",
    "best_loss = float('inf')\n",
    "#Funcs\n",
    "def normalize_TO_RANGE(arr, min_val, max_val):\n",
    "  arr = arr.astype('float32')\n",
    "  arr = (arr - arr.min()) / (arr.max() - arr.min())\n",
    "  arr = arr * (max_val - min_val) + min_val\n",
    "  return arr\n",
    "\n",
    "def Z_SCORE_normalize(arr):\n",
    "   arr = arr.astype('float32')\n",
    "   mean = np.mean(arr)\n",
    "   std_dev = np.std(arr)\n",
    "   arr = (arr - mean) / std_dev\n",
    "   return arr\n",
    "\n",
    "def add_image_grain_TRLRev2(image, intensity = 0.01):\n",
    "    # Generate random noise array\n",
    "    noise = (np.random.randint(-255, 255, size=image.shape, dtype=np.int16) \\\n",
    "          + np.random.randint(-255, 255, size=image.shape, dtype=np.int16)) / 2\n",
    "\n",
    "    # Scale the noise array\n",
    "    scaled_noise = (noise * intensity).astype(np.float32)\n",
    "    # Add the noise to the image\n",
    "    noisy_image = cv2.add(image, scaled_noise)\n",
    "\n",
    "    return noisy_image\n",
    "# noise_func_TRLRev2 ([REV1 OLD])\n",
    "if not USE_REV2_DP:\n",
    "    def noise_func_TRLRev2(image): \n",
    "        noise_type = np.random.choice(['L1', 'L2', 'L3', 'none'])\n",
    "        new_image = np.copy(image)\n",
    "        \n",
    "        if noise_type == 'L3':\n",
    "            intensityL2 = random.uniform(-0.04, 0.04)\n",
    "            intensityL1 = random.uniform(-0.03, 0.03)\n",
    "        else:\n",
    "            intensityL2 = random.uniform(-0.05, 0.05)\n",
    "            intensityL1 = random.uniform(-0.04, 0.04)\n",
    "            \n",
    "        block_size_L1 = random.randint(16, 32)\n",
    "        block_size_L2 = random.randint(32, 112)\n",
    "        \n",
    "        if noise_type == 'L2' or noise_type == 'L3':\n",
    "            for i in range(0, image.shape[0], block_size_L2):\n",
    "                for j in range(0, image.shape[1], block_size_L2):\n",
    "                    block = image[i:i+block_size_L2, j:j+block_size_L2]\n",
    "                    block = (np.random.rand() * intensityL2 + 1) * block\n",
    "                    new_image[i:i+block_size_L2, j:j+block_size_L2] = block\n",
    "            image = new_image      \n",
    "            \n",
    "        if noise_type == 'L1' or noise_type == 'L3': \n",
    "            for i in range(0, image.shape[0], block_size_L1):\n",
    "                for j in range(0, image.shape[1], block_size_L1):\n",
    "                    block = image[i:i+block_size_L1, j:j+block_size_L1]\n",
    "                    block = (np.random.rand() * intensityL1 + 1) * block\n",
    "                    new_image[i:i+block_size_L1, j:j+block_size_L1] = block\n",
    "        \n",
    "        if add_img_grain:\n",
    "            intensity = random.uniform(0, 0.05)  # Random intensity \n",
    "            new_image = add_image_grain_TRLRev2(new_image, intensity=intensity)\n",
    "        return new_image\n",
    "# noise_func_TRLRev2 ([REV2 NEW])\n",
    "else:\n",
    "    def noise_func_TRLRev2(image):\n",
    "        noise_type = np.random.choice(['L1', 'L2', 'L3', 'none'])\n",
    "        new_image = np.copy(image)\n",
    "        \n",
    "        if noise_type == 'L3':\n",
    "            intensityL2 = random.uniform(-0.07, 0.07)\n",
    "            intensityL1 = random.uniform(-0.06, 0.06)\n",
    "        else:\n",
    "            intensityL2 = random.uniform(-0.09, 0.09)\n",
    "            intensityL1 = random.uniform(-0.07, 0.07)\n",
    "        \n",
    "        block_size_L1 = random.randint(16, 32)\n",
    "        block_size_L2 = random.randint(32, 112)\n",
    "        \n",
    "        for channel in range(3):  # Iterate over each RGB channel\n",
    "            image_channel = image[:, :, channel]\n",
    "            new_image_channel = new_image[:, :, channel]\n",
    "            \n",
    "            if noise_type == 'L2' or noise_type == 'L3':\n",
    "                for i in range(0, image_channel.shape[0], block_size_L2):\n",
    "                    for j in range(0, image_channel.shape[1], block_size_L2):\n",
    "                        block = image_channel[i:i+block_size_L2, j:j+block_size_L2]\n",
    "                        block = (np.random.rand() * intensityL2 + 1) * block\n",
    "                        new_image_channel[i:i+block_size_L2, j:j+block_size_L2] = block\n",
    "                image_channel = new_image_channel      \n",
    "            \n",
    "            if noise_type == 'L1' or noise_type == 'L3': \n",
    "                for i in range(0, image_channel.shape[0], block_size_L1):\n",
    "                    for j in range(0, image_channel.shape[1], block_size_L1):\n",
    "                        block = image_channel[i:i+block_size_L1, j:j+block_size_L1]\n",
    "                        block = (np.random.rand() * intensityL1 + 1) * block\n",
    "                        new_image_channel[i:i+block_size_L1, j:j+block_size_L1] = block\n",
    "            \n",
    "            new_image[:, :, channel] = new_image_channel\n",
    "        \n",
    "        if add_img_grain:\n",
    "            intensity = random.uniform(0, 0.05)  # Random intensity \n",
    "            new_image = add_image_grain_TRLRev2(new_image, intensity=intensity)\n",
    "        return new_image\n",
    "#CONST\n",
    "train_SUB_datagen = ImageDataGenerator(\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        rotation_range=179,\n",
    "        zoom_range=0.18, \n",
    "        shear_range=0.18,\n",
    "        width_shift_range=0.18,\n",
    "        brightness_range=(0.82, 1.10),\n",
    "        height_shift_range=0.18,\n",
    "        channel_shift_range=100,\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        zca_whitening=False,\n",
    "        interpolation_order=2,\n",
    "        fill_mode='nearest',\n",
    "        preprocessing_function=noise_func_TRLRev2\n",
    "    )\n",
    "class TerminateOnHighTemp(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, active=True, check_every_n_batches=2, high_temp=75, low_temp=60, pause_time=60):\n",
    "        super().__init__()\n",
    "        self.active = active\n",
    "        self.check_every_n_batches = check_every_n_batches\n",
    "        self.high_temp = high_temp\n",
    "        self.low_temp = low_temp\n",
    "        self.pause_time = pause_time\n",
    "        self.batch_counter = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if not self.active:\n",
    "            return\n",
    "        self.batch_counter += 1\n",
    "        if self.batch_counter % self.check_every_n_batches == 0:\n",
    "            temperature = gpu_control.get_temperature()\n",
    "            if temperature > self.high_temp:\n",
    "                print_Color(f'\\nPausing training due to high GPU temperature! (for [{self.pause_time}]sec)', ['red'], advanced_mode=False)\n",
    "                time.sleep(self.pause_time) \n",
    "                while gpu_control.get_temperature() > self.low_temp:\n",
    "                    time.sleep(4)\n",
    "                print_Color('Resuming training...', ['yellow'])\n",
    "class ExtendedTensorBoard(TensorBoard):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "        logs['momentum'] = self.model.optimizer.momentum  \n",
    "        super().on_epoch_end(epoch, logs)\n",
    "class DummyCallback(Callback):\n",
    "    pass\n",
    "steps_per_epoch_train_SUB = subset_size // Conf_batch_size_REV2\n",
    "#callbacks>>>\n",
    "# EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy',\n",
    "                               patience=EarlyStopping_P,\n",
    "                               verbose=1, restore_best_weights=True,\n",
    "                               mode='max'\n",
    "                               ) if Use_ES_ONSUBT else DummyCallback()\n",
    "# ModelCheckpoint \n",
    "checkpoint_SUB = ModelCheckpoint(f'cache\\\\model_SUB_checkpoint-{{epoch:03d}}-{{{load_SUB_BRW_MODE}:.4f}}.h5', # f'cache\\\\model_SUB_checkpoint-{{epoch:03d}}-{{{load_SUB_BRW_MODE}:.4f}}.h5', \n",
    "                                 monitor=load_SUB_BRW_MODE,\n",
    "                                 save_best_only=True, mode=ModelCheckpoint_mode,\n",
    "                                 save_weights_only = True\n",
    "                                 ) if load_SUB_BRW else DummyCallback()\n",
    "checkpoint_SUB.best = ModelCheckpoint_Reset_TO\n",
    "# TerminateOnHighTemp\n",
    "TerminateOnHighTemp_CB = TerminateOnHighTemp(active=TerminateOnHighTemp_M,\n",
    "                                             check_every_n_batches=6,\n",
    "                                             high_temp=72,\n",
    "                                             low_temp=58,\n",
    "                                             pause_time=60)\n",
    "# TensorBoard\n",
    "log_dir = 'logs/fit/' + datetime.datetime.now().strftime('y%Y_m%m_d%d-h%H_m%M_s%S')\n",
    "if Use_extended_tensorboard:\n",
    "    tensorboard_callback = ExtendedTensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        write_images=False,  # Uses a lot of memory\n",
    "        histogram_freq=1,\n",
    "        update_freq='epoch',\n",
    "        write_grads=True,\n",
    "        profile_batch='256,512' if Use_tensorboard_profiler else 0\n",
    "    )\n",
    "else:\n",
    "    tensorboard_callback = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        write_images=False,  # Uses a lot of memory\n",
    "        histogram_freq=1,\n",
    "        update_freq='epoch',\n",
    "        write_grads=True,\n",
    "        profile_batch='256,512' if Use_tensorboard_profiler else 0\n",
    "    )\n",
    "# OneCycleLr\n",
    "if OneCycleLr_UFTS:    \n",
    "    learning_rate_schedule_SUB = OneCycleLr(max_lr=MAX_LR,\n",
    "                                            steps_per_epoch=steps_per_epoch_train_SUB,\n",
    "                                            epochs=(PL_epoch * subset_epoch) + ((max_epoch - PL_epoch) * subset_epoch_FT))        \n",
    "#PRES\n",
    "# ...\n",
    "#MAIN\n",
    "print('Training the model...')\n",
    "# INFOp\n",
    "print_Color('\\nSetup Verbose:', ['yellow'])\n",
    "print_Color(f'~*Setting TensorBoard Log dir to ~*[{log_dir}]~*...', ['cyan', 'green', 'cyan'], advanced_mode=True)\n",
    "print_Color(f'~*Use_extended_tensorboard ~*[{Use_extended_tensorboard}]~*.', ['cyan', 'green', 'cyan'], advanced_mode=True)\n",
    "print_Color(f'~*Debug_OUTPUT_DPS ~*[{Debug_OUTPUT_DPS}]~*.', ['cyan', 'green', 'cyan'], advanced_mode=True)\n",
    "print_Color(f'~*OneCycleLr_UFTS ~*[{OneCycleLr_UFTS}]~*.', ['cyan', 'green', 'cyan'], advanced_mode=True)\n",
    "print_Color('Setup Verbose END.', ['yellow'])\n",
    "# MAIN LOOP\n",
    "try:\n",
    "    for epoch in range(1, max_epoch):\n",
    "        # Start Epoch\n",
    "        STG = 'Learning the patterns' if epoch < PL_epoch else 'Fine tuning'\n",
    "        C_subset_epoch = subset_epoch if epoch < PL_epoch else subset_epoch_FT\n",
    "        if epoch > PL_epoch and load_SUB_BRW_LMODE_FN: load_SUB_BRW_LMODE = 1\n",
    "        start_FULL_time = time.time()\n",
    "        if Auto_clear_cache:\n",
    "            subprocess.run([\"Cache_clear.cmd\"], shell=True)\n",
    "        # TSEC: Total-Subset-Epoch-Count\n",
    "        print_Color(f'\\n~*Epoch: ~*{epoch}~*/~*{max_epoch} (TSEC: {Total_SUB_epoch_C})~* | ~*[{STG}]', ['normal', 'cyan', 'normal', 'green', 'blue', 'green'], advanced_mode=True)\n",
    "        # DP\n",
    "        if not AdvSubsetC:\n",
    "            print_Color('Shuffling data...', ['yellow'])\n",
    "            x_train, y_train = shuffle_data(x_train, y_train)\n",
    "        print_Color(f'~*Taking a subset of ~*[|{subset_size}|AdvSubset:{AdvSubsetC}]~*...', ['yellow', 'green', 'yellow'], advanced_mode=True)\n",
    "        if AdvSubsetC:\n",
    "            if AdvSubsetC_SHR > 0 and epoch % AdvSubsetC_SHR == 0:\n",
    "                print_Color('└───Shuffling data...', ['yellow'])\n",
    "                x_train, y_train = shuffle_data(x_train, y_train)\n",
    "                chosen_indices = []  # Reset chosen_indices\n",
    "\n",
    "            available_indices = list(set(range(x_train.shape[0])) - set(chosen_indices))\n",
    "            \n",
    "            if len(available_indices) < subset_size:\n",
    "                #DEBUG\n",
    "                # print('[DEBUG]-[AdvSubset]: Not enough available indices using the indices that were chosen the longest time ago.')\n",
    "                # If there are not enough available indices, choose from the indices that were chosen the longest time ago\n",
    "                old_indices = chosen_indices[:subset_size - len(available_indices)]\n",
    "                subset_indices = old_indices + list(np.random.choice(available_indices, len(available_indices), replace=False))\n",
    "                \n",
    "                # Update the list of chosen indices and their sizes\n",
    "                chosen_indices = chosen_indices[len(old_indices):] + subset_indices\n",
    "                subset_sizes = subset_sizes[len(old_indices):] + [subset_size] * len(subset_indices)\n",
    "            else:\n",
    "                subset_indices = list(np.random.choice(available_indices, subset_size, replace=False))\n",
    "                \n",
    "                # Add the chosen indices to the list of already chosen indices\n",
    "                chosen_indices += subset_indices\n",
    "                subset_sizes += [subset_size] * len(subset_indices)\n",
    "        else:\n",
    "            subset_indices = np.random.choice(x_train.shape[0], subset_size, replace=False)\n",
    "        # Taking the subset\n",
    "        x_SUB_train = x_train[subset_indices]\n",
    "        y_SUB_train = y_train[subset_indices]\n",
    "        x_SUB_train, y_SUB_train = shuffle_data(x_SUB_train, y_SUB_train)\n",
    "        assert len(x_SUB_train) == subset_size, f'Expected subset size of {subset_size}, but got {len(x_SUB_train)}'\n",
    "        print_Color('Preparing train data...', ['yellow']) \n",
    "        # if epoch == 1: # OLD\n",
    "        #     print_Color('- ImageDataGenerator fit...', ['yellow']) \n",
    "        #     train_SUB_datagen.fit(x_SUB_train * 255, augment=True, rounds=6)\n",
    "        #     print_Color('- ImageDataGenerator fit done.', ['yellow'])\n",
    "        \n",
    "        if epoch == 1:\n",
    "            if os.path.exists(IMAGE_GEN_PATH) and not ALWAYS_REFIT:\n",
    "                print_Color('- Loading fitted ImageDataGenerator...', ['yellow'])\n",
    "                train_SUB_datagen = pickle.load(open(IMAGE_GEN_PATH, 'rb')) \n",
    "            else:\n",
    "                print_Color('- Fitting ImageDataGenerator...', ['yellow'])\n",
    "                train_SUB_datagen.fit(x_SUB_train * 255, augment=True, rounds=6)\n",
    "                pickle.dump(train_SUB_datagen, open(IMAGE_GEN_PATH, 'wb'))\n",
    "            print_Color('- ImageDataGenerator fit done.', ['yellow']) \n",
    "\n",
    "        print_Color('- Augmenting Image Data...', ['yellow'])     \n",
    "        train_SUB_augmented_images = train_SUB_datagen.flow(x_SUB_train * 255,\n",
    "                                                            y_SUB_train,\n",
    "                                                            shuffle=False,\n",
    "                                                            batch_size=len(x_SUB_train)\n",
    "                                                            ).next()\n",
    "        print_Color('- Normalizing Image Data...', ['yellow'])\n",
    "        x_SUB_train = normalize_TO_RANGE(train_SUB_augmented_images[0], 0, 255)\n",
    "        # x_SUB_train = apply_clahe_rgb_array(x_SUB_train, 1) / 255\n",
    "        x_SUB_train = x_SUB_train / 255\n",
    "        x_SUB_train = normalize_TO_RANGE(Z_SCORE_normalize(x_SUB_train), 0, 1)\n",
    "        y_SUB_train = train_SUB_augmented_images[1]\n",
    "        # DEBUG\n",
    "        if Debug_OUTPUT_DPS and (epoch % Debug_OUTPUT_DPS_freq == 0 or epoch == 1):\n",
    "            SITD = np.random.choice(subset_size, size=400, replace=False)\n",
    "            S_dir = 'Samples/TSR_SUB_400_' + datetime.datetime.now().strftime('y%Y_m%m_d%d-h%H_m%M_s%S')\n",
    "            print_Color(f'~*- Debug DP Sample dir: ~*{S_dir}', ['red', 'green'], advanced_mode=True)\n",
    "            save_images_to_dir(normalize_TO_RANGE(x_SUB_train[SITD], 0, 1), y_SUB_train[SITD], S_dir)\n",
    "        # learning_rate_schedule_SUB\n",
    "        if PL_epoch == 0:\n",
    "            CU_LR = MIN_LR\n",
    "        elif epoch > PL_epoch and CU_LR > MIN_LR:\n",
    "            if (CU_LR - DEC_LR) < MIN_LR:\n",
    "                CU_LR = MIN_LR\n",
    "            else:\n",
    "                CU_LR -= DEC_LR\n",
    "        if not OneCycleLr_UFTS:    \n",
    "            learning_rate_schedule_SUB = OneCycleLr(max_lr=CU_LR,\n",
    "                                                    steps_per_epoch=steps_per_epoch_train_SUB,\n",
    "                                                    epochs=C_subset_epoch)\n",
    "        #FV\n",
    "        print_Color(f'~*Setting training OneCycleLr::maxlr to ~*[{(str(round(CU_LR, 8)) + \"~*~*\") if not OneCycleLr_UFTS else \"~*OneCycleLr_UFTS Is ON~*\"}]~*...',\n",
    "                    ['yellow', 'green', 'red', 'green', 'yellow'], advanced_mode=True)\n",
    "        print_Color(f'~*Setting training subset epoch.c to ~*[{C_subset_epoch}]~*...', ['yellow', 'green', 'yellow'], advanced_mode=True)\n",
    "        # Train\n",
    "        print_Color('Training on subset...', ['green'])\n",
    "        start_SUBO_time = time.time()\n",
    "        SUB_history = model.fit(x_SUB_train,\n",
    "                            y_SUB_train,\n",
    "                            epochs=C_subset_epoch + Total_SUB_epoch_C, # TO FIX TensorBoard (Total_SUB_epoch_C)\n",
    "                            batch_size=Conf_batch_size_REV2,\n",
    "                            validation_data=(x_test, y_test),\n",
    "                            verbose='auto',\n",
    "                            initial_epoch=Total_SUB_epoch_C, # TO FIX TensorBoard\n",
    "                            callbacks=[\n",
    "                                        learning_rate_schedule_SUB,\n",
    "                                        TerminateOnHighTemp_CB,\n",
    "                                        checkpoint_SUB,\n",
    "                                        early_stopping,\n",
    "                                        tensorboard_callback\n",
    "                                        ]\n",
    "        )\n",
    "        end_SUBO_time = time.time()\n",
    "        print_Color('Subset training done.', ['green'])\n",
    "        if load_SUB_BRW_LMODE == 1:\n",
    "            if max(SUB_history.history['val_accuracy']) > best_acc: \n",
    "                load_weights = True \n",
    "            elif min(SUB_history.history['val_loss']) < best_loss:\n",
    "                load_weights = True \n",
    "            else:\n",
    "                load_weights = False    \n",
    "        else: \n",
    "            load_weights = True \n",
    "        \n",
    "        if load_SUB_BRW and load_weights:\n",
    "            print_Color('Loading the best weights...', ['yellow'])\n",
    "            # Get the filename of the best weights file\n",
    "            list_of_files = glob.glob('cache\\\\*.h5') \n",
    "            try:\n",
    "                best_weights_filename = max(list_of_files, key=os.path.getctime)\n",
    "                print_Color(f'Loading weights from file {best_weights_filename}...', ['yellow'])\n",
    "                model.load_weights(best_weights_filename)\n",
    "            except Exception as Err:\n",
    "                print_Color(f'ERROR: Failed to load weights. Error: {Err}', ['red'])\n",
    "            checkpoint_SUB.best = ModelCheckpoint_Reset_TO\n",
    "        elif load_SUB_BRW and (not load_weights):\n",
    "            # print_Color(f'Not loading weights[BSR:acc{{{max(SUB_history.history[\"val_accuracy\"]):.4f}}}, loss{{{min(SUB_history.history[\"val_loss\"]):.4f}}}|BTR:acc{{{best_acc:.4f}}}, loss{{{best_loss:.4f}}}]',\n",
    "            #             ['yellow']) # OLD\n",
    "            print_Color_V2(f'<light_red>Not loading weights<green>[<light_blue>BSR:<yellow>acc{{{95.675647:.4f}}}, <yellow>loss{{{0.0111:.4f}}}<light_magenta>|<light_blue>BTR:<green>acc{{{97.56456:.4f}}}, loss{{{0.002:.4f}}}]')\n",
    "        all_histories.append(SUB_history.history)\n",
    "        # Garbage Collection (memory)\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()   \n",
    "        # Evaluate the model on the test data\n",
    "        evaluation = model.evaluate(x_test, y_test, verbose=0)\n",
    "        \n",
    "        # Extract the loss and accuracy from the evaluation results\n",
    "        loss = evaluation[0]\n",
    "        acc = evaluation[1]\n",
    "        print_Color(f'~*Model Test acc: ~*{acc:.4f}', ['yellow', 'green'], advanced_mode=True)\n",
    "        print_Color(f'~*Model Test loss: ~*{loss:.4f}', ['yellow', 'green'], advanced_mode=True)\n",
    "        # If the accuracy is higher than the best_acc\n",
    "        if acc > best_acc:\n",
    "            print_Color_V2(f'<yellow>Improved model accuracy from <green>{best_acc} <yellow>to <green>{acc}<yellow>. <light_cyan>Saving model.')\n",
    "            # Update the best_acc\n",
    "            best_acc = acc\n",
    "            if SAVE_FULLM:\n",
    "                # Save the model\n",
    "                if SAVE_TYPE == 'TF':\n",
    "                    print_Color_V2(f'<cyan>Saving full model tf format...')\n",
    "                    model.save(BEST_RSN, save_format='tf')\n",
    "                else:\n",
    "                    print_Color_V2(f'<cyan>Saving full model H5 format...')\n",
    "                    model.save(f'{BEST_RSN}.h5')\n",
    "            model.save_weights('PAI_model_weights.h5')\n",
    "        else:\n",
    "            print_Color_V2(f'<light_red>Model accuracy did not improve from {best_acc}. Not saving model.')\n",
    "            \n",
    "        # If the loss is higher than the best_loss\n",
    "        if loss < best_loss:\n",
    "            print_Color_V2(f'<yellow>Improved model loss from <green>{best_loss} <yellow>to <green>{loss}<yellow>. <light_cyan>Saving model.')\n",
    "            \n",
    "            # Update the best_acc\n",
    "            best_loss = loss\n",
    "            \n",
    "            if SAVE_FULLM:\n",
    "                # Save the model\n",
    "                if SAVE_TYPE == 'TF':\n",
    "                    print_Color_V2(f'<cyan>Saving full model tf format...')\n",
    "                    model.save(BEST_RSN + '_BL', save_format='tf')\n",
    "                else:\n",
    "                    print_Color_V2(f'<cyan>Saving full model H5 format...')\n",
    "                    model.save(f'{BEST_RSN}_BL.h5')\n",
    "            model.save_weights('PAI_model_weights_BL.h5')\n",
    "        else:\n",
    "            print_Color_V2(f'<light_red>Model loss did not improve from {best_loss}. Not saving model.') \n",
    "        # Garbage Collection (memory)\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()   \n",
    "        # Epoch end\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_FULL_time\n",
    "        print_Color_V2(f'<yellow>Time taken for epoch(FULL): <green>{epoch_time:.2f} <cyan>sec')\n",
    "        epoch_SUB_time = end_SUBO_time - start_SUBO_time\n",
    "        print_Color_V2(f'<yellow>Time taken for epoch(SUBo): <green>{epoch_SUB_time:.2f} <cyan>sec')\n",
    "        epoch_OTHERO_time = epoch_time - epoch_SUB_time\n",
    "        print_Color_V2(f'<yellow>Time taken for epoch(OTHERo): <green>{epoch_OTHERO_time:.2f} <cyan>sec')\n",
    "        print_Color(f'<---------------------------------------|Epoch [{epoch}] END|--------------------------------------->', ['cyan'])\n",
    "        Total_SUB_epoch_C += C_subset_epoch # TO FIX TensorBoard\n",
    "except KeyboardInterrupt:\n",
    "    print('\\nKeyboardInterrupt.')\n",
    "# End\n",
    "try:\n",
    "    history = {}\n",
    "    for key in all_histories[0].keys():\n",
    "        # For each metric, concatenate the values from all histories\n",
    "        history[key] = np.concatenate([h[key] for h in all_histories])\n",
    "except Exception as Err:\n",
    "    print(f'Failed to make model `history` var.\\nERROR: {Err}')\n",
    "    \n",
    "print('Training done.\\n')\n",
    "# del vars\n",
    "try:\n",
    "    del train_SUB_datagen\n",
    "    del train_SUB_augmented_images\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rev1 (⚠️deprecated⚠️)\n",
    "```\n",
    "Working: ✅\n",
    "Other:\n",
    " + Tensorboard works.\n",
    " - Can cause overfitting.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "# Garbage Collection (memory)\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "#CONF\n",
    "Conf_batch_size = 8 \n",
    "OneCycleLr_epoch = 20\n",
    "Learning_rate_conf = 3 # 1 and 2 for custom learning_rate_fn and 3 for OneCycleLr (Better for full training)\n",
    "#TensorBoard conf\n",
    "TensorBoard_UF = 1 # 1 for Slow 2 for fast (very slow tarining)\n",
    "# Learning rate configuration\n",
    "Learning_rate_conf_SET2C = 3 # 1 for SGD and 2 for Adam and... for lower lr 3 for very high lr\n",
    "MAX_LR = 0.0174\n",
    "# First time\n",
    "if Learning_rate_conf == 1:\n",
    "    learning_rate_start = 8e-04\n",
    "    learning_rate_max = 5e-03\n",
    "    learning_rate_min = 5e-05\n",
    "    learning_rate_rampup_epochs = 5\n",
    "    learning_rate_sustain_epochs = 1\n",
    "    learning_rate_exp_decay = .3\n",
    "    #TEMP\n",
    "    # learning_rate_start = 8e-04\n",
    "    # learning_rate_max = 1e-02\n",
    "    # learning_rate_min = 8e-04\n",
    "    # learning_rate_rampup_epochs = 5\n",
    "    # learning_rate_sustain_epochs = 3\n",
    "    # learning_rate_exp_decay = .45\n",
    "# 2th time\n",
    "if Learning_rate_conf == 2:\n",
    "    if Learning_rate_conf_SET2C == 1:\n",
    "        learning_rate_start = 4.10e-06\n",
    "        learning_rate_max = 4.10e-06\n",
    "        learning_rate_min = 4.10e-06\n",
    "        learning_rate_rampup_epochs = 0\n",
    "        learning_rate_sustain_epochs = 0\n",
    "        learning_rate_exp_decay = .1\n",
    "        \n",
    "    elif Learning_rate_conf_SET2C == 2:\n",
    "        learning_rate_start = 4e-07\n",
    "        learning_rate_max = 4e-07\n",
    "        learning_rate_min = 4e-07\n",
    "        learning_rate_rampup_epochs = 0\n",
    "        learning_rate_sustain_epochs = 0\n",
    "        learning_rate_exp_decay = .1\n",
    "    \n",
    "    elif Learning_rate_conf_SET2C == 3:\n",
    "        learning_rate_start = 5e-04\n",
    "        learning_rate_max = 5e-04\n",
    "        learning_rate_min = 5e-04\n",
    "        learning_rate_rampup_epochs = 0\n",
    "        learning_rate_sustain_epochs = 0\n",
    "        learning_rate_exp_decay = .1\n",
    "# Function to build learning rate schedule\n",
    "if Learning_rate_conf in [1,2]:\n",
    "    def build_learning_rate_fn(lr_start=learning_rate_start,\n",
    "                            lr_max=learning_rate_max,\n",
    "                            lr_min=learning_rate_min,\n",
    "                            lr_rampup_epochs=learning_rate_rampup_epochs,\n",
    "                            lr_sustain_epochs=learning_rate_sustain_epochs,\n",
    "                            lr_exp_decay=learning_rate_exp_decay):    \n",
    "        lr_max = lr_max * tf.distribute.get_strategy().num_replicas_in_sync\n",
    "        def learning_rate_fn(epoch):\n",
    "            if epoch < lr_rampup_epochs:\n",
    "                lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
    "            elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
    "                lr = lr_max\n",
    "            else:\n",
    "                lr = (lr_max - lr_min) *\\\n",
    "                    lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n",
    "            return lr\n",
    "        return learning_rate_fn\n",
    "    \n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch_train = len(x_train) // Conf_batch_size\n",
    "\n",
    "# Set up callbacks\n",
    "class EpochEndMON(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        optimizer = self.model.optimizer\n",
    "        if hasattr(optimizer, 'lr'):\n",
    "            lr = tf.keras.backend.get_value(optimizer.lr)\n",
    "            print(f'\\nLearning rate for epoch {epoch+1} is {lr}')\n",
    "        if hasattr(optimizer, 'momentum'):\n",
    "            momentum = tf.keras.backend.get_value(optimizer.momentum)\n",
    "            print(f'Momentum for epoch {epoch+1} is {momentum}')\n",
    "        if logs:\n",
    "            val_loss = logs.get('val_loss')\n",
    "            val_acc = logs.get('val_accuracy')\n",
    "            print(f'Validation loss for epoch {epoch+1} is {val_loss}')\n",
    "            print(f'Validation accuracy for epoch {epoch+1} is {val_acc}')\n",
    "\n",
    "        print_Color_V2(f'`red`<!--------------------------------------|Epoch`yellow` [{epoch+1}]`red` End|--------------------------------------!> `green`PBE↓', start_char='`', end_char='`')\n",
    "\n",
    "# Instantiate the callback\n",
    "EpochEndMON_callback = EpochEndMON()\n",
    "if Learning_rate_conf in [1,2]:\n",
    "    learning_rate_fn = build_learning_rate_fn()\n",
    "    learning_rate_schedule = LearningRateScheduler(learning_rate_fn, verbose=1)\n",
    "else:\n",
    "    learning_rate_schedule = OneCycleLr(max_lr=MAX_LR, steps_per_epoch=steps_per_epoch_train, epochs=OneCycleLr_epoch)\n",
    "if SAVE_TYPE == 'TF':\n",
    "    checkpoint_BVAC = ModelCheckpoint('models\\\\Temp\\\\bestVAC_model', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "    checkpoint_BVL = ModelCheckpoint('models\\\\Temp\\\\bestVL_model', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "else:\n",
    "    checkpoint_BVAC = ModelCheckpoint('models\\\\Temp\\\\bestVAC_model.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "    checkpoint_BVL = ModelCheckpoint('models\\\\Temp\\\\bestVL_model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, verbose=1, restore_best_weights=True)\n",
    "log_dir = 'logs/fit/' + datetime.datetime.now().strftime('y%Y_m%m_d%d-h%H_m%M_s%S')\n",
    "TensorBoard_update_freq = 'batch' if TensorBoard_UF == 2 else 'epoch'\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, write_images=True, histogram_freq=1, update_freq=TensorBoard_update_freq, write_grads=True)\n",
    "\n",
    "# Train the model\n",
    "print('Log dir:', log_dir)\n",
    "#MInfo\n",
    "print('Input Shape:', model.input_shape)\n",
    "print('Output Shape:', model.output_shape)\n",
    "print('Loss Function:', model.loss)\n",
    "print('Training the model...\\n')\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=256,\n",
    "                    batch_size=Conf_batch_size,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose='auto',\n",
    "                    callbacks=[early_stopping,\n",
    "                            tensorboard_callback,\n",
    "                            learning_rate_schedule,\n",
    "                            checkpoint_BVAC,\n",
    "                            checkpoint_BVL,\n",
    "                            EpochEndMON_callback])\n",
    "print('Training done.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "Extra_EXT = '_T'\n",
    "# Save the weights\n",
    "print('Saving weights...')\n",
    "model.save_weights('PAI_model_weights.h5')\n",
    "print('Saving full model...')\n",
    "if SAVE_TYPE == 'TF':\n",
    "    print('Saving full model tf format...')\n",
    "    model.save(f'PAI_model{Extra_EXT}', save_format='tf')\n",
    "else:\n",
    "    try:\n",
    "        model.save(f'PAI_model{Extra_EXT}.h5')\n",
    "    except ValueError:\n",
    "        print('failed to save in .h5 format!')\n",
    "        print('Saving full model in tf format...')\n",
    "        model.save(f'PAI_model{Extra_EXT}', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Garbage Collection (memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "# Garbage Collection (memory)\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse model Training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history\n",
    "save_list(history, 'history\\\\model_history.pkl.gz', compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load history\n",
    "history = load_list('history\\\\model_history.pkl.gz', compressed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "# Chunk size for 3D plot\n",
    "chunk_size = 6  # Change this to your desired chunk size\n",
    "    \n",
    "def convert_history(history):\n",
    "    if isinstance(history, tf.keras.callbacks.History):\n",
    "        return history.history\n",
    "    else:\n",
    "        return history\n",
    "    \n",
    "def chunked_data(data, chunk_size):\n",
    "    return [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "\n",
    "try:\n",
    "    EPM = 'Epoch(Subset)' if not isinstance(history, tf.keras.callbacks.History) else 'Epoch'    \n",
    "    history = convert_history(history)\n",
    "\n",
    "    # Calculate deltas\n",
    "    delta_loss = np.diff(history['loss'])\n",
    "    delta_accuracy = np.diff(history['accuracy'])\n",
    "\n",
    "    try:\n",
    "        delta_val_loss = np.diff(history['val_loss'])\n",
    "        delta_val_accuracy = np.diff(history['val_accuracy'])\n",
    "    except (ValueError, NameError):\n",
    "        print('\\033[91mfailed to load val_loss or val_accuracy for delta calculation.')\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    # Loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history['loss'], label='loss')\n",
    "    try:\n",
    "        plt.plot(history['val_loss'], label='val_loss', color='orange')\n",
    "    except (ValueError, NameError):\n",
    "        print('\\033[91mfailed to load val_loss.')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel(EPM)\n",
    "    plt.ylim(top=max(history['val_loss'][10:]), bottom=0) # (max(history['val_loss'][8:]) + min(history['val_loss'])) / 2\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Density plot for loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(history['loss'], label='loss density', color='blue', alpha=0.5, bins=120)\n",
    "    try:\n",
    "        plt.hist(history['val_loss'], label='val_loss density', color='orange', alpha=0.5, bins=120)\n",
    "    except (ValueError, NameError):\n",
    "        print('\\033[91mfailed to load val_loss (density plot).')\n",
    "    plt.title('Density Plot for Loss')\n",
    "    plt.xlabel('Loss')\n",
    "    plt.xlim(right=max(history['val_loss'][10:])) # (max(history['val_loss'][8:]) + min(history['val_loss'])) / 2\n",
    "    plt.grid(True)\n",
    "    \n",
    "    \n",
    "    # Accuracy\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history['accuracy'], label='accuracy')\n",
    "    try:\n",
    "        plt.plot(history['val_accuracy'], label='val_accuracy', color='orange')\n",
    "    except (ValueError, NameError):\n",
    "        print('\\033[91mfailed to load val_accuracy.')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel(EPM)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Density plot for accuracy\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hist(history['accuracy'], label='accuracy density', color='blue', alpha=0.5, bins=50)\n",
    "    try:\n",
    "        plt.hist(history['val_accuracy'], label='val_accuracy density', color='orange', alpha=0.5, bins=50)\n",
    "    except (ValueError, NameError):\n",
    "        print('\\033[91mfailed to load val_accuracy (density plot).')\n",
    "    plt.title('Density Plot for Accuracy')\n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Delta Loss\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(delta_loss, label='delta_loss')\n",
    "    try:\n",
    "        plt.plot(delta_val_loss, label='delta_val_loss', color='orange')\n",
    "    except (ValueError, NameError):\n",
    "        print('\\033[91mfailed to load delta_val_loss.')\n",
    "    plt.title('Delta Model Loss')\n",
    "    plt.ylabel('Delta Loss')\n",
    "    plt.ylim(top=1.5, bottom=-1.5) \n",
    "    plt.xlabel(EPM)\n",
    "    plt.grid(True)\n",
    "    # Delta Accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(delta_accuracy, label='delta_accuracy')\n",
    "    try:\n",
    "        plt.plot(delta_val_accuracy, label='delta_val_accuracy', color='orange')\n",
    "    except (ValueError, NameError):\n",
    "        print('\\033[91mfailed to load delta_val_accuracy.')\n",
    "    plt.title('Delta Model Accuracy')\n",
    "    plt.ylabel('Delta Accuracy')\n",
    "    plt.xlabel(EPM)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Calculate chunked data\n",
    "    chunked_loss = chunked_data(history['val_loss'], chunk_size)\n",
    "    chunked_accuracy = chunked_data(history['val_accuracy'], chunk_size)\n",
    "\n",
    "    # Clip the loss values to a maximum of max(history['val_loss'][10:])\n",
    "    max_loss = max(history['val_loss'][10:])\n",
    "    chunked_loss = np.clip(chunked_loss, a_min=None, a_max=max_loss)\n",
    "\n",
    "    # Create 3D surface plots for each chunk\n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "    ax = fig.add_subplot(121, projection='3d')\n",
    "    X = np.arange(len(chunked_loss))\n",
    "    Y = np.arange(chunk_size)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    Z = np.array(chunked_loss).T  # Transpose the array to match the shape of X and Y\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "    ax.set_title('3D Surface Plot of Chunked Loss')\n",
    "    ax.set_xlabel('Chunk Index')\n",
    "    ax.set_ylabel('Epoch')\n",
    "    ax.set_zlabel('Loss')\n",
    "\n",
    "    ax = fig.add_subplot(122, projection='3d')\n",
    "    X = np.arange(len(chunked_accuracy))\n",
    "    Y = np.arange(chunk_size)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    Z = np.array(chunked_accuracy).T  # Transpose the array to match the shape of X and Y\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "    ax.set_title('3D Surface Plot of Chunked Accuracy')\n",
    "    ax.set_xlabel('Chunk Index')\n",
    "    ax.set_ylabel('Epoch')\n",
    "    ax.set_zlabel('Accuracy')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except (ValueError, NameError) as E:\n",
    "    print(f'\\033[91mFailed to load model history.\\nError: {E}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse model Predicting performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradcam heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_heatmap(model, img_array, conv_layer_name, pred_index):\n",
    "    \"\"\"\n",
    "    Helper function to compute the heatmap for a given convolutional layer.\n",
    "    \"\"\"\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], \n",
    "        [model.get_layer(conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_layer_output, preds = grad_model(img_array)\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, conv_layer_output)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    conv_layer_output = conv_layer_output[0]\n",
    "    heatmap = conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, second_last_conv_layer_name=None, pred_index=None, threshold=0, sensitivity_map=1.0):\n",
    "    \"\"\"\n",
    "    Function to compute the Grad-CAM heatmap for a specific class, given an input image.\n",
    "    \"\"\"\n",
    "    if pred_index is None:\n",
    "        preds = model.predict(img_array)\n",
    "        pred_index = tf.argmax(preds[0])\n",
    "\n",
    "    # Compute heatmap for the last convolutional layer\n",
    "    heatmap = compute_heatmap(model, img_array, last_conv_layer_name, pred_index)\n",
    "    \n",
    "    # Apply threshold and adjust sensitivity\n",
    "    heatmap = np.where(heatmap > threshold, heatmap, 0)\n",
    "    heatmap = heatmap ** sensitivity_map\n",
    "\n",
    "    if second_last_conv_layer_name is not None:\n",
    "        # Compute heatmap for the second last convolutional layer\n",
    "        heatmap_second = compute_heatmap(model, img_array, second_last_conv_layer_name, pred_index)\n",
    "        \n",
    "        # Apply threshold and adjust sensitivity\n",
    "        heatmap_second = np.where(heatmap_second > threshold, heatmap_second, 0)\n",
    "        heatmap_second = heatmap_second ** sensitivity_map\n",
    "        \n",
    "        # Average the two heatmaps\n",
    "        heatmap = (heatmap + heatmap_second) / 2.0\n",
    "    \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from scipy.stats import binom\n",
    "from tqdm import tqdm\n",
    "import efficientnet.tfkeras\n",
    "import cv2\n",
    "import gc\n",
    "# Garbage Collection (memory)\n",
    "gc.collect()\n",
    "\n",
    "Extra_EXT = '_T' # _T or _T_BL\n",
    "prob_L = 0.9995\n",
    "tick_spacing = 5\n",
    "Train_data_test = False\n",
    "if SAVE_TYPE == 'TF':\n",
    "    # Load the pre-trained model\n",
    "    model = load_model(f'PAI_model{Extra_EXT}')\n",
    "else:\n",
    "    # Load the pre-trained model\n",
    "    model = load_model(f'PAI_model{Extra_EXT}.h5')\n",
    "\n",
    "# Ensure the model's input_shape matches your data\n",
    "assert model.input_shape[1:] == (img_res[0], img_res[1], img_res[2]), 'Models input shape doesnt match data.'\n",
    "\n",
    "# Make predictions on validation data\n",
    "val_predictions = model.predict(x_val)\n",
    "val_predictions = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "# Make predictions on Train data\n",
    "if Train_data_test:\n",
    "    Train_predictions = model.predict(x_train)\n",
    "    Train_predictions = np.argmax(Train_predictions, axis=1)\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = model.predict(x_test)\n",
    "test_predictions = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Convert y_val and y_test from one-hot encoder to their original form\n",
    "y_val_original = np.argmax(y_val, axis=1)\n",
    "y_test_original = np.argmax(y_test, axis=1)\n",
    "if Train_data_test:\n",
    "    y_train_original = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Calculate accuracy on validation data\n",
    "val_accuracy = accuracy_score(y_val_original, val_predictions)\n",
    "\n",
    "# Calculate accuracy on Train data\n",
    "if Train_data_test:\n",
    "    Train_accuracy = accuracy_score(y_val_original, Train_predictions)\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "test_accuracy = accuracy_score(y_test_original, test_predictions)\n",
    "\n",
    "# Print acc\n",
    "if Train_data_test:\n",
    "    print(f'The accuracy of the model on Train data is {Train_accuracy:.2%}')\n",
    "print(f'The accuracy of the model on validation data is {val_accuracy:.2%}')\n",
    "print(f'The accuracy of the model on test data is {test_accuracy:.2%}')\n",
    "\n",
    "# Visualize the predictions on validation data as a grid of squares\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(x_val[i])\n",
    "    plt.title(f'True: {y_val_original[i]}\\nPredicted: {val_predictions[i]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#Heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    img = x_val[i]\n",
    "    heatmap = make_gradcam_heatmap(img[np.newaxis, ...], model, 'top_conv', sensitivity_map = 2) \n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    # Apply Adaptive Histogram Equalization\n",
    "    clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(8,8))  # Create CLAHE object\n",
    "    heatmap = clahe.apply(heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    if RANGE_NOM:\n",
    "        superimposed_img = (heatmap / 255) * 0.5 + img\n",
    "    else:\n",
    "        superimposed_img = (heatmap / 255) * 0.5 + (img / 255)\n",
    "    #clip\n",
    "    superimposed_img = normalize_TO_RANGE(superimposed_img, 0, 1)  # ensure the values are in the range [0, 1]\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.title(f'True: {y_val_original[i]}\\nPredicted: {val_predictions[i]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define the list of labels\n",
    "labels = ['NORMAL', 'PNEUMONIA']\n",
    "\n",
    "# Create a confusion matrix for validation data\n",
    "val_cm = confusion_matrix(y_val_original, val_predictions)\n",
    "\n",
    "# Create a confusion matrix for test data\n",
    "test_cm = confusion_matrix(y_test_original, test_predictions)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap for validation data\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(val_cm, annot=True, cmap='Blues', fmt='d', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix - Validation Data')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Plot the confusion matrix as a heatmap for test data\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(test_cm, annot=True, cmap='Blues', fmt='d', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix - Test Data')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Define the range of test data sizes to use\n",
    "data_sizes = range(1, len(x_test), 4)  \n",
    "# Calculate the probability of a wrong prediction based on test accuracy\n",
    "prob_wrong = 1 - test_accuracy\n",
    "\n",
    "# Create a list to store the number of incorrect predictions for each test data size\n",
    "incorrect_predictions = []\n",
    "\n",
    "# Generate predictions and track incorrect predictions for each data size\n",
    "for size in tqdm(data_sizes, desc='Predicting', unit='dpb'):\n",
    "    # Garbage Collection (memory)\n",
    "    gc.collect()\n",
    "    # Randomly select a subset of test data\n",
    "    indices = np.random.choice(len(x_test), size, replace=False)\n",
    "    x_test_subset = x_test[indices]\n",
    "    y_test_subset = y_test[indices]\n",
    "\n",
    "    # Make predictions on the subset of test data\n",
    "    test_predictions = model.predict(x_test_subset, batch_size=1, verbose=0, max_queue_size=120, workers=1, use_multiprocessing=False)\n",
    "    test_predictions = np.argmax(test_predictions, axis=1)\n",
    "    y_test_original_subset = np.argmax(y_test_subset, axis=1)\n",
    "\n",
    "    # Calculate the number of incorrect predictions\n",
    "    incorrect_preds = np.sum(test_predictions != y_test_original_subset)\n",
    "    incorrect_predictions.append(incorrect_preds)\n",
    "    \n",
    "# Plot the number of incorrect predictions vs. the number of data points\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data_sizes, incorrect_predictions)\n",
    "plt.xlabel('Number of Data Points')\n",
    "plt.ylabel('Number of Incorrect Predictions')\n",
    "# Add gridlines for the x and y axes\n",
    "plt.grid(True)\n",
    "\n",
    "# Change the tick spacing for the x and y axes\n",
    "plt.xticks(np.arange(min(data_sizes), max(data_sizes)+1, 50))\n",
    "plt.yticks(np.arange(0, max(incorrect_predictions) + 5, 3))\n",
    "\n",
    "plt.title('Number of Incorrect Predictions vs. Number of Data Points')\n",
    "plt.show()\n",
    "\n",
    "# Define the range of test data sizes to use\n",
    "data_sizes = range(1, len(x_test), 1) \n",
    "\n",
    "# Calculate the probability of a wrong prediction based on test accuracy\n",
    "prob_wrong = 1 - test_accuracy\n",
    "\n",
    "# Create a list to store the probability of getting at least one wrong answer for each test data size\n",
    "probabilities = []\n",
    "\n",
    "# Calculate the probability of getting at least one wrong answer for each data size\n",
    "for size in data_sizes:\n",
    "    # Calculate the cumulative distribution function (CDF) of the binomial distribution at 0\n",
    "    cdf = binom.cdf(0, size, prob_wrong)\n",
    "    # Subtract the CDF from 1 to get the probability of getting at least one wrong answer\n",
    "    prob = 1 - cdf\n",
    "    probabilities.append(prob)\n",
    "\n",
    "# Find the index of the first data point that has a probability greater than prob_L%\n",
    "index = next((i for i, p in enumerate(probabilities) if p > prob_L), len(probabilities))\n",
    "\n",
    "# Limit the x-axis to the first data point that has a probability greater than prob_L%\n",
    "data_sizes = data_sizes[:index+1]\n",
    "probabilities = probabilities[:index+1]\n",
    "\n",
    "# Plot the probability vs. the number of data points\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data_sizes, probabilities)\n",
    "plt.xlabel('Number of Data Points')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "# Add gridlines for the x and y axes\n",
    "plt.grid(True)\n",
    "\n",
    "# Change the tick spacing for the x and y axes\n",
    "plt.xticks(np.arange(min(data_sizes), max(data_sizes)+1, tick_spacing + 2))\n",
    "plt.yticks(np.arange(0, max(probabilities)+0.1, tick_spacing / 100))\n",
    "\n",
    "plt.ylim(top=1.01)\n",
    "\n",
    "plt.title('Probability of Getting at Least One Wrong Answer vs. Number of Data Points')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
